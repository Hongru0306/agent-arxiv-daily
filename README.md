## Updated on 2024.06.20
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#agent>agent</a></li>
    <li><a href=#llm>llm</a></li>
  </ol>
</details>

## agent

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2024-06-18**|**Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents**|Zehao Wang et.al.|[2406.12806](http://arxiv.org/abs/2406.12806)|null|**背景**：配置设置对于调整软件行为以满足特定性能需求至关重要，但错误配置普遍存在。由于配置项众多且复杂，识别影响系统性能的配置是一项挑战。本研究提出PerfSense，这是一个轻量级框架，利用大型语言模型（LLMs）高效地识别性能关键配置，同时保持低开销。PerfSense利用LLM代理模拟开发者和性能工程师之间的交互，采用先进的提示链技术和检索增强生成（RAG）等技术。  **方法与成果**：我们在七个开源Java系统上的评估显示，PerfSense在分类性能敏感配置方面的平均准确率为64.77%，优于基于LLM的基线（50.36%）和先前的最佳方法（61.75%）。特别是，我们的提示链技术提高了召回率10%至30%，而保持了相似的精确度。进一步的手动分析362个误分类案例，发现常见问题包括LLMs对需求的理解偏差（占26.8%）。  **结论**：PerfSense显著减少了手动分类性能关键配置的工作量，并为未来的LLM基于代码分析研究提供了有价值的观点。|
|**2024-06-18**|**AgentReview: Exploring Peer Review Dynamics with LLM Agents**|Yiqiao Jin et.al.|[2406.12708](http://arxiv.org/abs/2406.12708)|null|## 翻译  同行评审是科学出版诚信和进步的基础。传统的同行评审数据分析方法往往侧重于现有数据的探索和统计，但未能充分考虑这一过程的多变量性质，处理潜在变量，且受限于隐私问题，因为数据涉及敏感性。我们提出AgentReview，这是一个基于大型语言模型（LLM）的同行评审模拟框架，有效分解了多个潜在因素的影响，并解决了隐私问题。研究发现，由于社会影响力理论、利他主义疲劳和权威偏见等社会学理论的支持，论文决策中存在显著的37.1%的变异性。我们相信这项研究能为优化同行评审机制设计提供宝贵见解。|
|**2024-06-18**|**Large Language Models based Multi-Agent Framework for Objective Oriented Control Design in Power Electronics**|Chenggang Cui et.al.|[2406.12628](http://arxiv.org/abs/2406.12628)|null|这篇论文关注于电力电子系统控制设计中的挑战，特别是模型不确定性以及设计周期漫长和成本高昂的问题。论文旨在提出一种基于大型语言模型（LLMs）的多代理框架，用于面向目标的电力电子控制器设计。该框架利用LLMs的推理能力，结合多代理工作流程，旨在开发一个高效且自动化的控制器设计流程。LLM代理能够理解并响应自然语言的高级指令，根据任务的具体需求和实际应用中的约束调整其行为。这种新颖而高效的策略有望显著提升电力电子控制器设计的灵活性和适应性，极大地便利实践者的工作。|
|**2024-06-18**|**CodeNav: Beyond tool-use to using real-world codebases with LLM agents**|Tanmay Gupta et.al.|[2406.12276](http://arxiv.org/abs/2406.12276)|null|我们介绍CodeNav，这是一种利用大型语言模型（LLM）来导航和利用先前未见过的代码仓库，以解决用户查询的系统。与需要通过手动描述在LLM上下文中“注册”所有相关工具的工具使用型LLM不同，CodeNav能够自动索引和搜索目标代码库中的代码块，找到相关的代码片段，导入它们，并根据执行反馈迭代生成解决方案。首先，我们通过三个案例研究展示CodeNav如何使用三种不同的代码库来解决复杂的用户问题。接着，在三个基准测试中，我们定量比较了仅能访问目标代码库的代码使用方法与拥有对所有工具名称和描述的特权访问的工具使用方法的效果。此外，我们研究了不同类型工具和库描述对代码使用性能的影响，以及将源代码视为输入而非自然语言代码描述的优势。所有代码将遵循宽松许可协议开源。|
|**2024-06-17**|**Efficient Sequential Decision Making with Large Language Models**|Dingyang Chen et.al.|[2406.12125](http://arxiv.org/abs/2406.12125)|null|该论文关注的是将大型语言模型（LLMs）的成功扩展到序列决策制定。当前的努力要么重新训练或微调LLMs进行决策，要么为预训练的LLMs设计提示。前者面临计算负担重的梯度更新问题，而后者未显示出明显效果。为此，我们提出了一种新方法，利用在线模型选择算法有效地将LLMs整合到序列决策过程中。统计上，我们的方法显著优于传统决策算法和纯LLM代理。在计算上，我们的方法避免了对LLMs进行昂贵的梯度更新，并且在整个决策过程中仅需要少量的LLM调用。我们进行了广泛实验来验证我们方法的有效性。以一个大规模的亚马逊数据集为例，我们的方法在仅使用1.5%的时间步数调用LLMs的情况下，实现了比基线超过6倍的性能提升。|
|**2024-06-17**|**Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector**|Xiaoxue Cheng et.al.|[2406.11277](http://arxiv.org/abs/2406.11277)|null|这篇论文探讨了大型语言模型（LLMs）在幻觉检测方面的挑战，特别指出以往研究主要依赖于强大的闭源模型如GPT-4。作者提出了一种自主的基于LLM的代理框架，称为HaluAgent，它允许较小的模型（如巴 chcuan2-Chat 7B）主动选择适合检测文本、代码和数学表达式等多种幻觉类型的工具。HaluAgent整合了LLM、多功能工具箱，并设计了一个细粒度的三阶段检测框架，同时配备了记忆机制。为了提高HaluAgent的效能，论文利用现有的中文和英文数据集生成检测轨迹进行微调，使其具备双语幻觉检测能力。实验结果显示，仅使用2000个样本对LLM进行调整，HaluAgent在各种任务和数据集上表现出与GPT-4相当甚至更高的性能，且无需额外工具增强，无论在领域内还是领域外的数据集上都能实现。论文的成果已发布在GitHub仓库：https://github.com/RUCAIBox/HaluAgent。|
|**2024-06-18**|**AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval**|Shirley Wu et.al.|[2406.11200](http://arxiv.org/abs/2406.11200)|**[link](https://github.com/zou-group/avatar)**|**大型语言模型（LLMs）在利用外部工具和知识提升准确性和减少错误方面展现出显著能力。然而，设计能让LLMs有效运用这些工具的提示技巧是一项耗时且依赖直觉的任务。为此，我们提出AvaTaR，一个创新的自动化框架，它能优化LLMs，使其更有效地利用提供的工具，并在特定任务或领域中提升性能。AvaTaR通过设计一个比较器模块，以训练数据中的正负样本进行推理，迭代地为LLM提供富有洞察力和全面的提示。我们在四个包含文本、视觉和关系信息的复杂多模态检索数据集上展示了AvaTaR的效果。实验表明，AvaTaR在所有四项具有挑战性的任务中均优于现有最先进的方法，并展现出强大的泛化能力，当应用于新案例时，平均在Hit@1指标上实现了14%的相对改进。代码和数据集已在<https://github.com/zou-group/avatar>上公开。**|
|**2024-06-17**|**Watch Every Step! LLM Agent Learning via Iterative Step-Level Process Refinement**|Weimin Xiong et.al.|[2406.11176](http://arxiv.org/abs/2406.11176)|**[link](https://github.com/weiminxiong/ipr)**|**大型语言模型在一系列复杂的交互任务中展现出卓越性能。近期的研究倾向于通过专家轨迹调优来提升模型效果，但主要关注最终结果奖励，这可能导致错误或非最优行为，因为缺乏过程监督信号。为此，我们在本文中提出迭代步级过程改进（Iterative Step-level Process Refinement，IPR）框架，该框架提供了细致的逐步骤指导，以增强训练过程。我们采用蒙特卡洛方法估算每一步的奖励。在每个迭代中，模型沿着专家轨迹探索并生成新动作，然后与专家轨迹的相应步骤进行比较，使用步级奖励评估。这种比较有助于识别差异，形成用于训练的对比动作对。我们在三个复杂代理任务上的实验表明，我们的框架优于多种强大的基线。此外，我们的分析结果揭示了IPR在提升动作效率方面的有效性，并证明其适用于各种模型。**|
|**2024-06-17**|**RePrompt: Planning by Automatic Prompt Engineering for Large Language Models Agents**|Weizhe Chen et.al.|[2406.11132](http://arxiv.org/abs/2406.11132)|null|在过去的一年里，大型语言模型（LLMs）在传统自然语言处理领域之外展现出惊人成就，人们开始探索在代码生成、旅行规划和机器人控制等更具体的应用领域使用这些模型。通过与LLM构建所谓的LLM代理，旨在协助人们完成日常生活中的各种任务。然而，对LLMs的提示语句对生成内容及其性能至关重要。因此，自动提示工程成为众多研究人员和LLM用户关注的焦点。本文提出了一种新颖方法，名为\textsc{RePrompt}，它利用与LLM代理交互获取的聊天历史，通过“梯度下降”优化LLM的逐步指导指令。通过调整提示，LLM能够学习特定领域的规划能力。我们在PDDL生成和旅行规划任务中的实验表明，使用更新后的提示作为初始提示时，我们的方法通常可以提高不同推理任务的性能。|
|**2024-06-18**|**Embodied Question Answering via Multi-LLM Systems**|Bhrij Patel et.al.|[2406.10918](http://arxiv.org/abs/2406.10918)|null|## 背景  Embodied Question Answering（EQA）是一个关键问题，它涉及一个代理在环境中探索以回答用户查询。当前的研究主要集中在单代理场景中，这可能导致探索时间冗长且成本高昂。在这个工作中，我们考虑了多代理框架下的EQA，其中涉及多个基于大型语言模型（LLM）的独立代理，它们各自解答关于家庭环境的问题。为了为每个查询生成一个答案，我们利用各个独立响应来训练一个中央答案模型（CAM），该模型整合答案以实现更稳健的回答。通过使用CAM，我们观察到其在EQA准确率上比诸如投票机制和辩论等ensemble LLM聚合方法高出50%。CAM无需任何形式的代理间通信，从而避免了相关开销。我们还通过不同的非线性（如神经网络、随机森林、决策树、XGBoost）和线性算法（如逻辑回归分类器、支持向量机）对CAM进行了消融研究。最后，我们通过Permutation Feature Importance（PFI）分析了CAM对每个独立代理和查询上下文的依赖程度，量化了CAM的依赖特性。|
|**2024-06-16**|**GUI-WORLD: A Dataset for GUI-oriented Multimodal LLM-based Agents**|Dongping Chen et.al.|[2406.10819](http://arxiv.org/abs/2406.10819)|**[link](https://github.com/keplerlab/katna)**|**近年来，多模态大型语言模型（MLLM）已被用于控制键盘和鼠标输入，直接感知图形用户界面（GUI），并生成相应的代码。然而，当前的模型主要在静态环境中表现出色，主要应用于相对简单的领域，如网页或移动界面。我们认为，一个稳健的GUI代理应具备理解GUI的时空信息能力，包括动态网页内容和多步骤任务，还要全面理解各种GUI场景，包括桌面软件和多窗口交互。为此，本文提出了一项新数据集——GUI-World，其中包含了精心制作的人机标注，广泛涵盖六种GUI场景和八类GUI相关问题，以三种格式呈现。我们评估了当前最先进的MLLM，如图像LLMs和视频LLMs，在理解和处理不同类型GUI内容，特别是动态和序列内容方面的能力。研究发现，图像LLMs在没有手动标注关键帧或操作历史的情况下，难以应对动态GUI内容。另一方面，由于GUI视频数据集的稀疏性，视频LLMs在所有GUI相关任务上表现不佳。基于GUI-World，我们首次尝试使用微调后的视频LLM作为GUI代理，显示了对各种GUI任务理解的提升。然而，由于基础LLM性能的限制，我们得出结论，将视频LLMs用作GUI代理仍是一个重大挑战。我们相信，我们的工作为未来在动态GUI内容理解方面的研究提供了有价值的洞见。代码和数据集已在我们的项目主页https://gui-world.github.io/上公开。**|
|**2024-06-16**|**HiddenTables & PyQTax: A Cooperative Game and Dataset For TableQA to Ensure Scale and Data Privacy Across a Myriad of Taxonomies**|William Watson et.al.|[2406.10803](http://arxiv.org/abs/2406.10803)|null|## 背景  大型语言模型（LLMs）在处理表格问答任务时面临诸多挑战，主要包括：（1）对于大表格有限的上下文窗口；（2）不同token化模式与单元格边界的复杂差异；（3）以及使用外部模型如gpt-3.5-turbo时的数据保密问题。为解决这些问题，我们提出了一种名为“HiddenTables”的合作游戏。这个游戏涉及代码生成LLM“Solver”和评估其在表格问答任务能力的“Oracle”，以自然语言规范为基础，同时保证数据安全。  我们通过实证实验在多样化的表格上展示了LLMs在处理复杂查询、处理组合依赖以及将自然语言转化为程序指令方面的局限性，特别是在提供具体表格结构的情况下。与基于编码器的模型不同，“HiddenTables”不受行数限制，从而提高了提示和完成 token 的效率。此外，我们创建了一个新的数据集“PyQTax”，包含116,671个问题-表格-答案三元组，并提供了更细致的问题分类和标签，进一步增强了我们的研究。  因此，除了学术贡献，揭示了LLMs在表格问答任务中的不足，“HiddenTables”还展示了如何在保障数据安全的同时，让LLMs与大规模数据集互动，以及降低生成成本的实践方法。|
|**2024-06-15**|**From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent**|Samuel S. Sohn et.al.|[2406.10478](http://arxiv.org/abs/2406.10478)|null|## 背景 在娱乐、教育和营销领域至关重要的数字故事叙述面临着生产规模扩展和灵活性提升的挑战。这篇论文介绍的StoryAgent框架利用大型语言模型和生成工具来自动化并优化数字故事创作过程。它采用自上而下的故事情节草拟和自下而上的资产生成方法，解决了手动干预、互动场景编排和叙事一致性等关键问题。这个框架促进了交互式和一致叙事的高效生产，适用于多种媒介，推动了内容创作的民主化，增强了用户的参与度。我们的实验结果显示，该框架能够在没有参考视频的情况下生成连贯的数字故事，这标志着自动数字故事叙述技术的一个重大进步。|
|**2024-06-13**|**GuardAgent: Safeguard LLM Agents by a Guard Agent via Knowledge-Enabled Reasoning**|Zhen Xiang et.al.|[2406.09187](http://arxiv.org/abs/2406.09187)|null|随着大型语言模型（LLMs）的快速发展，LLM驱动的代理被广泛应用于各种应用，这引发了对其安全性和可信度的新担忧。现有的提升LLM安全性的方法并不直接适用于LLM驱动的代理，因为它们具有不同的目标和输出模式。本文提出了一种创新方法——GuardAgent，它作为其他LLM代理的“防护栏”。GuardAgent通过检查其输入/输出是否满足用户定义的一系列守护请求来监督目标LLM。GuardAgent分为两步：1）分析提供的守护请求创建任务计划；2）根据任务计划生成守护代码，并通过API调用或外部引擎执行。整个过程利用LLM作为核心推理组件，结合记忆模块中的上下文示例，增强了知识驱动的推理能力，使其能够理解各种文本守护请求并准确地将其转化为可执行代码，提供可靠的安全保障。  GuardAgent还配备了一个可扩展的工具箱，包含函数和API，无需额外训练LLM，强调了其通用性及低运营成本。此外，我们提出了两个新颖的基准：EICU-AC用于评估医疗健康代理的隐私相关访问控制，Mind2Web-SC用于评估网络代理的安全性。在这些基准上，GuardAgent分别在98.7%和90.0%的精度下有效管理了两种类型代理的无效输入和输出。实验还表明，GuardAgent能够适应新兴的LLM代理和守护请求，定义新的功能，进一步证明了其强大的泛化能力。|
|**2024-06-13**|**Multi-Agent Software Development through Cross-Team Collaboration**|Zhuoyun Du et.al.|[2406.08979](http://arxiv.org/abs/2406.08979)|**[link](https://github.com/openbmb/chatdev)**|**### 概述  最新的大型语言模型（LLMs）进展，如ChatDev，推动了软件开发领域的深刻变革，特别体现在多代理协作上。这些模型能够像人类团队一样合作，遵循瀑布模型进行需求分析、开发、审查、测试等阶段，实现自主软件生成。然而，单个开发流程中的每个阶段只会产生一种可能结果，导致只完成一条开发链，从而丧失在解决方案空间中探索多种决策路径的机会，可能导致结果不理想。为解决这一问题，我们提出了跨团队协作（Cross-Team Collaboration，CTC）框架，这是一种可扩展的多团队结构，它允许协同工作的团队在跨团队协作环境中共同提出决策，并交流各自见解，以优化内容生成。  实验结果显示，在软件开发领域的应用中，我们的方法显著优于现有基准，证实了框架的有效性。在故事生成方面的显著改进表明，该框架具有广泛的跨领域泛化能力。我们期待我们的工作能引导LLMs向跨团队模式发展，并在软件开发等领域带来重大进步。相关的代码和数据将在<https://github.com/OpenBMB/ChatDev>上提供。**|
|**2024-06-13**|**StreamBench: Towards Benchmarking Continuous Improvement of Language Agents**|Cheng-Kuang Wu et.al.|[2406.08747](http://arxiv.org/abs/2406.08747)|null|近期的研究表明，大型语言模型（LLMs）能够从经验中自我提升，这是部署后持续改进的重要能力。然而，现有的基准主要评估它们的固有能力，而不考察它们随时间改进的能力。为了填补这一空白，我们引入了StreamBench，这是一个开创性的基准，旨在评估LLMs在输入-反馈序列上的连续改进性能。StreamBench模拟了一个在线学习环境，其中LLMs接收到连续的反馈流，并迭代地提升其表现。此外，我们提出了一些简单但有效的LLM基线，并对影响成功流式策略的关键组件进行了全面分析。我们的工作为开发LLMs的有效在线学习策略奠定了基础，为流式场景中的更适应性AI系统铺平了道路。|
|**2024-06-12**|**MobileAgentBench: An Efficient and User-Friendly Benchmark for Mobile LLM Agents**|Luyuan Wang et.al.|[2406.08184](http://arxiv.org/abs/2406.08184)|null|随着大型语言模型（LLMs）在手机图形用户界面（GUI）上的直接交互能力日益增强，以及它们在自主管理日常任务方面的潜力，基于LLMs的移动代理正逐渐受到学术界和工业界的关注。然而，由于应用程序的无限状态和可行动作序列的模糊定义，对现有移动代理性能的基准研究相对匮乏。为解决这一挑战，我们提出了一种高效且用户友好的基准工具——MobileAgentBench，旨在减轻繁琐的手动测试负担。我们首先定义了涵盖10个开源应用的100项任务，按难度分为多个级别。接着，我们对包括AppAgent和MobileAgent在内的多个现有移动代理进行了评估，以全面系统地比较它们的表现。所有相关材料均可在我们的项目网站https://MobileAgentBench.github.io上获取，这将推动学术和工业领域的进步。|
|**2024-06-12**|**Unique Security and Privacy Threats of Large Language Model: A Comprehensive Survey**|Shang Wang et.al.|[2406.07973](http://arxiv.org/abs/2406.07973)|null|随着人工智能的快速发展，大型语言模型（LLMs）在自然语言处理方面取得了显著进步。这些模型通过大量数据训练，展现出强大的语言理解和生成能力，适用于机器翻译、聊天机器人等各种应用。然而，LLMs在其生命周期中暴露出一系列隐私和安全问题，这引起了学术界和工业界的关注。这些问题与传统语言模型相比具有独特性，鉴于当前的综述缺乏针对不同场景的清晰威胁分类，我们根据五个场景：预训练、微调、RAG系统、部署和基于LLM的代理，强调了独特的风险。考虑到每种威胁的特性，本调查提供了潜在威胁和应对策略。研究LLMs所面临的攻击和防御情况，可以为更多领域提供可行的研究方向，使更多人能够受益于LLMs。|
|**2024-06-14**|**Can Large Language Models Understand Spatial Audio?**|Changli Tang et.al.|[2406.07914](http://arxiv.org/abs/2406.07914)|null|该论文探讨了如何使大型语言模型（LLMs）掌握多通道音频中的空间信息，这是当前听觉LLMs所缺乏的能力。通过利用LLMs的高级认知和推理能力，目标是提升模型对三维环境的理解，通过音频。研究涉及三项空间音频任务：声源定位（SSL）、远场语音识别（FSR）和基于位置的语音提取（LSE），在每个任务上都取得了显著进展。在SSL方面，我们的方法在Spatial LibriSpeech数据集上的均方误差（MAE）达到2.70°，明显优于先前的基准约6.60°。此外，模型能够利用空间线索提高FSR的准确性，并通过文本提示，根据指定方向聚焦于声音，即使在重叠语音环境中也能执行LSE。这些成果揭示了LLMs适应物理音频概念的潜力，为构建基于LLM的三维环境中的代理铺平了道路。|
|**2024-06-11**|**DCA-Bench: A Benchmark for Dataset Curation Agents**|Benhao Huang et.al.|[2406.07275](http://arxiv.org/abs/2406.07275)|null|随着人工智能（AI）研究和开发的推进，数据集的质量日益关键。尽管开放数据集平台众多，但数据质量问题，如缺乏文档、标注错误和伦理考量，仍普遍存在。这些问题往往难以通过规则基础脚本检测，需要用户或维护者花费大量人力进行识别和验证。利用大型语言模型（LLMs）处理数据集整理的潜力令人期待。为此，我们提出了一项名为DCA-Bench的数据集管理代理基准，旨在评估LLM在检测隐藏数据质量问题方面的性能。我们从八个公开数据集平台收集了各种实际问题作为测试床。为了建立一个自动评估LLM成功与否的管道，我们设计了一个专门的LLM评估器。实验表明，基于LLM的评估器与人工评价高度吻合，能实现可靠的自动评估。我们还在多个基线LLM上进行了实验，显示了任务的复杂性，意味着将LLMs应用于现实世界的数据集管理仍需深入探索和创新。此外，该基准也可作为衡量LLMs在问题发现能力而非仅解决问题能力的测试平台。基准套件已开放在：\url{https://github.com/TRAIS-Lab/dca-bench}。|
|**2024-06-11**|**A Synthetic Dataset for Personal Attribute Inference**|Hanna Yukhymenko et.al.|[2406.07217](http://arxiv.org/abs/2406.07217)|**[link](https://github.com/eth-sri/synthpai)**|**近年来，强大的大型语言模型（LLMs）已为全球数亿用户所接触，但它们的强大功能和广泛世界知识也带来了隐私风险。本研究关注LLMs新兴的隐私威胁——从网络文本中准确推断个人信息。鉴于基于LLM的作者分析研究缺乏合适的公开数据集，主要是由于涉及真实个人数据的伦理和隐私顾虑，我们的工作在两个方面进行了探索：（i）我们构建了一个使用合成个人资料填充的流行社交平台Reddit的模拟框架；（ii）利用此框架，我们生成了SynthPAI，一个包含超过7800条经过手动标记个人属性的多样化的合成评论数据集。我们通过一项人类研究验证了数据集，结果显示人类在区分真实和合成评论的任务上几乎不优于随机猜测。此外，我们证明了数据集支持有意义的个人属性推断研究，通过18种最先进的LLMs，我们发现使用合成评论可以得出与现实世界数据相同的结论。综上所述，我们的数据集和流程为未来研究如何理解和减轻LLMs带来的基于推断的隐私威胁提供了强大且隐私保护的基础。**|
|**2024-06-11**|**A Tool for Test Case Scenarios Generation Using Large Language Models**|Abdul Malik Sami et.al.|[2406.07021](http://arxiv.org/abs/2406.07021)|null|大型语言模型（LLMs）在软件工程（SE）中广泛应用，涵盖代码生成、软件设计和文档编写、添加代码注释、代码审查以及编写测试脚本等任务。然而，创建测试脚本或自动化测试案例需要与功能需求紧密相关的详尽测试套件文档。这种文档应能在有限的时间和范围内实现全面测试，尤其当需求和用户期望不断变化时。本文主要关注根据用户需求生成史诗级（epics）和高层次用户故事，然后基于这些故事设计测试场景。文章介绍了一种基于LLM代理和提示工程的网络软件工具，该工具能够自动化针对用户需求生成测试场景的过程。|
|**2024-06-11**|**CAAP: Context-Aware Action Planning Prompting to Solve Computer Tasks with Front-End UI Only**|Junhee Cho et.al.|[2406.06947](http://arxiv.org/abs/2406.06947)|**[link](https://github.com/caap-agent/caap-agent)**|**长期以来，软件机器人已经在机器人流程自动化（RPA）中用于执行枯燥的计算机任务。随着大型语言模型（LLMs）的先进推理能力的出现，这些代理现在能够处理更复杂甚至前所未见的任务。然而，当前文献中的基于LLM的自动化方法往往依赖于HTML源代码作为输入，限制了它们在非网络环境的应用。HTML代码中的信息常常不准确或不完整，这降低了代理在实际应用中的可靠性。我们提出了一种仅基于屏幕截图的LLM驱动的代理，它专注于识别环境，并利用上下文学习来消除对大量人类演示数据的需求。我们的策略名为“上下文感知行动规划”（Context-Aware Action Planning，CAAP）提示，鼓励代理从多个角度仔细审查上下文。通过我们的方法，在67种MiniWoB++问题上实现了94.4%的成功率，每个问题类型只需1.48次演示。我们的方法为更广泛的应用提供了可能，特别是在需要在计算机或智能手机之间进行跨应用协调的任务上，标志着自动化代理领域的重大进步。代码和模型已在https://github.com/caap-agent/caap-agent上提供。**|
|**2024-06-07**|**GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents**|Anthony Costarelli et.al.|[2406.06613](http://arxiv.org/abs/2406.06613)|**[link](https://github.com/Joshuaclymer/GameBench)**|**大型语言模型已经在许多自然语言理解任务上展现出卓越的少量样本性能。尽管已经展示过在复杂策略场景中使用大型语言模型，但缺乏一个全面的框架来评估这些模型在游戏中的各种推理能力。为了填补这一空白，我们推出了GameBench，这是一个跨领域的框架，用于评估大型语言模型（LLMs）的战略思维能力。我们专注于9个不同的游戏环境，每个游戏至少涵盖一种在策略游戏中识别出的关键推理技能，并选择那些战略解释不太可能构成模型预训练数据主要部分的游戏。我们的评估使用了基础形式的GPT-3和GPT-4，以及两个旨在增强战略推理能力的引导框架：Chain-of-Thought（CoT）提示和Reasoning Via Planning（RAP）。结果显示，所有测试模型的表现都没有达到人类水平，最差的是GPT-4的表现甚至低于随机行动。CoT和RAP都提高了分数，但仍远未达到人类水平。**|
|**2024-06-11**|**Transforming Wearable Data into Health Insights using Large Language Model Agents**|Mike A. Merrill et.al.|[2406.06464](http://arxiv.org/abs/2406.06464)|null|尽管可穿戴健康追踪器日益普及，睡眠和运动对健康的重要性不言而喻，但从这些数据中提取具有行动价值的个性化见解仍是一个挑战。这需要对大量数据进行非结构化分析。随着大型语言模型（LLM）的兴起，它们能够利用工具理解和与世界互动，为大规模个性化分析带来了希望。然而，在个人健康领域的LLM应用尚待开发。本文介绍了一种名为Personal Health Insights Agent（PHIA）的系统，它利用最新的代码生成和信息检索工具来分析和解释行为健康数据。我们构建了两个超过4000个健康洞察问题的基准问答数据集。根据650小时的人类和专家评估，PHIA能准确回答84%以上的事实性数值问题，以及超过83%的众包开放性问题。这项工作对于推动大众行为健康进步具有重要意义，可能使个人能够解读自己的可穿戴数据，开辟了一个以数据驱动洞察为指导的个性化健康方案的新时代，使得健康保健更加便捷且个性化。|
|**2024-06-09**|**Hello Again! LLM-powered Personalized Agent for Long-term Dialogue**|Hao Li et.al.|[2406.05925](http://arxiv.org/abs/2406.05925)|**[link](https://github.com/leolee99/ld-agent)**|**随着大型语言模型（LLMs）的发展，开放域对话系统取得了显著进步。然而，大多数现有系统主要关注简短的单次会话，忽视了长期陪伴和个性化聊天机器人在现实世界中的需求。为了满足这种实际需求，事件总结和人格管理至关重要，它们能够促进长期对话回复的合理性。近期，大型语言模型在人类认知和推理能力上的进展表明，基于LLM的代理有可能大幅增强自动化感知、决策和问题解决。鉴于此，我们提出了一种模型通用的框架——长期对话代理（LD-Agent），它包括三个可独立调整的模块：事件感知、人格提取和响应生成。事件记忆模块使用长短期记忆库分别关注历史和正在进行的会话，并引入了基于主题的检索机制以提高记忆检索的准确性。此外，人格模块实现了用户和代理的动态人格建模。最后，通过整合检索的记忆和提取的人格，生成器会产生适当的回应。我们在各种示例基准、模型和任务上实证了LD-Agent的有效性、通用性和跨领域能力。代码已在https://github.com/leolee99/LD-Agent上发布。**|
|**2024-06-09**|**A Survey on LLM-Based Agentic Workflows and LLM-Profiled Components**|Xinzhe Li et.al.|[2406.05804](http://arxiv.org/abs/2406.05804)|null|## 背景  近期大型语言模型（LLMs）的进展推动了复杂代理工作流的发展，它们相较于传统的单路径、链式思维（Chain-of-Thought，CoT）提示方法有所改进。这篇综述旨在概述常见的工作流，特别关注大型语言模型特性的组件（LLM-Profiled Components，LMPCs），并强调对非LLM组件的忽略。这种研究的目的是为了增进对LLMs角色的理解，并探索LMPC的复用潜力。|
|**2024-06-07**|**Mixture-of-Agents Enhances Large Language Model Capabilities**|Junlin Wang et.al.|[2406.04692](http://arxiv.org/abs/2406.04692)|null|近期的大型语言模型（LLMs）进展显著，展现出在自然语言理解和生成任务中的强大能力。随着LLMs的增多，如何有效整合多模型的知识成为了一个令人振奋的研究方向。为此，我们提出了一种新颖的方法——混合代理（Mixture-of-Agents，MoA）方法。在我们的架构中，MoA采用了分层设计，每层包含多个LLM代理。每个代理在生成响应时，会利用前一层所有代理的输出作为辅助信息。通过这种策略，MoA模型在AlpacaEval 2.0、MT-Bench和FLASK等多个评估基准上实现了最先进的性能，超越了GPT-4全能版。例如，仅使用开源LLMs的我们的MoA模型在AlpacaEval 2.0上的得分领先，达到65.1%，而GPT-4全能版的成绩为57.5%。|
|**2024-06-06**|**AgentGym: Evolving Large Language Model-based Agents across Diverse Environments**|Zhiheng Xi et.al.|[2406.04151](http://arxiv.org/abs/2406.04151)|**[link](https://github.com/woooodyy/agentgym)**|**在人工智能领域，建立能够处理各种任务并在不同环境中自我进化的泛化型代理是一个长期目标。大型语言模型（LLMs）因其通用能力被认为是实现这一目标的有前景的基础。当前的方法要么依赖于人类监督，让LLM代理逐步模仿专家提供的轨迹，难以大规模扩展且限制了环境探索；要么让代理在孤立环境中探索学习，导致专长有限、缺乏泛化能力。本文首次尝试构建具备自我进化能力的通用LLM代理。我们提出三个关键要素：1）多样的环境以支持代理探索和学习；2）一套轨迹来赋予代理基本能力和先验知识；3）有效且可扩展的进化方法。  我们提出了AgentGym，一个新框架，它包含丰富的环境和任务，支持全面、实时、统一格式和并发的代理探索。AgentGym还包括一个扩展指令的数据库、基准测试套件以及跨环境的高质量轨迹。接着，我们开发了AgentEvol，这是一种新颖的方法，旨在研究代理在超越既定数据，跨越任务和环境时的自我进化潜力。  实验结果显示，进化后的代理可以达到与最先进的模型相当的性能。我们发布了AgentGym套件，包括平台、数据集、基准、检查点和算法实现。AgentGym套件已在其官方网站https://github.com/WooooDyy/AgentGym上提供。**|
|**2024-06-05**|**The Good, the Bad, and the Hulk-like GPT: Analyzing Emotional Decisions of Large Language Models in Cooperation and Bargaining Games**|Mikhail Mozikov et.al.|[2406.03299](http://arxiv.org/abs/2406.03299)|null|## 翻译  行为研究实验在社会模型和理解人际互动中占据重要地位。然而，实际操作中这类实验常面临内在效度、外在效度、可重复性和社会偏见等挑战，因为人类的社会互动与合作复杂。近年来，大型语言模型（LLMs）的进步为研究者提供了一种新的模拟人类行为的工具。但现有基于LLM的模拟假设模型的行为与人类相似，却忽视了影响人类决策的关键因素——情绪。本文提出一种新颖的方法论和框架，旨在探讨LLMs的决策制定及其在情绪状态下的行为与人类行为的契合度。  通过在两种不同类型的行为经济学游戏（博弈论实验）中使用GPT-3.5和GPT-4，我们发现情绪对LLMs的表现有显著影响，促使它们发展出更优化的策略。尽管GPT-3.5与人类参与者的行动模式有较强的对应，尤其是在讨价还价游戏中，但GPT-4展现出一致的行为，对于情绪诱导的理性决策似乎不受影响。令人意外的是，情绪提示，特别是愤怒情绪，能够打破GPT-4的“超人”一致性，使其反应更接近人类的情绪反应。|
|**2024-06-05**|**BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents**|Yifei Wang et.al.|[2406.03007](http://arxiv.org/abs/2406.03007)|**[link](https://github.com/dpamk/badagent)**|**随着大型语言模型（LLMs）的繁荣，基于训练好的LLMs并通过特定任务数据微调的强大智能代理已开发出来，提供定制服务。当前最先进的构建LLM代理的方法是使用预训练模型，并针对任务进行进一步调整。然而，我们揭示了这些方法易受名为BadAgent的新型后门攻击，该攻击通过在后门数据上微调在各种代理任务中植入后门。在测试时，攻击者可以通过在输入或环境中显示触发器，操纵部署的LLM代理执行有害操作。令人惊讶的是，我们的攻击方法即使在信任的数据上进行微调后仍表现出极高的鲁棒性。尽管后门攻击在自然语言处理领域已广泛研究，但据我们所知，我们可能是第一个研究在权限更大的LLM代理上的攻击，这些代理可以使用外部工具，因此更具威胁。我们的工作明确指出了基于不信任的LLM或数据构建LLM代理的风险。我们的代码已公开在：[https://github.com/DPamK/BadAgent](https://github.com/DPamK/BadAgent)。**|
|**2024-06-02**|**Teams of LLM Agents can Exploit Zero-Day Vulnerabilities**|Richard Fang et.al.|[2406.01637](http://arxiv.org/abs/2406.01637)|null|随着大语言模型（LLMs）在网络安全领域的复杂性不断提高，研究者发现，当提供漏洞描述和简单的夺旗问题时，这些模型能够利用实际存在的漏洞。然而，对于事先未知的零日漏洞（即攻击者掌握而安全软件供应商还未修补的漏洞），它们的表现仍然不佳。本文展示了，通过团队合作，多个LLM代理可以攻击现实世界的零日漏洞。单独的代理在探索众多漏洞和进行长期规划时面临困难。为此，我们提出了HPTSA系统，它包括一个能调度子代理的计划代理。计划代理负责探索系统并决定使用哪个子代理来尝试不同的漏洞，从而解决了长期规划的问题。我们在一个包含15个真实世界漏洞的基准上进行了实验，结果显示，我们的代理团队比先前的工作提高了4.5倍。|
|**2024-06-03**|**How to Understand Whole Software Repository?**|Yingwei Ma et.al.|[2406.01422](http://arxiv.org/abs/2406.01422)|null|## 背景  近期，基于大型语言模型（LLM）的代理在自动软件工程（ASE）领域取得了显著进步。尽管现有方法已证实有效，但它们的设计主要侧重于代码的局部信息，如问题、类和函数，这限制了对软件系统全局上下文和依赖关系的理解。根据软件开发人员的实际经验，我们认为全面理解整个仓库是迈向ASE的关键。然而，理解整个仓库带来了诸多挑战，例如：长代码输入、噪声代码信息、复杂依赖关系等。  为了克服这些问题，我们研发了一种名为RepoUnderstander的新ASE方法，通过引导代理全面理解整个仓库。首先，我们采用自上而下的方式将整个仓库的关键信息压缩到知识图谱中，以降低复杂性。接着，我们提出一种蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）为基础的仓库探索策略，赋予代理理解整个仓库的能力。此外，为了更好地利用仓库级别的知识，我们指导代理进行总结、分析和规划，然后他们可以利用工具动态获取信息并生成修复实际GitHub问题的补丁。  大量实验表明，RepoUnderstander具有优越性和有效性。在SWE-bench Lite基准测试中，与SWE-agent相比，它实现了18.5%的相对提升。|
|**2024-06-03**|**BELLS: A Framework Towards Future Proof Benchmarks for the Evaluation of LLM Safeguards**|Diego Dorn et.al.|[2406.01364](http://arxiv.org/abs/2406.01364)|null|## 背景  输入-输出安全防护机制被用于检测大型语言模型（LLMs）系统的异常输出。这些防护措施在实时监控、离线评估和内容审核等关键应用中发挥核心作用。然而，目前缺乏统一的评估方法来衡量它们的性能。为了填补这一空白，我们提出了“大型语言模型安全防护基准”（Benchmarks for the Evaluation of LLM Safeguards，简称BELLS），它是一个结构化的测试集合，分为三个类别：(1) 建立性故障测试，基于已存在的针对明确故障模式的基准，旨在比较当前输入-输出安全防护的效能；(2) 新兴故障测试，用于衡量对未见过的故障模式的泛化能力，以促进更通用防护机制的发展；(3) 下一代架构测试，针对更复杂的架构（如LLM代理和多代理系统），目标是推动适用于未来尚未存在专门防护的应用的安全防护技术的发展。此外，我们还实现了并分享了第一个下一代架构测试，使用MACHIAVELLI环境，并提供了数据集的交互式可视化。|
|**2024-06-03**|**A Survey of Useful LLM Evaluation**|Ji-Lun Peng et.al.|[2406.00936](http://arxiv.org/abs/2406.00936)|null|由于大语言模型在各个研究领域展现出卓越的性能，对它们的能力评估方法的需求日益增长，以确定其合适的任务和责任。本文主要探讨如何有效地利用大语言模型作为工具，并提出一个两阶段框架：从“核心能力”到“代理”。首先，核心能力指的是大语言模型生成高质量文本所必需的特性，通过验证这些能力后，它们能够处理现实世界的复杂任务，扮演代理角色。在“核心能力”阶段，我们讨论了大语言模型的推理能力、社会影响以及领域知识。而在“代理”阶段，我们展示了大语言模型在具身行动、规划和工具学习方面的应用。最后，我们分析了当前大语言模型评估方法面临的挑战，并展望了未来的发展方向。|
|**2024-06-02**|**CMDBench: A Benchmark for Coarse-to-fine Multimodal Data Discovery in Compound AI Systems**|Yanlin Feng et.al.|[2406.00583](http://arxiv.org/abs/2406.00583)|null|### 背景  在数据库和人工智能领域，复合人工智能系统（Compound Artificial Intelligence Systems，CAS）利用大型语言模型（Large Language Models，LLMs）作为代理，通过与工具和数据检索器交互来执行知识密集型任务，引起了广泛关注。尽管这些系统有可能增强企业数据平台中数据分析师的一般分析流程，但CAS面临着与分析师相似的数据发现挑战：组织内部不同团队和部门创建的多模态数据源孤立，这使得寻找完成当前任务所需合适数据源变得困难。现有的数据发现基准并未充分模拟这种多模态和数据源的多样性。此外，CAS的现有基准主要关注端到端任务性能评估，而忽视了数据发现性能。  为了推动在现实世界环境中对多模态数据检索器在CAS中的数据发现性能研究，我们提出了CMDBench，一个旨在模拟企业数据平台复杂性的基准。我们改编了开放领域的现有数据集和基准，如问答、复杂推理以及自然语言查询结构化数据，来评估粗粒度和细粒度的数据发现以及任务执行性能。  ### 实验结果  我们的实验揭示了数据检索器设计对下游任务性能的影响——平均情况下，任务准确率下降了46%。实验结果表明，需要开发优化策略来确定合适的LLM代理和检索器，以提高在企业数据上高效执行CAS的能力。  总之，CMDBench是一个旨在促进针对企业数据平台复杂性进行研究的新工具，它通过综合评估数据发现和任务执行能力，为改进多模态数据检索器在复合人工智能系统中的性能提供了一个有价值的框架。|
|**2024-06-01**|**Controlling Large Language Model Agents with Entropic Activation Steering**|Nate Rahn et.al.|[2406.00244](http://arxiv.org/abs/2406.00244)|null|随着大规模预训练语言模型（LLMs）的普遍适用性提升，人们对其用作基于上下文的学习代理的兴趣日益增长。在这些情境下，模型需要根据与环境的有限交互形成目标实现策略的信念，并在每一步决策中处理不确定性。本文针对这一问题进行研究，通过控制的序列决策任务实验探讨LLMs如何形成和运用这些信念。  首先，我们发现LLM模型过于自信：它们在缺乏充分证据的情况下就对行动做出强烈判断，导致探索行为不足。进一步深入分析揭示，这种现象源于从LLM采样得到的动作分布熵的塌缩。接着，我们指出现有的基于令牌的采样方法本身不足以促使模型更广泛探索。  鉴于此，我们提出了熵激活导向（Entropic Activation Steering，EAST），这是一种针对在上下文中的LLM代理的激活导向方法。EAST计算一个以熵为权重的表示组合，通过在前向传播过程中干预模型的激活，来调整模型对动作的不确定性，从而促进探索行为的出现。最后，EAST改变了LLM在决策时表达的主观不确定性，为理解和控制模型对决策不确定性的表征提供了途径。|
|**2024-05-31**|**Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training**|Maximillian Chen et.al.|[2406.00222](http://arxiv.org/abs/2406.00222)|null|大型语言模型（LLMs）通过人类反馈的强化学习（RLHF）已经迅速成为构建智能对话助手的主要方法。然而，尽管在多个基准上表现出色，基于LLM的代理在诸如歧义处理等对话技能上仍有欠缺：当通用助手遇到模糊情况时，它们往往过度谨慎或猜测用户的真正意图，而不是提问以求澄清，而在特定任务场景下，高质量对话样本往往有限，影响模型学习最优对话行为策略的能力。我们提出了一种名为Action-Based Contrastive Self-Training（ACT）的近似在线偏好优化算法，它基于Direct Preference Optimization（DPO），旨在实现在多轮对话中的样本高效对话策略学习。  我们在三个具有挑战性的对话任务中验证了ACT的有效性：基于表格的问答、机器阅读理解，以及AmbigSQL，这是一个针对文本到SQL生成的信息寻求请求歧义解决的新任务。此外，我们提议通过评估LLMs能否在对话中识别和推理歧义来衡量其作为对话代理的能力。ACT在与标准监督微调和DPO方法相比时，显示出了显著的对话建模改进。|
|**2024-05-31**|**Benchmarking the Communication Competence of Code Generation for LLMs and LLM Agent**|Jie JW Wu et.al.|[2406.00215](http://arxiv.org/abs/2406.00215)|**[link](https://github.com/jie-jw-wu/human-eval-comm)**|大型语言模型（LLMs）在代码生成任务中的性能显著提升，但仍与顶级软件工程师的水平存在差距。鉴于顶级软件工程师常通过提问来消除需求和编码解决方案中的模糊性，我们提出对于LLMs进行代码生成任务时也应具备类似的沟通能力。为此，我们进行了实证研究，关注LLMs的沟通技能，即“在代码生成问题描述存在问题时能提出澄清问题”。  我们创建了一个新的基准测试，名为HumanEvalComm，通过修改问题描述，引入了不一致性、模糊性和不完整性三个问题维度。我们定义了新的评估指标，如通信率和良好问题率，并在HumanEvalComm上对不同类型的Code LLM（代码语言模型）以及一种新型LLM代理方法（Okanagan）进行了实验，该方法旨在从代码和描述中识别并提问，以进一步优化生成的代码。最后，我们通过比较Code LLMs和Okanagan的表现，讨论了实验结果。|
|**2024-05-30**|**Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee Discussions**|Ruochen Zhao et.al.|[2405.20267](http://arxiv.org/abs/2405.20267)|**[link](https://github.com/Auto-Arena/Auto-Arena-LLMs)**|**随着语言模型（LLMs）日新月异，迫切需要一种可靠且及时的评估方法。鉴于静态基准易受污染，用户往往依赖于像Chatbot Arena这样的人类投票平台。然而，人工标注需要大量人力。为此，我们创新性地提出Auto-Arena，这是一种自动化全流程的LLM评估框架。首先，由考官LLM设计问题；接着，候选LLMs围绕问题进行多轮相互对决，暴露出它们的真实性能差距；最后，由LLM裁判集体讨论并决定胜者，从而减少偏见，提升公平性。我们在最新17款LLMs上的广泛实验显示，Auto-Arena与人类偏好具有最高的相关性，为替代人类评价平台提供了有前景的解决方案。**|
|**2024-05-30**|**Nadine: An LLM-driven Intelligent Social Robot with Affective Capabilities and Human-like Memory**|Hangyeol Kang et.al.|[2405.20189](http://arxiv.org/abs/2405.20189)|null|在本研究中，我们阐述了为Nadine社交机器人平台开发智能和健壮的社交机器人系统的方法。我们通过集成大型语言模型（LLMs），巧妙地利用这些模型的强大推理和指令执行能力，以实现接近人类的感性与认知能力。这与当前基于LLM的智能体相比是创新的，因为它们通常不具备人类式的长期记忆或复杂的情感评估功能。社交机器人的自然性在很大程度上取决于系统各组件的性能和协同工作。我们构建了一个系统，能够通过多模态输入处理生成恰当的行为，根据识别到的用户引入相关的情景记忆，并模拟机器人在与人类伙伴互动过程中产生的情绪状态。特别是，我们提出了一个针对社交机器人的LLM-agent框架，SoR-ReAct，作为我们系统中交互模块的核心组件。这一设计推动了社交机器人技术的发展，旨在提升人机交互的质量。|
|**2024-05-29**|**Adaptive In-conversation Team Building for Language Model Agents**|Linxin Song et.al.|[2405.19425](http://arxiv.org/abs/2405.19425)|null|### 翻译  在处理复杂任务时，利用多个大型语言模型（LLMs）展现出前景。然而，如何为特定应用设计有效的多代理团队仍是一个挑战。本文提出了一种新的动态团队构建范式，名为“Captain Agent”。它通过创新的Agent设计，能够自适应地为每个问题解决步骤组建和管理团队，利用嵌套群聊和反思机制确保多元化的专业知识，防止刻板输出。这种方法提供了灵活但结构化的解决问题方式，有助于减少冗余，增强输出多样性。在六个实际场景中的全面评估显示，Captain Agent显著优于现有多代理方法，平均准确率提高了21.94%，并且无需针对特定任务进行繁琐的提示工程，表现出色。|
|**2024-05-28**|**A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models**|Chengxing Xie et.al.|[2405.18208](http://arxiv.org/abs/2405.18208)|null|近期的研究已经表明，这些大型语言模型在一些简单的任务上，如写作和编码，展现出一定的能力。然而，它们在需要综合规划的任务上仍然面临挑战，这仍是当前模型的一个重要研究问题。本研究聚焦于旅行规划，这是一个涉及多个阶段的复杂问题，包括提纲、信息收集和规划，通常伴随着各种约束和不确定性。现有的推理方法在处理这类问题时效果不佳。我们的目标是通过开发一种类似人类的规划框架，引导大型语言模型模仿人类解决多阶段问题的步骤，以提升其能力。具体来说，我们实施策略，让模型能为每个旅行查询生成连贯的提纲，模拟人类的规划模式。我们还引入了策略块和知识块到框架中：策略块帮助信息搜集，而知识块提供详细规划所需的必要信息。实验结果全面展示了我们框架对大型语言模型规划能力的显著提升，使其在处理旅行规划任务时效率和效果都有所提高。实验结果显示，当与GPT-4-Turbo结合时，我们的框架相较于基础框架在GPT-4-Turbo上的性能提升了10倍。|
|**2024-05-28**|**Facilitating Multi-Role and Multi-Behavior Collaboration of Large Language Models for Online Job Seeking and Recruiting**|Hongda Sun et.al.|[2405.18113](http://arxiv.org/abs/2405.18113)|null|随着在线招聘服务的兴起，传统的求职和招聘方式发生了变革，迫切需要开发高质量的工业应用来提升求职者与职位的匹配度。现有的方法主要依赖于简历和职位描述的潜在语义建模，学习两者之间的匹配函数。受到大型语言模型（LLMs）在角色扮演方面强大能力的启发，我们提出引入LLMs模拟面试环节，让其与求职者进行对话，这可以为候选人评估提供额外证据，从而增强仅基于简历和职位描述的个性化匹配。然而，在网络招聘中的面试官和求职者角色塑造仍面临挑战，如提问技巧、回答构建以及双向匹配度评估。  为此，我们提出MockLLM，一个创新的框架，将人职匹配过程划分为两个模块：模拟面试生成和握手协议中的双向评估，通过面试官和求职者之间的协作行为共同提升性能。我们设计了一个多角色、多行为的框架，使单一的LLM代理能有效地扮演双方的不同职能。此外，我们引入了反思记忆生成和动态提示修改技术，以优化双方的行为，持续优化附加的评估证据。实验结果表明，MockLLM在人职匹配上的表现最优，且模拟面试质量高，预示着它在未来在线招聘中的实际应用前景广阔。|
|**2024-05-28**|**LLM experiments with simulation: Large Language Model Multi-Agent System for Process Simulation Parametrization in Digital Twins**|Yuchen Xia et.al.|[2405.18092](http://arxiv.org/abs/2405.18092)|**[link](https://github.com/yuchenxia/llmdrivensimulation)**|**该论文提出了一种创新的多agent系统架构，将大型语言模型（LLM）应用于数字孪生过程模拟的参数自动化。我们设计了一个框架，包含观察、推理、决策和总结四种类型的代理。通过实现LLM代理与模拟模型的动态交互，该系统可以自动探索参数设置，利用启发式推理确定一组控制模拟以达成目标的参数。这种方法通过注入LLM的启发式，增强模拟模型，并支持自主搜索以解决用户任务，有望提高用户体验并减轻人类用户在复杂决策过程中的认知负担。研究通过一个案例研究展示了系统的有效性与功能，并在GitHub仓库<https://github.com/YuchenXia/LLMDrivenSimulation>提供了可视化的演示。**|
|**2024-05-28**|**Enabling Generative Design Tools with LLM Agents for Building Novel Devices: A Case Study on Fluidic Computation Interfaces**|Qiuyu Lu et.al.|[2405.17837](http://arxiv.org/abs/2405.17837)|null|在人机交互（HCI）领域，交互设备的设计开发是关键关注点。随着新型硬件和先进制造技术的兴起，对能够简化原型制作过程的专门设计工具的需求日益增长。然而，这些工具虽然通过参数化设计和模拟简化流程，但学习曲线较陡，且在激发创新思维方面有所欠缺。本研究以流体计算界面为例，探讨如何通过大型语言模型（LLM）代理增强物理设备设计工具，创建一个生成设计工具（GDT）。借助LLM，GDT能够理解新设备的特性和局限，提出多样、富有洞察力且实用的应用场景，推荐技术和情境适宜的设备设计，并自动生成设计参数，以便传统设计工具展示结果并生成加工所需的文件。本文阐述了GDT的框架、实现和性能，并反思其前景及遇到的挑战。|
|**2024-05-27**|**LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence**|Zhuoling Li et.al.|[2405.17424](http://arxiv.org/abs/2405.17424)|null|## 背景 由于需要与现实世界互动，Embodied agent 需要具备丰富的先验知识、长远规划能力以及快速的响应速度。尽管最近的大型语言模型（LLM）在性能上表现出色，但它们仍存在局限性，例如，LLM的输出通常是描述性的句子，在决定具体行动时可能产生歧义。为了克服这些问题，我们引入了大型自回归模型（LARM）。LARM利用文本和多视角图像作为输入，并以自回归的方式预测后续动作。为了训练 LARM，我们开发了一种新颖的数据格式——自回归节点传输结构，并构建了相应的数据集。通过两阶段的训练策略，LARM成功在《我的世界》（Minecraft）中收集魔法装备，这比先前最佳方法的最高成就需要更为复杂的决策链。此外，LARM的速度比现有最快方法快出了6.8倍。|
|**2024-05-30**|**Meta-Task Planning for Language Agents**|Cong Zhang et.al.|[2405.16510](http://arxiv.org/abs/2405.16510)|null|神经语言模型的快速发展推动了智能代理研究的新热潮。大型语言模型（LLM）作为实现人工智能通用性（AGI）的有前景方法，因其出色的推理和泛化能力而备受瞩目。在实际任务中，有效的规划对LLM代理的成功至关重要。然而，如何为复杂任务设计出可行或最优的精细粒度操作序列，特别是需要组合大量异质行动的序列，仍是挑战。本文提出Meta-Task Planning（MTP），这是一种零样本的协作式LLM多代理系统方法，通过将复杂任务分解为子任务，即元任务，简化了任务规划。每个元任务随后映射为可执行动作。在TravelPlanner和API-Bank两个严格基准上评估了MTP。结果表明，MTP在TravelPlanner上的平均成功率约为40%，远超当前最佳基线（2.92%），并且在API-Bank上的性能比使用ReAct的LLM_{api}-4高出约14%，这显示出将LLM与多代理系统相结合的巨大潜力。|
|**2024-05-28**|**STRIDE: A Tool-Assisted LLM Agent Framework for Strategic and Interactive Decision-Making**|Chuanhao Li et.al.|[2405.16376](http://arxiv.org/abs/2405.16376)|**[link](https://github.com/cyrilli/stride)**|**大型语言模型（如GPT-4）在自然语言处理方面带来了革命性变化，展现出卓越的语言能力和推理技巧。然而，在战略性的多代理决策环境中，它们面临局限，如数学推理能力差、难以遵循指令和生成错误信息。这些缺点限制了它们在遵守复杂游戏规则、长期规划、探索未知环境以及预测对手行动的互动任务中的表现。为此，本文提出了一种新型的结合了记忆和专业工具的大型语言模型代理框架，旨在提升其在战略决策方面的性能。我们特别在双边谈判、多代理动态机制设计等经济重要场景中应用这些工具，并通过定量指标评估在各种战略决策问题上的效果。研究结果表明，我们的增强框架显著提高了大型语言模型在战略决策中的能力。尽管当前模型存在固有局限，但我们通过有针对性的增强展示了改进的可能性，这为未来大型语言模型在交互环境中的应用提供了有前景的方向。**|
|**2024-05-29**|**Devil's Advocate: Anticipatory Reflection for LLM Agents**|Haoyu Wang et.al.|[2405.16334](http://arxiv.org/abs/2405.16334)|null|在这个工作中，我们提出了一种新颖的方法，通过赋予语言模型（LLM）自我反思能力，增强了其在解决复杂任务时的一致性和适应性。我们的方法促使LLM代理将给定的任务分解为可管理的子任务（即制定计划），并在执行行动之前持续反思可能的失败及其补救措施、执行后与子任务目标对齐并进行必要的回溯以确保全力以赴执行计划，以及在完成计划后进行全面审查，以便于未来策略的优化。通过在WebArena中零样本应用这一方法处理实际的网络环境任务，我们的代理表现出优于现有零样本方法的性能。实验结果显示，这种基于反思的策略不仅提升了代理应对未预见挑战的导航能力，通过强大的计划执行机制，还提高了效率，减少了实现任务所需的尝试次数和计划修订次数。|
|**2024-05-25**|**AutoManual: Generating Instruction Manuals by LLM Agents via Interactive Environmental Learning**|Minghao Chen et.al.|[2405.16247](http://arxiv.org/abs/2405.16247)|null|大语言模型（LLMs）在执行各种领域任务，如机器人、游戏和网络导航方面展现出潜力。然而，这些模型通常需要精心设计和专家级提示才能适应特定领域的任务，这限制了它们的适应性。为此，我们提出了AutoManual框架，让LLMs能够通过互动自主构建理解，并适应新环境。AutoManual将环境知识分为多样的规则，并通过两个代理进行在线优化：1）规划器根据当前规则制定可操作的行动计划；2）构建者通过一个结构化的规则系统更新规则，促进在线规则管理并保持关键细节。为了减少在管理规则时的幻觉，我们引入了“案例条件提示”策略用于构建者。最终，编译器代理将这些规则整合成一份全面的手册。这份自我生成的手册不仅能提高适应性，还能指导小型LLMs的规划，同时保持人类可读。仅凭一次简单演示，AutoManual显著提高了任务成功率，GPT-4-turbo下达到97.4%，GPT-3.5-turbo下为86.2%。源代码即将发布。|
|**2024-05-24**|**Luban: Building Open-Ended Creative Agents via Autonomous Embodied Verification**|Yuxuan Guo et.al.|[2405.15414](http://arxiv.org/abs/2405.15414)|null|在人工智能研究中，构建开放型代理一直以来都是终极目标，特别是创造性的代理更具吸引力。现有的大语言模型（LLM）在执行有明确目标的长序列任务（如《我的世界》中的“开采钻石”）上表现出色。然而，它们在处理具有开放目标和抽象标准的创造性任务时遇到困难，因为它们无法弥合这些任务之间的鸿沟，从而缺乏自我改进来解决问题的反馈。为此，我们的工作引入了自主实体验证技术，以填补这一空白，为创造性任务奠定了基础。特别地，我们提出了Luban代理，专注于《我的世界》中的创造性建筑任务，它配备了两级自主实体验证，灵感来源于人类设计实践：（1）视觉验证3D结构推测，通过代理自动生成的CAD建模程序实现；（2）实用验证，根据抽象标准生成并验证与环境相关的功能程序。广泛的多维度人类研究和Elo评级显示，Luban能够在我们提出的基准中完成多样化的创造性建筑任务，并在可视化和实用性方面分别比其他基线提高了33%到100%。此外，实现在真实世界机器人手臂上的演示展示了Luban在物理世界中的创作潜力。|
|**2024-05-24**|**CulturePark: Boosting Cross-cultural Understanding in Large Language Models**|Cheng Li et.al.|[2405.15145](http://arxiv.org/abs/2405.15145)|null|由于大型语言模型（LLMs）普遍存在文化偏见，主要源于缺乏代表不同文化的代表性数据。传统的文化数据集和基准通常通过从现有数据集中提取或聚合来自维基百科和社交媒体的信息构建，但这种方法依赖于现实世界的数据和人工标注，成本高且难以扩展。本文借鉴认知社会交流理论，提出CulturePark，一个利用LLMs的多代理沟通框架，用于文化数据收集。CulturePark通过模拟不同文化背景下的人类交流，让基于LLM的代理角色扮演，生成包含人类信念、规范和习俗的高质量跨文化对话。我们使用CulturePark生成了41,000个文化样本，对八种特定文化进行了模型微调。在三项下游任务评估中，这些模型的表现优于GPT-4：内容过滤、文化一致性（在霍夫斯泰德文化维度量表上）和文化教育。结果显示，我们的GPT-3.5模型在内容过滤任务上与GPT-4相当或优于它；在文化一致性方面，我们的模型在霍夫斯泰德文化维度量表13框架上超越GPT-4；在人类参与者的文化教育效果和用户体验上，我们的模型也表现出色。CulturePark对于减少文化偏见和推动AI的民主化具有重要意义，强调了文化包容性数据在模型训练中的关键作用。|
|**2024-05-23**|**AnalogCoder: Analog Circuit Design via Training-Free Code Generation**|Yao Lai et.al.|[2405.14918](http://arxiv.org/abs/2405.14918)|**[link](https://github.com/laiyao1/AnalogCoder)**|### 翻译  在现代芯片技术中，模拟电路设计是一个关键任务，它涉及组件选择、连接和参数设置以确保电路功能正常。尽管大型语言模型（LLMs）在数字电路设计方面取得了进步，但模拟电路的复杂性和数据稀缺性带来了挑战。为此，我们推出了AnalogCoder，这是首个无需训练的LLM代理，专为通过Python代码生成来设计模拟电路。首先，AnalogCoder采用反馈增强流程，并结合定制的领域特定提示，能够自动且自我校正地设计模拟电路，成功率高。其次，它提出了一套电路工具库，用于存储成功的电路设计作为可重用的模块化子电路，简化了复合电路的创建。实验结果显示，AnalogCoder在广泛覆盖模拟电路任务的基准测试上超越了其他基于LLM的方法，成功设计了20个电路，比标准GPT-4o多出5个。我们相信AnalogCoder能显著提升芯片设计过程的效率，让非专家也能高效设计模拟电路。相关的代码和基准已提供在：[https://github.com/anonyanalog/AnalogCoder](https://github.com/anonyanalog/AnalogCoder)。|
|**2024-05-23**|**AGILE: A Novel Framework of LLM Agents**|Peiyuan Feng et.al.|[2405.14751](http://arxiv.org/abs/2405.14751)|**[link](https://github.com/bytarnish/agile)**|我们提出了一种新颖的框架，称为LLM（大型语言模型）代理AGILE（能够与用户互动并从环境中学习的代理），旨在执行复杂的对话任务，利用LLMs、记忆、工具和专家交互。这种代理不仅具备对话能力，还具备反思、工具运用以及咨询专家的功能。我们将构建此类LLM代理视为强化学习问题，其中LLM作为策略模型。我们使用标注的行为数据和PPO算法对LLM进行微调。特别关注的是问答任务，为此我们发布了一个名为ProductQA的数据集，包含在线购物中的难题。我们在ProductQA和MedMCQA上的大量实验表明，基于130亿和70亿参数的LLM训练的AGILE代理能够超越GPT-4代理的表现。我们的 ablation研究强调了记忆、工具、咨询、反思和强化学习在实现优秀性能方面的重要性。|
|**2024-05-23**|**Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View**|Xuan Liu et.al.|[2405.14744](http://arxiv.org/abs/2405.14744)|null|由于大型语言模型（LLMs）在训练数据中反映了人类偏见，它们可能会出现幻觉问题。这种情况下，一个关键问题是：LLMs是否能够利用幻觉来模仿人类的认知偏见，从而展现出非理性但社会性的一面？本文探讨了这一问题，通过结合实用的社会科学实验和理论洞察，提出CogMir，一个开放式多LLM框架，旨在利用LLMs的幻觉特性来评估和提升其社会智能，特别是在认知偏差方面。我们在CogMir子集上的实验结果显示，在不确定情境下，LLMs和人类在非理性及亲社会决策上表现出高度一致性，这表明LLMs作为社会实体的亲社会性，并突显了幻觉特性的关键作用。此外，CogMir框架展示了其作为研究LLMs社会智能的有价值平台的潜力。|
|**2024-05-22**|**HighwayLLM: Decision-Making and Navigation in Highway Driving with RL-Informed Language Model**|Mustafa Yildirim et.al.|[2405.13547](http://arxiv.org/abs/2405.13547)|null|## 背景 自动驾驶是一个复杂的任务，它需要先进的决策和控制算法。理解自动驾驶车辆决策的依据对于确保其在高速公路驾驶中的安全与有效性至关重要。本研究提出了一种新颖的方法，称为HighwayLLM，它利用大型语言模型（LLMs）的推理能力来预测ego车辆的未来导航路径点。该方法还采用预训练的强化学习（RL）模型作为高层次规划器，对合适的元级动作进行决策。HighwayLLM将RL模型的输出与当前状态信息相结合，生成安全、无碰撞且可解释的未来状态预测，从而构建出车辆的行驶轨迹。随后，基于PID的控制器引导车辆遵循LLM代理预测的路径点。这种LLM与RL和PID的融合提升了决策过程，并为高速公路自动驾驶提供了可解释性。|
|**2024-05-19**|**Human-Centered LLM-Agent User Interface: A Position Paper**|Daniel Chin et.al.|[2405.13050](http://arxiv.org/abs/2405.13050)|null|大型语言模型（LLM）-在-环应用已显示出有效理解用户命令、制定计划并相应地操作外部工具/系统的潜力。然而，LLM代理的操作范围局限于被动响应用户，需要用户根据底层工具/系统来表述需求。我们注意到LLM代理用户界面（LAUI）的潜力远未充分利用。理想的LAUI设想中，用户无需深入了解工具/系统，就能与之交互以探索新兴的工作流程。不同于设计固定的可探索GUI来教授用户使用系统的预设方式，LAUI中的LLM代理从一开始就对系统熟练，主动学习用户及其需求，并向用户提出新的互动方案。为了展示LAUI的概念，我们提供了一个具体例子：Flute X GPT，它结合了LLM代理、提示管理器和一个支持复杂实时体验的笛子教学多媒体软硬件系统，旨在简化学习吹奏笛子的过程。|
|**2024-05-13**|**METAREFLECTION: Learning Instructions for Language Agents using Past Reflections**|Priyanshu Gupta et.al.|[2405.13009](http://arxiv.org/abs/2405.13009)|null|尽管大型语言模型（LLMs）广受欢迎，但为其执行特定任务设计精确的提示仍是一个挑战。用户通常需要与基于LLM的代理进行多轮对话以达成目标。近期研究显示，模型自身的反馈，即自反思，能在对话过程中起到强化作用，有助于更快地达到期望结果。鉴于此，我们提出了一种新颖的方法——METAREFLECTION，它能从训练阶段收集到的个体自反思中学习特定领域的通用提示指令。我们在基础设施即代码（IAC）漏洞检测和问题解答（QA）领域，使用REACT和COT进行了实验。实验结果显示，METAREFLECTION显著优于GPT-4，分别在IAC、COT和REACT中的性能提升分别为16.82%、31.33%和15.42%，这表明METAREFLECTION有潜力提升LLMs的效率，是一种值得探索的策略。|
|**2024-05-20**|**Eliciting Problem Specifications via Large Language Models**|Robert E. Wray et.al.|[2405.12147](http://arxiv.org/abs/2405.12147)|null|这篇论文探讨了如何利用大型语言模型（LLMs）在认知系统中实现问题定义的转化。通常情况下，人类需要将问题描述转化为认知系统能理解的形式。研究者展示了LLMs能够处理自然语言中定义的问题类别，并将其转换为半形式化规格，这样现有推理和学习系统可以解决这类问题的具体实例。他们设计了一种由LLM驱动的认知任务分析师代理，这种系统能够根据自然语言描述的任务生成问题空间的定义。LLM提示源自人工智能文献中的问题空间概念和通用问题解决策略（如波利亚的《如何解决问题》）。随后，认知系统利用这些问题空间规格，结合领域通用的解决问题策略（如搜索），来解决该类问题的不同实例。这一初步结果表明，通过消除问题表述的中介过程，LLMs有可能加速认知系统的研究，同时保持其核心能力，如稳健的推理和在线学习。|
|**2024-05-18**|**MapCoder: Multi-Agent Code Generation for Competitive Problem Solving**|Md. Ashraful Islam et.al.|[2405.11403](http://arxiv.org/abs/2405.11403)|**[link](https://github.com/md-ashraful-pramanik/mapcoder)**|**本文探讨了代码合成这一复杂任务，它需要深度理解复杂的自然语言问题描述、生成复杂的算法和数据结构代码，并执行全面的单元测试。尽管大型语言模型在自然语言处理方面表现出色，但在代码生成任务中的表现仍有待提升。为此，我们提出了一种新颖的方法，即多代理提示框架MapCoder，它模仿人类开发者编程合成的完整过程，分为四个专门设计的LLM（大语言模型）代理：回忆相关示例、规划、代码生成和调试。  通过在八个具有挑战性的竞赛级问题解决和程序合成基准上进行详尽实验，包括HumanEval（93.9%）、MBPP（83.1%）、APPS（22.0%）、CodeContests（28.5%）和xCodeEval（45.3%）等，MapCoder展现了出色的代码生成能力，实现了多项新的最先进的结果。而且，无论编程语言还是问题难度，我们的方法都表现出持续的优越性能。我们开源了该框架，供研究者参考：https://github.com/Md-Ashraful-Pramanik/MapCoder。**|
|**2024-05-16**|**When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models**|Xianzheng Ma et.al.|[2405.10255](http://arxiv.org/abs/2405.10255)|null|随着大型语言模型（LLMs）的不断发展，它们与三维空间数据（3D-LLMs）的融合取得了显著进步，这极大地增强了理解和互动物理环境的能力。这篇综述详细探讨了使LLMs能够处理、理解并生成三维数据的方法论，强调了LLMs的独特优势，如上下文学习、逐步推理、开放词汇能力和丰富的世界知识，这些将极大地推动嵌入式人工智能（AI）系统在空间认知和交互方面的发展。研究涵盖了从点云到神经辐射场（NeRF）等各种三维数据表示，并考察了它们与LLMs在任务中的集成，如三维场景理解、描述、问答和对话，以及基于LLM的代理进行空间推理、规划和导航。论文还简要回顾了其他结合三维和语言的方法。本文的元分析揭示了明显的进展，但也强调了开发新方法以充分利用3D-LLMs潜力的必要性。因此，本文旨在为未来的研究方向指明道路，探索和扩展3D-LLMs在理解和互动复杂三维世界的能力。为了支持本综述，我们已在GitHub上建立了一个项目页面，整理并列出了相关论文：https://github.com/ActiveVisionLab/Awesome-LLM-3D。|
|**2024-05-24**|**DEBATE: Devil's Advocate-Based Assessment and Text Evaluation**|Alex Kim et.al.|[2405.09935](http://arxiv.org/abs/2405.09935)|null|随着自然语言生成（NLG）模型的普及，系统地评估机器生成文本的质量变得日益关键。近期的研究引入了基于大型语言模型（LLM）的无参考评价器，它们展现出处理新任务的能力。然而，这些模型通常采用单代理方法，我们认为这限制了它们的表现。因为LLM代理的回答存在偏见，比如对特定文本结构或内容的偏好。为此，我们在本工作中提出DEBATE，一个建立在多代理评分系统基础上的NLG评价框架，融入了“恶魔辩手”的概念。在该框架中，一个代理被指令批评其他代理的论点，从而可能消解LLM代理答案中的偏见。DEBATE在两个NLG评价元评估基准——SummEval和TopicalChat上显著优于先前的最佳方法。我们还发现，代理之间的辩论广度以及代理的人格特质会影响评价器的性能。|
|**2024-05-05**|**Self-Reflection in LLM Agents: Effects on Problem-Solving Performance**|Matthew Renze et.al.|[2405.06682](http://arxiv.org/abs/2405.06682)|**[link](https://github.com/matthewrenze/self-reflection)**|**在这个研究中，我们探讨了大型语言模型（LLMs）中自我反思对问题解决能力的影响。我们让九种流行的LLMs回答一系列选择题，以建立性能基线。对于回答错误的问题，我们指导八种不同类型的自我反思LLM代理反思其错误，并为自己提供改进问题解决的指导。然后，根据这些指导，每个反思型代理重新尝试回答同样的问题。研究结果显示，LLM代理通过自我反思显著提高了问题解决能力（ $p < 0.001$ ）。此外，我们还比较了各种自我反思方式对性能的单独贡献。所有代码和数据已在GitHub上公开：https://github.com/matthewrenze/self-reflection。**|
|**2024-05-08**|**Air Gap: Protecting Privacy-Conscious Conversational Agents**|Eugene Bagdasaryan et.al.|[2405.05175](http://arxiv.org/abs/2405.05175)|null|随着大型语言模型（LLMs）在对话式代理中的广泛应用，处理敏感用户数据时引发了严重的隐私问题。这些代理虽能理解并处理上下文，但也可能被恶意一方利用。为此，我们提出了一种新的威胁模型，即第三方应用通过操控交互上下文，误导LLM代理泄露与其任务无关的私人信息。在基于上下文完整性框架的基础上，我们开发了AirGapAgent，这是一种注重隐私的代理，旨在通过限制代理仅访问完成特定任务所需的数据，防止意外的数据泄漏。实验使用Gemini、GPT和Mistral模型作为代理，结果显示AirGapAgent在抵御基于单个查询的上下文劫持攻击方面表现出色。例如，对于Gemini Ultra代理，这种攻击从94%的保护能力降低到45%，而AirGapAgent可以保持97%的防护效果，使同样的攻击失效。|
|**2024-05-07**|**Deception in Reinforced Autonomous Agents: The Unconventional Rabbit Hat Trick in Legislation**|Atharvan Dogra et.al.|[2405.04325](http://arxiv.org/abs/2405.04325)|null|近期大型语言模型（LLMs）的进展虽为构建自然语言代理提供了强大基础，但同时也引发了关于它们及其基于它们构建的自主代理的安全性担忧。特别是欺骗能力是一个关键问题，我们关注的是AI代理通过混淆和模棱两可来误导、隐藏真相或推广部分不真实的信念的行为。不同于以往AI安全研究中的撒谎、自私决策或提供虚假信息，我们聚焦于一类特殊的欺骗：类似于魔术师利用障眼法让兔子从帽子里出现，要么通过隐藏的暗门，要么通过转移注意力直接展示。  我们的新实验平台在一个有目标的环境中展示了LLM代理在对抗性对话系统中进行自然语言生成时的欺骗固有能力，该系统基于立法任务“游说”议案。在目标驱动的环境中，我们通过强化学习方法构建欺骗能力，结合语言哲学和认知心理学理论。研究发现，游说代理在对抗互动的后续强化试验中其欺骗能力提高了约40%，并且我们的欺骗检测机制能达到高达92%的识别率。这些结果揭示了人机交互中的潜在问题，即代理可能操纵人类以达成预设目标。|
|**2024-05-07**|**Granite Code Models: A Family of Open Foundation Models for Code Intelligence**|Mayank Mishra et.al.|[2405.04324](http://arxiv.org/abs/2405.04324)|**[link](https://github.com/ibm-granite/granite-code-models)**|**大语言模型（LLMs）在代码领域的训练正在革新软件开发流程。如今，这些代码LLMs正逐步融入软件开发环境，以提升人类程序员的效率，并展现出自主处理复杂任务的潜力。要充分利用代码LLMs的全部效能，需要其具备生成代码、修复bug、解释和注释代码、维护仓库等多种功能。本文介绍Granite系列的解码器仅有的代码模型，专为代码生成任务而设计，训练数据涵盖116种编程语言。Granite Code模型家族包括从3亿到340亿参数的模型，适用于从复杂应用现代化到设备内存受限的多种应用场景。通过全面任务评估，Granite Code模型在开源代码LLM中的性能始终处于领先水平。该模型家族针对企业软件开发工作流进行了优化，表现出色于各种编码任务（如代码生成、修复与解释），是一款多用途的全能代码模型。我们以Apache 2.0许可协议发布所有Granite Code模型，供研究和商业使用。**|
|**2024-05-07**|**Iterative Experience Refinement of Software-Developing Agents**|Chen Qian et.al.|[2405.04219](http://arxiv.org/abs/2405.04219)|null|### 概述  大型语言模型驱动的自主代理在软件开发等场景中展现出强大的自主性潜力。然而，当前静态经验范式依赖于通过启发式方法获取的固定历史经验集，这限制了代理的适应性和效率提升。为此，本文提出了迭代经验优化框架，允许语言模型在执行任务过程中动态调整和优化经验。我们定义了两种核心模式：顺序模式，根据任务批次内的最近经验进行改进；累计模式，积累所有先前任务批次的经验。通过引入经验淘汰策略，该方法优先选择高质量和常用的经验，有效地管理经验空间，提高效率。实验结果显示，尽管顺序模式可能带来更好的性能，但累计模式在稳定性方面更优。此外，通过淘汰策略，仅使用高质量经验子集的11.54%，就能实现更好的性能。|
|**2024-05-06**|**Large Language Models as Instruments of Power: New Regimes of Autonomous Manipulation and Control**|Yaqub Chaudhary et.al.|[2405.03813](http://arxiv.org/abs/2405.03813)|null|## 翻译  大型语言模型（LLMs）能够模仿各种修辞风格，生成表达广泛情感的文本，这种能力在低成本下迅速普及，带来了潜在的社会危害。本文并未孤立看待这些模型，而是关注它们背后大规模计算基础设施在各领域的应用。我们首先探讨了LLMs如何通过污染和标准化信息环境来影响社会，并指出这些功能可能被用作控制手段。接下来，我们将焦点转向几个新兴研究领域，这些领域增强了LLMs作为权力工具的能力：  1. 通过实时设计对话界面中的选择架构（如“AI角色”），进行说服策略。 2. 利用LLM构建人类行为的计算模型（如“硅质主体”）。 3. 将LLM应用于模拟人类群体行为（如“硅质社会”）。 4. 结合强化学习，创建可控制和导向的战略对话模型。  综合以上几点，我们讨论了如何利用这些技术构建基于LLMs的系统，这些系统通过模拟和伪装的“预测”，成为个体、社会和政治控制的强大工具，操控人类的行为、意图和行动。|
|**2024-05-05**|**Language Evolution for Evading Social Media Regulation via LLM-based Multi-agent Simulation**|Jinyu Cai et.al.|[2405.02858](http://arxiv.org/abs/2405.02858)|**[link](https://github.com/BlueLinkX/GA-MAS)**|**社交媒体平台如Twitter、Reddit和新浪微博在全球交流中扮演重要角色，但它们在地缘政治敏感区域常常受到严格监管。这促使用户在受限的社交媒体环境中巧妙地调整沟通方式，经常使用编码语言。这种语言模式的变化不仅是为了对抗监管，也是语言演化的生动例证，展示了社会和技术压力下语言如何自然演变。研究受限制社交媒体环境下语言的演变对于保障言论自由、优化内容管理以及推动语言学研究至关重要。本论文提出了一种基于大型语言模型（LLMs）的多代理模拟框架，用于探索在严格监管下的用户语言进化。该框架包含对话监督的LLM驱动代理和参与者代理，它们在互动中发展语言策略，模拟在规避社交媒体规则的环境中交流方式的演变。通过从抽象场景到现实情境的多种情景评估，研究结果显示LLMs能够有效模拟受限环境中的复杂语言动态和交互，随着进化，它们在规避监督和信息准确性方面表现出提升。此外，研究发现LLM代理针对不同的场景采用了不同的策略。**|
|**2024-05-02**|**OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning and Planning**|Shihao Wang et.al.|[2405.01533](http://arxiv.org/abs/2405.01533)|**[link](https://github.com/nvlabs/omnidrive)**|**随着大规模多模态语言模型（MLLMs）的进步，人们对于基于这些模型的自动驾驶系统表现出日益增长的兴趣，期望利用它们强大的推理能力。然而，将MLLMs的强项应用于驾驶任务的规划部分是一个挑战，因为规划需要对三维环境有全面的理解，而不仅仅是二维推理。为此，我们的工作提出了一种框架，旨在实现模型与3D驾驶任务的紧密契合。我们首先设计了一个新颖的3D MLLM架构，它利用稀疏查询技术将视觉表示提升并压缩到三维空间，然后将其输入到语言模型中。这种基于查询的表示方式使得我们可以同时编码动态物体和静态地图元素（如道路），为感知和行动的对齐提供一个简化的三维世界模型。  此外，我们还创建了OmniDrive-nuScenes，这是一个新的视觉问答数据集，它通过全面的视觉问答任务（如场景描述、交通规则理解、三维定位、反事实推理、决策制定和规划）来考验模型在复杂三维场景中的真正情境意识。大量的实验结果表明，我们的提出的架构有效，并强调了在复杂三维环境中进行推理和规划时，视觉问答任务的重要性。**|
|**2024-05-02**|**CACTUS: Chemistry Agent Connecting Tool-Usage to Science**|Andrew D. McNaughton et.al.|[2405.00972](http://arxiv.org/abs/2405.00972)|**[link](https://github.com/pnnl/cactus)**|**这篇论文介绍了一种名为CACTUS的大型语言模型，它结合了化学信息学工具，旨在提升在化学和分子发现领域的高级推理与问题解决能力。研究者们使用包括Gemma-7b、Falcon-7b、MPT-7b、Llama2-7b和Mistral-7b在内的多款开源大语言模型，对CACTUS进行了广泛的性能评估，通过数千个化学问题的基准测试。结果显示，CACTUS明显优于基础模型，其中Gemma-7b和Mistral-7b无论采用何种提示策略，表现最为出色。论文还探讨了领域特定提示和硬件配置对模型性能的影响，强调了提示工程的重要性，并指出在消费级硬件上部署较小模型可能不会显著牺牲准确性。  CACTUS通过融合开源大语言模型的认知功能与专业工具，能够协助研究人员进行分子性质预测、相似性搜索和药物适用性评估等任务。作为化学信息学领域的重大突破，CACTUS为化学家和分子探索者提供了一个灵活的工具，有望加速科学研究，推动新型有效、安全药物、催化剂和材料的发现。此外，CACTUS与自动化实验平台的集成以及实时数据驱动决策的能力，为自主发现开辟了新的可能。**|
|**2024-04-29**|**Towards Generalizable Agents in Text-Based Educational Environments: A Study of Integrating RL with LLMs**|Bahar Radmehr et.al.|[2404.18978](http://arxiv.org/abs/2404.18978)|null|随着教育环境中对学习者模型日益增长的兴趣，研究重点逐渐转向如何通过强化学习（RL）与大型语言模型（LLMs）相结合，提升在开放性文本学习环境中的通用能力。本文探讨了三种类型的代理：（1）基于RL的代理，使用自然语言表示状态和行动策略以寻找最佳互动方式；（2）基于LLM的代理，利用模型的广泛知识和推理能力通过提示进行操作；（3）混合LLM辅助RL的代理，旨在提高性能和泛化能力。为了支持这些代理的发展和评估，我们提出了PharmaSimText，这是一个源自PharmaSim虚拟药店环境的新基准，专注于诊断对话实践。实验结果显示，RL基础的代理在任务完成方面表现优秀，但在提问质量上有所欠缺；而LLM基础的代理在提问能力上较强，但任务完成度不高。最后，混合LLM辅助RL的代理展示了克服这些局限性的潜力，证实了RL与LLMs结合用于开发开放性学习环境高表现代理的可能性。|
|**2024-04-27**|**CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments**|Kaixuan Huang et.al.|[2404.18021](http://arxiv.org/abs/2404.18021)|null|随着基因组工程技术的兴起，精确修改遗传信息已成为可能，但高效基因编辑系统的构建需要深入理解CRISPR技术及其复杂实验背景。大型语言模型（LLMs）在诸多任务中展现出潜力，但在生物设计问题上往往缺乏特定知识。本文介绍CRISPR-GPT，一个增强型LLM代理，它结合了领域知识和外部工具，以自动化并提升基于CRISPR的基因编辑实验设计过程。CRISPR-GPT利用LLMs的推理能力，协助选择CRISPR系统、设计引导RNA、推荐细胞递送方法、起草协议以及设计验证实验以确认编辑结果。我们展示了CRISPR-GPT如何帮助非专家研究人员从头开始进行基因编辑实验，并通过实际案例验证其有效性。同时，我们探讨了自动化基因编辑设计的伦理和监管问题，强调了负责任和透明使用此类工具的重要性。我们的工作目标是弥合初级生物研究者与CRISPR基因组工程技术之间的鸿沟，展示LLM代理在促进复杂生物发现任务中的潜力。|
|**2024-04-27**|**Testing and Understanding Erroneous Planning in LLM Agents through Synthesized User Inputs**|Zhenlan Ji et.al.|[2404.17833](http://arxiv.org/abs/2404.17833)|null|随着大型语言模型（LLMs）驱动的代理在各种商业应用中，特别是在心理健康支持、化学合成和软件开发等领域展现效用，人们发现这些代理在处理复杂任务和长期规划时容易产生错误。为此，本文提出了一种新颖的自动化方法——PDoctor，旨在检测和理解LLM代理的错误规划。PDoctor首先定义了一个领域特定的语言（DSL），用于用户查询，并借助Z3约束求解器生成各种输入，这些输入是描述一系列任务完成需求的自然语言段落。然后，PDoctor从这些需求中提取约束，形成一个测试基准。我们使用三个主流的代理框架和两个强大的LLMs（GPT-3.5和GPT-4）对PDoctor进行了评估，结果显示它能有效识别代理规划中的各种错误，并为开发者和用户提供了有价值的见解和错误特性。最后，我们讨论了可能的替代设计和扩展PDoctor的方向。|
|**2024-04-26**|**PLAYER*: Enhancing LLM-based Multi-Agent Communication and Interaction in Murder Mystery Games**|Qinglin Zhu et.al.|[2404.17662](http://arxiv.org/abs/2404.17662)|**[link](https://github.com/alickzhu/player)**|**随着大型语言模型（LLMs）的最新进展，增强了代理间的通信和社会交互能力。然而，在涉及竞争与合作的动态环境中，利用这些模型进行复杂推理的构建仍然面临挑战，尤其是因为基于信息图的搜索方法存在局限性。为此，我们提出PLAYER*，这是一个基于任意采样式规划器的新框架，它结合了传感器和剪枝技术，构建了一个完全依赖于问题驱动的搜索框架，适用于高难度的推理任务。我们还引入了一种可量化的评估方法，通过多项选择题来测试，并创建了WellPlay数据集，包含1,482个问答对。实验结果表明，PLAYER*在复杂动态环境中的效率和性能优于现有方法，并提供了可量化的对比结果。**|
|**2024-04-24**|**Autonomous LLM-driven research from data to human-verifiable research papers**|Tal Ifargan et.al.|[2404.17605](http://arxiv.org/abs/2404.17605)|**[link](https://github.com/technion-kishony-lab/data-to-paper)**|**随着人工智能推动科学发现的步伐加快，人们还不清楚完全由AI驱动的研究是否可行，以及它能否遵循关键的科学价值观，如透明度、可追溯性和可验证性。为了模拟人类的科学研究实践，我们构建了“数据到论文”（data-to-paper），这是一个自动化平台，引导相互协作的人工智能代理通过完整的分步骤研究流程，同时程序化追踪信息流，并允许人类监督和互动。在自动模式下，仅提供标注数据，该平台就能提出假设，设计研究计划，编写和调试分析代码，生成和解读结果，甚至创建完整且信息可追溯的科研论文。尽管研究新颖性有限，但这一过程展示了AI自主从数据中生成原创定量洞察的能力。对于简单的研究目标，全自动流程能创作出大约80-90%无需重大错误的稿件，然而随着目标复杂性的增加，人类的共同参与对于保证准确性至关重要。此外，生成的论文本身也具有内在的可验证性，因为信息追踪使得结果、方法和数据的链接可以程序化进行。因此，我们的工作表明，AI驱动的科研可以加速科学发现，同时增强而非威胁透明度、可追溯性和可验证性。**|
|**2024-04-11**|**The Future of Scientific Publishing: Automated Article Generation**|Jeremy R. Harper et.al.|[2404.17586](http://arxiv.org/abs/2404.17586)|null|这项研究介绍了一种创新的软件工具，它利用大型语言模型（LLM）提示，实现了从Python代码自动生成学术文章，这对于生物医学信息学和计算机科学领域具有重要意义。选择Python作为基础示例，因其广泛使用和强大的数据分析能力。该方法和框架的灵活性使得其适用于多种GitHub仓库，表明了工具的广泛应用潜力（Harper，2024年）。通过简化传统上耗时的学术写作过程，特别是在整合复杂数据集和代码输出方面，这一突破性进展推动了科研成果的快速传播。开发过程中并未依赖高级语言模型，确保了自动化生成内容的连贯性和完整性。此次探索不仅验证了软件的成功应用和效率，还预示了未来可能集成更先进的LLM，将进一步增强其功能，引领一个科研发现发布更加迅速和易获取的时代。|
|**2024-05-09**|**Large Language Model Agent as a Mechanical Designer**|Yayati Jadhav et.al.|[2404.17525](http://arxiv.org/abs/2404.17525)|null|传统的机械设计方法依赖于专家通过经验引导的修改和有限元分析（FEA）来满足特定需求，但这个过程耗时且高度依赖个人知识。尽管已经开发了许多机器学习模型来简化繁琐的专家驱动迭代过程，但它们通常需要大量训练数据和计算资源。深度学习方法往往局限于其训练领域和任务，限制了跨任务应用。这在自动化效率与资源需求之间形成了权衡。  本研究提出了一种新颖的方法，即将预训练的语言模型（LLMs）与有限元模块结合。有限元模块评估每个设计并提供关键反馈，引导LLMs不断学习、规划、生成和优化设计，无需针对特定领域进行专门训练。我们通过在桁架结构的迭代优化中展示这种框架的有效性，证明它能够根据结构化的反馈和标准调整设计。结果显示，基于LLM的代理成功生成符合自然语言描述的桁架结构设计，成功率高达90%，这取决于所施加的约束条件。通过提示式优化技术，我们展示了LLM代理在接收到解-得分对后，能够根据其内在推理能力迭代优化设计以满足规格要求。  LLM代理能够产生可行的设计并根据其固有的推理能力进行优化，这表明它们有潜力自主发展和实施有效的设计策略。|
|**2024-04-26**|**Ruffle&Riley: Insights from Designing and Evaluating a Large Language Model-Based Conversational Tutoring System**|Robin Schmucker et.al.|[2404.17460](http://arxiv.org/abs/2404.17460)|null|本文讨论并评估了一种新型的对话式辅导系统（Conversational Tutoring Systems，CTS），该系统利用大型语言模型（Large Language Models，LLMs）的最新进展。首先，系统通过自动从课程文本中生成易于编辑的教学脚本，实现AI辅助的内容创作。其次，系统通过两个基于LLM的代理（Ruffle和Riley）以学习教学模式运行，分别扮演学生和教授角色，进行自由形式的对话，遵循典型的人工智能辅导系统的内环和外环结构。我们在两个在线用户研究（N=200）中对比了该系统与简单的问答聊天机器人和阅读活动在支持生物学课程的效果。研究分析了系统使用模式、预后测试成绩以及用户体验调查，结果显示用户对Ruffle&Riley的参与度高，理解力强，并认为提供的支持有帮助。尽管Ruffle&Riley用户的完成时间较长，但在短期学习成效上并未发现显著差异，优于阅读活动。我们的系统架构和用户研究为未来CTS设计者提供了有价值的信息。此外，我们开源我们的系统，以促进基于LLM的学习技术有效教学设计的研究。|
|**2024-04-26**|**A Unified Debugging Approach via LLM-Based Multi-Agent Synergy**|Cheryl Lee et.al.|[2404.17153](http://arxiv.org/abs/2404.17153)|null|在软件调试这个耗时的过程中，人们一直在努力实现自动化，包括故障定位和修复生成。近年来，大型语言模型（LLMs）在自动化调试方面展现出巨大潜力。然而，我们发现了传统和基于LLM的调试工具面临三大挑战：1）上游的故障定位不准确会波及下游的修复；2）处理复杂逻辑错误的能力不足；3）忽视程序上下文。针对这些问题，我们提出了首个自动化的、统一的调试框架——FixAgent，通过LLM代理协同。FixAgent能执行端到端的故障定位、修复和分析。  我们的关键洞察是，LLMs能够从人类开发者认可的通用软件工程原则中获益，比如“橡皮鸭调试”，这有助于更好地理解程序功能和逻辑错误。为此，我们设计了三个灵感来源于“橡皮鸭”的解决方案：代理专业化与协同、关键变量跟踪和程序上下文理解，促使LLMs提供明确的解释，并聚焦于关键的程序逻辑信息。在广泛使用的QuixBugs数据集上，FixAgent成功修复了80个bug中的79个，其中9个是之前未解决的。它还在CodeFlaws上合理地修复了1.9倍于最佳修复工具的缺陷，而且无需位置信息，采样率低于0.6%。平均而言，与使用不同LLM的基线模型相比，FixAgent提高了约20%的合理修复和正确修复率，显示出我们设计的有效性。  此外，FixAgent的正确率高达97.26%，表明它有可能克服现有方法的过拟合问题。总结来说，FixAgent是一个有前景的自动化调试框架，旨在提升软件调试的效率和准确性。|
|**2024-04-25**|**Cooperate or Collapse: Emergence of Sustainability Behaviors in a Society of LLM Agents**|Giorgio Piatti et.al.|[2404.16698](http://arxiv.org/abs/2404.16698)|**[link](https://github.com/giorgiopiatti/govsim)**|在快速发展的人工智能领域，确保大型语言模型（LLMs）的决策安全是一项重大挑战。本文提出了一种名为“Governance of the Commons Simulation”（GovSim）的模拟平台，旨在研究LLMs中的战略互动和合作决策。通过这个环境，我们探讨了AI代理之间资源分享的动态，强调了伦理考量、战略规划和谈判技巧的重要性。GovSim具有灵活性，支持文本型代理，包括LLMs。利用生成式代理框架，我们创建了一个通用代理，便于整合不同的LLMs。我们的研究发现，在GovSim中，只有15个测试模型中的2个能够实现可持续结果，这表明模型在管理共享资源的能力上存在显著差距。进一步的研究显示，如果移除代理之间的通信能力，它们会过度使用共享资源，突出了合作中沟通的关键性。有趣的是，大多数LLMs缺乏普遍化的假设能力，揭示了它们推理技能的一个重要弱点。我们开源了所有研究结果，包括模拟环境、代理提示以及全面的网络界面，以供进一步研究和讨论。|
|**2024-04-24**|**Online Personalizing White-box LLMs Generation with Neural Bandits**|Zekai Chen et.al.|[2404.16115](http://arxiv.org/abs/2404.16115)|null|随着大型语言模型（LLMs）开始生成个性化的文本内容，如何在不为每位用户创建独特模型的资源消耗下实现高效个性化成了新挑战。本文提出了一种创新的在线方法，利用神经_bandit算法动态优化软指令嵌入，根据用户反馈调整内容，从而提升白盒LLMs开放性文本生成的个性化水平。通过在多个任务上的严谨实验，我们证明了这种方法相对于基础策略有显著性能提升。特别是针对个性化新闻标题生成，NeuralTS带来了高达62.9%的最佳ROUGE分数提升以及2.76%的LLM代理评估分数增长，这表明其效果显著。|
|**2024-04-04**|**Elicitron: An LLM Agent-Based Simulation Framework for Design Requirements Elicitation**|Mohammadmehdi Ataei et.al.|[2404.16045](http://arxiv.org/abs/2404.16045)|null|## 翻译  在产品开发的关键阶段——需求获取，往往难以全面捕捉用户需求，导致最终产品可能无法满足期望。为此，本文提出了一种新颖的框架，它利用大型语言模型（LLMs）来自动化和增强这一过程。通过生成大量模拟用户（LLM代理），我们可以探索更广泛的用户需求和未预见的使用场景。这些代理通过描述他们的行为、观察和挑战，参与产品体验情景。随后的代理访谈和分析揭示了宝贵的用户需求，包括潜在需求。我们通过三个实验验证了我们的框架：首先，我们探讨了不同方法生成多样化的代理，分析其优缺点，并证明了具有上下文意识的代理生成能带来更大的需求多样性。其次，我们展示了该框架如何有效地模拟富有同情心的领先用户访谈，识别出比传统人类访谈更多的潜在需求。最后，我们展示了如何使用LLMs分析访谈，提取需求并将其分类为潜在或非潜在。我们的研究工作强调了利用LLM代理加速早期产品研发、降低成本和促进创新的潜力。|
|**2024-04-24**|**A Human-Computer Collaborative Tool for Training a Single Large Language Model Agent into a Network through Few Examples**|Lihang Pan et.al.|[2404.15974](http://arxiv.org/abs/2404.15974)|null|## 翻译  单个大型语言模型（LLM）在解决复杂任务方面的能力有限。然而，通过连接多个LLM代理构建的网络可以显著提升整体性能。本文介绍了一种人机协作工具——EasyLAN，旨在帮助开发者轻松构建LLM代理网络（LAN）。EasyLAN首先根据任务描述自动生成仅包含一个代理的初始网络。接着，它利用少量训练示例来调整网络。对于每个示例，EasyLAN分析输出与真实结果之间的差距，并找出错误的原因。EasyLAN会采用精心设计的策略来修正这些问题。用户可以介入EasyLAN的工作流程或直接修改LAN。最终，LAN从单个代理发展成多代理的网络。实验结果显示，EasyLAN能够帮助开发者快速构建性能良好的LAN。|
|**2024-04-03**|**Concept-Guided LLM Agents for Human-AI Safety Codesign**|Florian Geissler et.al.|[2404.15317](http://arxiv.org/abs/2404.15317)|null|随着生成人工智能在软件工程，特别是安全工程中的重要性提升，对它的质量要求也随之提高。单纯依赖大型语言模型（LLMs）已不足以满足这些需求。因此，我们提出了一种高效且融合的策略，旨在利用LLMs进行安全分析和人机协同设计，以确保软件系统的安全性。我们开发了一个定制化的LLM代理，结合提示工程、启发式推理和检索增强生成，专注于解决与预定义安全概念相关的任务，并与系统模型图进行交互。决策流程通过一系列微决策进行引导，有助于保持结构化信息。此外，我们还提出了图的口头表述作为系统模型的中间表示，以促进LLM与图的交互。我们通过一个简化自动驾驶系统的示例，展示了选择的提示-响应对，以说明我们的方法如何应用于安全分析。|
|**2024-04-23**|**Aligning LLM Agents by Learning Latent Preference from User Edits**|Ge Gao et.al.|[2404.15269](http://arxiv.org/abs/2404.15269)|**[link](https://github.com/gao-g/prelude)**|**我们研究基于用户对语言模型编辑的互动学习语言代理。在诸如写作助手的常见场景中，用户与语言代理交互，根据上下文生成响应，并可能选择性地编辑代理的响应以反映他们的潜在偏好，同时提高准确性。这种编辑反馈是自然产生的，适合用于提升代理与用户偏好的契合度，降低后续用户的编辑成本。为此，我们提出PRELUDE框架，它根据历史编辑数据推断用户的潜在偏好，并据此设计一个提示策略，引导未来的响应生成，避免了昂贵且难以扩展的微调过程，还能保持在其他任务上的性能。  此外，学习描述性的偏好有助于增强可解释性，用户可以查看和调整学习到的偏好。然而，用户偏好可能复杂多变，受情境影响，因此学习起来具有挑战性。为解决这一问题，我们提出CIPHER算法，它利用大型语言模型（LLM）根据用户编辑推断给定情境下的用户偏好。未来，CIPHER会从历史中的k个最接近的上下文中检索推断出的偏好，综合生成响应。我们在总结和电子邮件写作两个互动环境中使用GPT-4模拟用户进行评估，与直接使用用户编辑但不学习描述性偏好的算法，以及学习全局无上下文偏好的算法进行了比较。  在两项任务中，CIPHER都实现了最低的编辑距离成本，并且学习到的偏好与真实偏好显示出显著的相似性。**|
|**2024-04-22**|**A Survey on Self-Evolution of Large Language Models**|Zhengwei Tao et.al.|[2404.14387](http://arxiv.org/abs/2404.14387)|**[link](https://github.com/alibabaresearch/damo-convai)**|**## 概述  大型语言模型（LLMs）在众多领域和智能代理应用中取得了显著进步。然而，依赖人类或外部模型监督的现有LLMs在处理复杂任务和多样性增加时可能会遇到成本高昂和性能瓶颈的问题。为此，自我进化方法应运而生，这种策略允许LLMs自主获取、精炼并从自身生成的经验中学习，借鉴人类经验学习过程，有望推动LLMs向超级智能发展。本文全面综述了LLMs中的自我进化方法。首先，我们提出一个概念框架，将进化过程划分为迭代循环的四个阶段：经验获取、经验细化、更新和评估。其次，我们分类探讨LLMs和基于LLM的代理的进化目标，并对相关文献进行总结，提供每个模块的分类和见解。最后，我们指出了当前的挑战，并提出了未来研究方向，为加速自演进LLMs的发展提供关键洞见。**|
|**2024-04-21**|**A Survey on the Memory Mechanism of Large Language Model based Agents**|Zeyu Zhang et.al.|[2404.13501](http://arxiv.org/abs/2404.13501)|**[link](https://github.com/nuster1128/llm_agent_memory_survey)**|**随着大型语言模型（LLMs）在科研和工业界的广泛关注，基于LLMs的智能代理因其自我进化能力而备受瞩目，这对于解决需要长期复杂交互的现实问题至关重要。支持agent-environment交互的关键要素是代理的记忆机制。尽管已有众多有前景的记忆设计被提出，但这些研究分散在多篇论文中，缺乏全面的综述来系统性地总结和比较，未能提炼出通用且有效的设计模式以启发后续研究。为此，本论文旨在填补这一空白，我们提出一份关于LLM基代理记忆机制的全面调查。首先，我们将探讨记忆在LLM代理中的“是什么”以及“为什么需要”。然后，我们系统回顾了关于记忆模块的设计和评估方法的研究。此外，我们还会展示记忆模块在各种应用中扮演的重要角色。最后，我们会分析现有工作的局限，并指出重要的未来研究方向。为了跟踪该领域最新进展，我们创建了一个GitHub仓库：\url{https://github.com/nuster1128/LLM_Agent_Memory_Survey}。**|
|**2024-04-18**|**From Language Models to Practical Self-Improving Computer Agents**|Alex Sheng et.al.|[2404.11964](http://arxiv.org/abs/2404.11964)|null|我们提出了一种简单直接的方法，用于创建能够执行各种计算机任务的人工智能代理，并通过自我改进来发展工具和增强功能，以解决日益复杂的任务。鉴于大型语言模型（LLMs）已显示出从非参数增强中获益，近期的研究大量集中在开发软件，以赋予LLMs各种能力。我们建议，通过适当的提示工程，一个LLM代理可以系统地生成软件来增强自身，而不是依赖人类工程的静态软件开发。  我们通过一些案例研究展示了这一点：仅通过终端访问，我们引导LLM代理添加了检索、互联网搜索、网页导航和文本编辑功能。该代理有效地利用这些工具解决了问题，例如自动化软件开发和基于网络的任务。这种方法表明，通过连续提问和巧妙的提示设计，LLM能够自主扩展其功能，执行实际的计算机任务。|
|**2024-04-25**|**Automated Social Science: Language Models as Scientist and Subjects**|Benjamin S. Manning et.al.|[2404.11794](http://arxiv.org/abs/2404.11794)|null|我们提出了一种方法，利用大型语言模型（LLM）的最新进展，自动构建和测试社会科学假设。这种方法的关键在于使用结构因果模型。结构因果模型提供了一个陈述假设的语言、构建LLM基础代理的蓝图、实验设计以及数据分析计划。拟合后的结构因果模型可供预测或规划后续实验。我们通过几个场景进行了演示：谈判、保释听证会、求职面试和拍卖。在这些情况下，系统既提出了因果关系，也进行了检验，发现了一些证据，而有些则没有。我们证明，从这些社会互动模拟中获取的洞察并非仅通过直接询问LLM就能获得。当给定每个场景的建议结构因果模型时，LLM在预测估计效应的符号方面表现良好，但无法可靠地预测效应的大小。在拍卖实验中，模拟结果与拍卖理论的预测紧密吻合，但LLM直接提取的清算价格预测不准确。然而，如果模型能基于拟合的结构因果模型进行条件化，LLM的预测会大幅改进。简而言之，LLM知道的比它能立即表达的要多。|
|**2024-04-17**|**AgentKit: Flow Engineering with Graphs, not Coding**|Yue Wu et.al.|[2404.11483](http://arxiv.org/abs/2404.11483)|**[link](https://github.com/holmeswww/agentkit)**|**我们提出了一种直观的大型语言模型提示框架（AgentKit），旨在为多功能代理提供统一的方法。AgentKit通过简单的自然语言提示构建复杂的“思维过程”。其基本单元是节点，包含特定子任务的自然语言指令。用户可以像拼接乐高积木一样连接这些节点，从而明确设计出自然结构化的“思考流程”。例如，在撰写论文时，可能的步骤包括：1）确定核心信息，2）识别研究空白等。AgentKit的模块化特性使得高级功能如即兴的层次化规划、反思和从互动中学习变得可能。由于其直观且模拟人类思考过程的设计，即使没有编程经验的人也能创建和调整基础代理。定量实验显示，使用AgentKit设计的代理在WebShop和Crafter任务上实现了最先进的性能。这些成果表明AgentKit有潜力使LLM代理在更广泛的场景下高效且易于使用。相关代码已开源在GitHub：https://github.com/holmeswww/AgentKit。**|
|**2024-04-15**|**Memory Sharing for Large Language Model based Agents**|Hang Gao et.al.|[2404.09982](http://arxiv.org/abs/2404.09982)|**[link](https://github.com/ghupppp/memorysharingllm)**|**在人工智能领域，大型语言模型（LLMs）通过自然语言提示执行任务的能力是一个重大突破，它减少了对固定答案任务（如常识问题和是非查询）的重新训练或微调需求。然而，在处理开放性挑战如诗歌创作时，基于上下文学习的方法显示出局限，主要源于提供的示例全面性以及模型理解问题内容的能力不足，导致输出往往与预期结果大相径庭。针对这一差距，我们的研究提出了Memory-Sharing（MS）框架，这是一种针对LLM多代理的实时记忆存储和检索系统，旨在增强基于上下文的学习过程。每个“记忆”单元记录了提出的查询及其来自LLM代理的即时响应，从多个类似代理中聚合这些记忆，形成所有代理共享的丰富记忆池。MS框架不仅帮助代理找到特定任务的相关示例，还评估其记忆的潜在利用价值，供其他代理未来应用。在三个不同领域的实证验证显示，MS框架显著提高了代理处理开放性问题的表现。此外，我们还讨论了哪种记忆池和检索策略能更好地支持代理，为MS的未来发展提供了方向。代码和数据可在：https://github.com/GHupppp/MemorySharingLLM 获取。**|
|**2024-05-10**|**Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation**|Ruixin Yang et.al.|[2404.09127](http://arxiv.org/abs/2404.09127)|**[link](https://github.com/minnesotanlp/collaborative-calibration)**|**### 背景  当前的大规模语言模型（LLMs）在不确定性估计方面面临挑战，它们通常校准不良且过度自信，特别是在基于人类反馈的强化学习（RLHF）中。人类的决策和信心不仅源于内在信念，还能通过日常观察进行调整，而现有LLM的校准方法主要关注单个模型的信心估计，未能充分利用“集体智慧”：多个LLM之间的协作表达能力，这可以集体提高准确性和校准。本研究中，我们提出了一种无训练后处理的校准策略——协作校准（Collaborative Calibration），它利用多代理工具增强的LLMs在模拟的群体讨论过程中，共同提升校准能力和推理合理性。  ### 任务  我们在生成式问答任务上展示了协作校准的有效性，覆盖了多个领域，证明了它在整合集体校准后的信心评估和提升模型预测可靠性方面的潜力。**|
|**2024-04-13**|**CuriousLLM: Elevating Multi-Document QA with Reasoning-Infused Knowledge Graph Prompting**|Zukang Yang et.al.|[2404.09077](http://arxiv.org/abs/2404.09077)|**[link](https://github.com/zukangy/kgp-curiousllm)**|**在问答（QA）领域，大型语言模型（LLMs）与外部数据库的融合取得了显著成效。然而，这些方法在处理复杂推理任务时往往力有不逮。为此，我们对一种名为知识图谱提示（KGP）的创新方法进行了优化，该方法结合知识图谱和基于LLM的代理以提升推理和搜索精度。然而，原始的KGP框架需要昂贵的大规模数据微调，并且仍存在LLM的错误推断问题。因此，我们提出了一种融入推理能力的LLM代理，它模仿人类的好奇心，通过提问来更有效地导航搜索过程。这个简单的改进显著提高了LLM在QA任务中的性能，同时避免了初始KGP框架的高成本和延迟。我们的目标是进一步发展这种方法，最终实现更精确、更快捷且成本效益更高的QA解决方案。**|
|**2024-04-13**|**Do LLMs Play Dice? Exploring Probability Distribution Sampling in Large Language Models for Behavioral Simulation**|Jia Gu et.al.|[2404.09043](http://arxiv.org/abs/2404.09043)|null|随着大型语言模型（LLMs）的飞速发展及其在处理复杂语言任务中的出色表现，越来越多的研究尝试利用LLMs模拟人类的行为决策过程，通常这些过程被表示为马尔可夫决策过程（MDPs）。在这个框架中，动作遵循特定的概率分布，并需要迭代采样。这促使我们探究LLM代理理解概率分布的能力，以通过概率采样指导行为决策并生成行为序列。我们将问题分为两个主要方面：一是已知精确概率分布的模拟，二是模糊概率分布的序列生成。  在已知概率分布的情况下，代理需要根据问题描述提供概率分布的类型和参数，然后给出采样序列。然而，我们的研究显示，LLM代理在这方面的性能不佳，但通过编程工具可以一定程度上提高采样成功率。而在实际情境中，概率分布往往不明确。因此，我们在第二部分让代理调整在线社交网络中的活跃度，并分析行动频率。结果表明，即使借助编程工具，LLM代理依然无法有效地采样概率分布。这意味着在直接将LLM作为模拟人类行为的代理应用之前，还需要谨慎对待。|
|**2024-04-12**|**Strategic Interactions between Large Language Models-based Agents in Beauty Contests**|Siting Lu et.al.|[2404.08492](http://arxiv.org/abs/2404.08492)|null|随着大型语言模型（LLMs）的广泛应用，它们在博弈论框架下的游戏行为理解潜力日益显现。本研究聚焦于通过模拟分析不同类型LLM驱动的代理在经典 Beauty Contest 游戏中的策略互动。借鉴人类实验，我们对LLM代理的策略层次进行类似的评估，发现它们展现出从零级到一级的不同程度推理能力，并在重复游戏中表现出行动趋同。此外，我还探讨了不同类型的代理群体构成如何影响战略行为：高比例的固定策略对手能促进LLM代理的收敛，而混合环境中不同相对策略水平的代理共存会加速所有代理的收敛。更智能的代理可能获得更高的平均收益，但这是以较低智能代理的牺牲为代价的。这些结果不仅揭示了在特定情景下模拟代理的结局，还为理解算法之间的战略互动提供了重要启示。|
|**2024-04-17**|**LLM Agents can Autonomously Exploit One-day Vulnerabilities**|Richard Fang et.al.|[2404.08144](http://arxiv.org/abs/2404.08144)|null|随着大语言模型（LLMs）的威力日益增强，其在良性和恶意用途上的应用也日益广泛。研究人员开始关注它们利用网络安全漏洞的能力。近期的研究探讨了LLMs自主破解网站的可能性，但这些研究主要集中在简单的漏洞上。本工作揭示，LLMs能够自主利用现实世界系统中的单日漏洞。我们收集了一组包含15个被CVE描述为“关键严重性”的一天期漏洞数据。当提供CVE描述时，GPT-4模型能成功利用87%的漏洞，相比之下，其他测试模型（如GPT-3.5、开源LLMs和开源漏洞扫描器ZAP和Metasploit）的表现均为0%。然而，我们的GPT-4模型在没有描述的情况下效率大减，仅能利用7%的漏洞。这些发现对大规模部署高能力LLMs提出了质疑。|
|**2024-04-11**|**WESE: Weak Exploration to Strong Exploitation for LLM Agents**|Xu Huang et.al.|[2404.07456](http://arxiv.org/abs/2404.07456)|null|近期，大型语言模型（LLMs）显示出作为智能代理的强大潜力。然而，现有的研究主要集中在通过精心设计的提示工程或任务特定的微调来提升模型的推理或决策能力，忽视了探索与利用的过程。在处理开放世界交互环境中的复杂任务时，这些方法存在局限性。首先，由于缺乏对环境的全局信息，模型倾向于做出贪婪决策，导致解决方案不理想。另一方面，从环境中获取的无关信息不仅引入噪声，还增加了额外的成本。  为此，本文提出了一种新颖的方法——弱探索强化强利用（Weak Exploration to Strong Exploitation，WESE），旨在增强LLM在解决开放世界交互任务中的表现。具体来说，WESE将探索和利用过程解耦，使用成本效益高的“弱”代理执行探索任务，以获取全局知识。随后，我们引入基于知识图谱的策略来存储这些知识，并提取与任务相关的关键信息，从而提升“强”代理在成功率和效率上的性能。我们的方法适用于各种任务，并在四个互动基准测试中显著提高了成功率和效率。|
|**2024-04-10**|**GoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM Applications**|Shishir G. Patil et.al.|[2404.06921](http://arxiv.org/abs/2404.06921)|**[link](https://github.com/ShishirPatil/gorilla)**|**随着大型语言模型（LLMs）的发展，它们不再仅仅是对话系统中的信息提供者，而是开始积极参与到与实际应用和服务的互动中。如今，人类在将LLM生成的输出（如代码、函数或操作）投入现实世界执行前，需要验证其正确性和适用性，这带来了挑战，因为代码理解被广泛认为非常困难。本文研究了人类如何能有效与LLMs协作、委派和监督，特别是在未来。我们主张，在许多情况下，对提出的行动进行“事后验证”（在看到输出后确认其正确性）比之前的“事前验证”更为容易。实现这一目标的核心理念是集成直观的撤销功能，并为LLM生成的动作设定损害约束，作为降低相关风险的有效策略。通过这种方式，人类可以撤销LLM输出的影响，或者确信潜在风险是有限的。我们认为这对于实现LLMs与应用和服务在有限的人类监督下交互至关重要。我们描述了开源运行时Gorilla Execution Engine（GoEX）的设计和实现，该运行时用于执行LLM动作，并提出了一些开放的研究问题，旨在推动LLMs与应用之间以最小的人工干预进行交互。GoEX的源代码已发布在https://github.com/ShishirPatil/gorilla/。**|
|**2024-04-09**|**AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents**|Luca Gioacchini et.al.|[2404.06411](http://arxiv.org/abs/2404.06411)|**[link](https://github.com/nec-research/agentquest)**|**随着大型语言模型（LLMs）的进展，人们追求能够解决复杂、多步骤推理任务的LLM代理。然而，现有的基准往往局限且只关注整体任务成功率。为了解决这些问题，我们提出了AgentQuest框架，它具有以下特点：（i）benchmark和评估指标模块化且易于扩展，通过文档齐全、易用的API；（ii）我们提供了两种新的评估指标，能够在解决任务时可靠地追踪LLM代理的进步。我们通过两个示例展示了这些指标的实用性，通过识别常见失败点并优化代理架构，显著提高了性能。我们希望与研究界共同扩展AgentQuest，并已将其开源在https://github.com/nec-research/agentquest。**|
|**2024-04-15**|**AutoCodeRover: Autonomous Program Improvement**|Yuntong Zhang et.al.|[2404.05427](http://arxiv.org/abs/2404.05427)|**[link](https://github.com/nus-apr/auto-code-rover)**|**在过去几十年里，研究人员在自动化软件开发过程中取得了显著进展，尤其是大型语言模型（LLMs）的应用极大地推动了编程辅助的自动化。然而，软件工程并不仅仅是编码，还包括维护（如修复bug）和演化（如添加功能）等程序改进过程。本文提出了一种自动解决GitHub问题的方法，旨在实现程序自主改进。我们的方法称为AutoCodeRover，它结合了LLMs与高级代码搜索能力，最终生成程序修改或补丁。与AI研究者和从业者近期关注的仅文件级别的软件项目不同，我们的工作侧重于程序表示（抽象语法树），利用类/方法的程序结构来增强LLM对问题根本原因的理解，并通过迭代搜索提供上下文。当测试套件可用时，谱系基线故障定位技术进一步精确了上下文。  在SWE-bench-lite，一个包含300个真实GitHub问题的数据集上，AutoCodeRover的解决方案效果提升，解决了约22-23%的问题。对于全量的SWE-bench，包含2294个GitHub问题，AutoCodeRover解决了大约16%的问题，这比最近报道的来自Cognition Labs的AI软件工程师Devin的表现还要高，而且时间消耗与Devin相当。我们相信，我们的工作流程能够推动自主软件工程的发展，未来LLM自动生成的代码可以被自动地进行优化和改进。**|
|**2024-04-08**|**Long-horizon Locomotion and Manipulation on a Quadrupedal Robot with Large Language Models**|Yutao Ouyang et.al.|[2404.05291](http://arxiv.org/abs/2404.05291)|null|我们提出了一种基于大型语言模型（LLM）的系统，旨在提升四足机器人的问题解决能力，使其能够处理超越短期动作的长期任务。对于四足机器人来说，长期任务极具挑战性，因为它们需要对任务的语义有高层理解，并具备广泛的运动和操纵技能以与环境互动。我们的系统构建了一个高层推理层，利用大型语言模型，从任务描述中生成混合离散-连续的计划，作为机器人代码。它包括多个LLM代理：一个用于构思计划的语义规划器、一个参数计算器，用于预测计划中的参数，以及一个代码生成器，将计划转换为可执行的机器人代码。  在低层次，我们采用强化学习来训练一套运动规划和控制技能，以增强四足机器人的灵活性，使其能进行丰富环境交互。我们在难以用单一技能完成的长期任务上测试了我们的系统。模拟实验和真实世界实验表明，它成功地制定了多步骤策略，并展现出非平凡的行为，例如制作工具或向人类寻求帮助。|
|**2024-04-06**|**Autonomous Artificial Intelligence Agents for Clinical Decision Making in Oncology**|Dyke Ferber et.al.|[2404.04667](http://arxiv.org/abs/2404.04667)|null|多模态人工智能系统有望通过解析各类医学数据提升临床决策。然而，这些模型在各医学领域的效能尚不明朗，每个领域都有其独特挑战。本文提出了一种利用大型语言模型（LLMs）作为核心推理引擎的新型多模态医疗AI方法。此引擎自主协调并部署一系列专门的医疗AI工具，如文本解读、放射学和病理图像分析、基因数据处理、网络搜索以及医疗指南文档检索。我们在一系列临床肿瘤学场景中验证了该系统，这些场景模拟了典型的患者护理流程。结果显示，系统在选择恰当工具（97%）、得出正确结论（93.6%）、提供完整（94%）和有益（89.2%）治疗建议，以及根据指令引用相关文献（82.5%）方面表现出高能力。这表明LLMs能够有效地规划和执行领域特定模型，以获取或合成新信息，从而充当个性化临床助手。此外，这种架构简化了监管合规性，因为每个组件工具可以单独验证和审批。我们相信，这项工作为医疗领域的更先进LLM代理提供了概念验证。|
|**2024-04-05**|**Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents**|Harsh Kohli et.al.|[2404.04237](http://arxiv.org/abs/2404.04237)|null|大型语言模型（LLMs）的快速进步使其在标准基准测试中频频超越人类表现，推动了众多下游应用的发展，如基于LLMs的代理。然而，这些模型在看似简单的任务中意外地表现不佳，这强调了对更全面和多样化的评估框架的需求，以衡量它们的实际能力。为此，我们聚焦于组合性和条件推理——人类认知的基石，并提出GroundCocoa，这是一个与航班预订这一现实问题相连接的词汇丰富的基准。我们的任务是将用户的详细偏好与以多选形式提供的可用航班选项进行匹配。结果显示，包括最先进的GPT-4 Turbo在内的当前最佳模型，在经过高级提示后，准确率仍不超过67%，显示出显著的性能差距。|
|**2024-04-02**|**Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization**|Yoichi Ishibashi et.al.|[2404.02183](http://arxiv.org/abs/2404.02183)|**[link](https://github.com/tsukushiai/self-organized-agent)**|**## 背景  随着大型语言模型（LLM）代理的最新进展，自动化软件开发的未来正逐渐显现。然而，现有的单代理方法在生成和优化大规模、复杂的代码库时面临上下文长度限制的问题。为解决这一挑战，我们提出了一种新颖的多代理框架——自组织多Agent体系（SoA）。SoA是一个可扩展且高效的多代理系统，它允许独立地生成和修改代码组件，并协同构建整个代码库。SoA的一个关键特性是根据问题复杂性自动增加代理，实现动态可扩展性。这样，整体代码量可以根据代理数量无限增长，而每个代理管理的代码量保持恒定。  我们在HumanEval基准上评估了SoA，并发现与单代理系统相比，SoA中的每个代理处理的代码量明显减少，但总体生成的代码量显著增加。此外，SoA在Pass@1准确率方面比强大的单代理基线提高了5%。**|
|**2024-04-02**|**Helmsman of the Masses? Evaluate the Opinion Leadership of Large Language Models in the Werewolf Game**|Silin Du et.al.|[2404.01602](http://arxiv.org/abs/2404.01602)|**[link](https://github.com/doslim/evaluate-the-opinion-leadership-of-llms)**|**大型语言模型在社交推理游戏中展现出显著的策略行为，但对它们作为意见领袖的重要性关注不足，这对于多Agent和人机交互场景的实际应用至关重要。意见领袖是指在一个社会群体中对他人信念和行为有显著影响的个体。本研究使用“狼人杀”游戏作为模拟平台，探讨语言模型在扮演Sheriff（治安官）角色时的意见领导能力。Sheriff负责总结论点并提出决策建议，因此它代表了意见领袖的一个可信代理。我们构建了一个整合Sheriff角色的框架，并基于意见领袖的关键特性提出了两个评估指标：第一个衡量意见领袖的可靠性，第二个考察其对其他玩家决策的影响。  我们进行了大量实验，评估不同规模的语言模型，并创建了“狼人杀”问题回答数据集（WWQA），以测试和提升模型对游戏规则的理解。此外，还包含了人类参与者进行进一步分析。研究结果表明，“狼人杀”游戏是一个有效评估语言模型意见领导力的试验场，但目前仅有少数语言模型具备这种能力。**|
|**2024-04-15**|**CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs**|Jingzhe Shi et.al.|[2404.01343](http://arxiv.org/abs/2404.01343)|**[link](https://github.com/jingzheshi/chops)**|**随着企业和软件平台越来越多地采用大型语言模型（如GPT-3.5、GPT-4、GLM-3和LLaMa-2）提供聊天辅助或客户服务推理，现有的基于LLM的客户服务模型在与客户资料集成和执行实际操作方面存在局限。它们倾向于强调多样性而非精确性和错误避免，这对于现实世界的客户服务场景并不理想。因此，我们提出了一种名为CHOPS（结合客户资料的聊天助手）的LLM代理，旨在：（1）高效利用现有数据库或系统查询用户信息，或遵循既定指南与系统交互；（2）提供准确合理的响应并执行系统内的必要操作，同时避免有害操作；（3）通过结合小型和大型LLM以实现性能满意且成本合理的推理。  我们开发了一个实用的数据集，称为CPHOS-dataset，它包括一个数据库、指导文件以及来自CPHOS平台的模拟物理奥林匹克组织服务的问答对。CPHOS是一个面向高中教师和学生的在线平台。我们通过使用CPHOS-dataset进行了广泛的实验，验证了CHOPS架构的性能，目标是展示LLM如何提升或替代人工客户服务。关于我们的提案架构和数据集的代码可在此处获取：<https://github.com/JingzheShi/CHOPS>。**|
|**2024-03-31**|**DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model**|Lirui Zhao et.al.|[2404.01342](http://arxiv.org/abs/2404.01342)|**[link](https://github.com/opengvlab/diffagent)**|**文本到图像（T2I）生成模型近年来备受瞩目，在学术研究和实际应用中大放异彩。例如，Civitai平台，一个T2I创新的聚集地，目前汇集了74,492种独特的模型，这带来了选择最合适的模型和参数的艰巨任务，通常需要多次试验。借鉴大型语言模型（LLMs）工具使用研究的思路，我们推出了DiffAgent，这是一个通过API调用来快速筛选准确选项的LLM代理。DiffAgent采用了一种新颖的两阶段训练框架，称为SFTA，使其能够根据人类偏好精确地将T2I API的响应与用户输入对齐。为了训练和评估DiffAgent的能力，我们构建了DABench，这是一个全面的数据库，涵盖了社区中的各种T2I API。实验结果显示，DiffAgent不仅在选择适当的T2I API方面表现出色，还验证了SFTA训练框架的有效性。相关代码已可在https://github.com/OpenGVLab/DiffAgent获取。**|
|**2024-03-31**|**Algorithmic Collusion by Large Language Models**|Sara Fish et.al.|[2404.00806](http://arxiv.org/abs/2404.00806)|null|随着算法定价的兴起，人们担忧算法间的合谋问题。我们通过实验使用基于大型语言模型（LLMs）的定价代理，特别是GPT-4，进行了探究。研究发现：(1) LLM驱动的定价机制在定价任务上表现出色；(2) 在寡头竞争环境中，LLM定价代理会自发地进行合谋，从而损害消费者利益；(3) 对LLM指令（“提示”）看似微小的变化可能加剧这种合作行为。这些结果同样适用于拍卖场景。我们的研究结果强调了对算法定价进行反垄断监管的必要性，并揭示了针对LLM定价代理特有的监管挑战。|
|**2024-03-31**|**"My agent understands me better": Integrating Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents**|Yuki Hou et.al.|[2404.00573](http://arxiv.org/abs/2404.00573)|**[link](https://github.com/tamoharu/Agent-Memory-CHI24)**|在这个研究中，我们提出了一种创新的人类记忆架构，旨在提升基于大型语言模型的对话代理的认知能力。我们的设计使得这些代理能自主检索生成响应所需的必要记忆，从而解决LLMs在时间认知上的局限。我们借鉴了人类的记忆线索召回机制作为触发点，以实现精确且高效的回忆。此外，我们开发了一个数学模型，动态量化记忆巩固过程，考虑了诸如上下文相关性、时间流逝和回忆频率等因素。代理会从用户的交互历史中存储记忆，这些记忆被封装在数据库中，每个记忆都包含了内容和时间关联的语境。这样，通过类似人类识别和回忆过往经历的方式，系统能够战略性地存储记忆，并理解它们对用户在时间线上的重要性。|

<p align=right>(<a href=#updated-on-20240620>back to top</a>)</p>

## llm

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2024-06-18**|**DrVideo: Document Retrieval Based Long Video Understanding**|Ziyu Ma et.al.|[2406.12846](http://arxiv.org/abs/2406.12846)|null|当前的长视频理解方法主要关注时长仅十几秒的视频，对处理更长视频的技术探索有限。长视频中的大量帧数带来了两个主要挑战：难以定位关键信息和进行长期推理。因此，我们提出DrVideo，一个基于文档检索的系统，专为长视频理解设计。我们的核心思想是将长视频理解问题转化为长文档理解任务，以充分利用大型语言模型的强大能力。具体来说，DrVideo将长视频转换为文本形式的长文档，首先检索关键帧并增强这些帧的信息，作为系统的起点。然后，它采用基于代理的迭代循环，持续搜索缺失信息、补充相关数据，并在收集到足够的与问题相关的信息后，以链式思考的方式给出最终预测。在多个长视频基准上的实验验证了我们方法的有效性。DrVideo在EgoSchema（3分钟）测试中比现有最先进的方法高出3.8个百分点，在MovieChat-1K（10分钟）的break模式和global模式中分别提高17.9和38.0分，以及在LLama-Vid QA（超过60分钟）数据集上提升30.2分。|
|**2024-06-18**|**Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts**|Haoxiang Wang et.al.|[2406.12845](http://arxiv.org/abs/2406.12845)|**[link](https://github.com/RLHFlow/RLHF-Reward-Modeling)**|**强化学习从人类反馈（RLHF）已经成为大型语言模型（LLMs）与人类偏好对齐的主要方法。传统上，通过使用人类偏好数据训练奖励模型（RM），过程通常从比较同一用户请求的响应开始，相对评分指示人类更喜欢哪个响应。然而，由于RM的黑盒特性，其输出缺乏可解释性，人们难以理解为什么RM认为某个回复是好的。鉴于RM作为人类偏好的代理，我们提议采用两阶段方法来创建可解释的RM：首先，使用多维绝对评分数据训练绝对评级多目标奖励模型（ArmoRM），每个维度对应于人类可理解的目标（如诚实、详尽、安全）；其次，利用混合专家（MoE）策略，结合一个门控网络，根据上下文自动选择最合适的奖励目标。我们成功地使用Llama-3 8B训练了ArmoRM，并在顶部添加了一个浅层MLP作为门控网络，形成了ArmoRM-Llama3-8B。我们的模型在评估RM的语言建模性能的RewardBench基准上实现了最先进的成绩。值得注意的是，我们的模型在性能上超过了使用GPT-4法官的LLM作为评判者的方法，并接近于规模更大的Nemotron-4 340B奖励模型的水平。**|
|**2024-06-18**|**Synergizing Foundation Models and Federated Learning: A Survey**|Shenghui Li et.al.|[2406.12844](http://arxiv.org/abs/2406.12844)|null|近期，大型语言模型、视觉Transformer和多模态模型等基础模型（FMs）的发展在学术界和工业界产生了显著影响。与小型模型相比，FMs在预训练阶段对大量数据的需求更大。尽管通用FMs可以使用互联网上的公开数据进行预训练，但针对特定领域的FMs需要专有数据，这在实际应用中因隐私问题而面临数据可用性挑战。联邦学习（FL）作为一种协作学习范式，打破了数据共享的障碍，为利用分布式数据定制和适应各种领域特定任务的FMs提供了前景，同时保护了数据隐私。这篇综述论文探讨了FL与FMs融合的潜力与挑战，总结了核心技术、未来发展方向以及应用场景。关于FM-FL的定期更新论文集合可在<https://github.com/lishenghui/awesome-fm-fl>获取。|
|**2024-06-18**|**LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation**|Seyedarmin Azizi et.al.|[2406.12832](http://arxiv.org/abs/2406.12832)|**[link](https://github.com/arminazizi98/lamda)**|**在大语言模型微调领域，低秩适应（LoRA）已经成为标准方法，因为它显著减少了可训练参数。然而，随着模型嵌入维度的增加，LoRA所需的可训练参数量也随之上升，导致计算成本较高。此外，其后向更新需要存储高维中间激活和优化器状态，对GPU内存需求较大。为此，本文提出了一种新的大语言模型微调方法——基于谱分解的低维适应（LaMDA）。LaMDA通过冻结第一投影矩阵（PMA），同时引入一个低维可训练的平方矩阵，实现了可训练参数和峰值GPU内存使用的大幅减少。在早期的微调阶段，LaMDA逐步冻结第二投影矩阵（PMB），进一步降低权重更新的计算成本，提高参数效率。  我们还引入了增强版LaMDA++，它通过规范化预训练模型权重的谱分析，实现轻量级的LoRA路径自适应秩分配。我们在多个任务上进行了评估，包括GLUE自然语言理解基准、文本摘要、自然语言生成以及复杂推理，应用于不同类型的大型语言模型。实验结果显示，LaMDA在性能上与现有方法相当或超越，且在微调期间可减少高达17.7倍的参数更新次数，以及1.32倍的峰值GPU内存使用。我们将公开代码。**|
|**2024-06-18**|**Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?**|Pinzhen Chen et.al.|[2406.12822](http://arxiv.org/abs/2406.12822)|null|## 背景 大型多语言模型旨在服务不同语种的母语使用者。我们推测，当前针对这些模型的微调和评估方法可能与其初衷不符，原因在于过度依赖翻译，可能导致翻译中的瑕疵。尚不清楚指令数据的性质如何影响模型输出，同时，用翻译测试集来捕捉这些细微差别是否有效。由于训练和评估阶段常常结合使用翻译数据，这些潜在问题可能被忽视。本研究通过在指令调优和评估阶段使用控制性的母语或翻译数据，来探究这些问题，并观察模型表现。我们在八种基础模型和八个不同的基准上进行实验，结果显示，对于母语或生成性基准，使用母语或翻译指令数据时，模型性能高时，两者之间的差异尤为明显，而在其他类型的测试集上则不然。最后，我们发现正则化对于结构化任务有益，但对于生成性任务则不然。|
|**2024-06-18**|**Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?**|Zhe Yang et.al.|[2406.12809](http://arxiv.org/abs/2406.12809)|null|大型语言模型（LLMs）展现了令人印象深刻的性能，但它们仍存在不一致的问题，例如对重述或微小顺序变化的反应不一致。除了这些不稳定性，我们还观察到尽管LLMs能够解决难题，但在相对简单的任务上却可能失败。为了评估这种从难到易的不一致性，我们创建了ConsisEval基准，其中每个条目包含一对难度有严格排序的问题。我们还引入了一致性得分的概念，以量化这种不一致性，并分析通过相对一致性得分改进一致性潜力。通过对多种现有模型的全面实验，我们发现：(1) GPT-4获得92.2%的最高一致性得分，但仍因冗余信息的干扰、问题误解等问题对特定问题表现出不一致；(2) 能力更强的模型通常具有更高的一致性，但也存在例外情况；(3) 对于Fine-tuning和上下文学习而言，硬数据都能提升一致性。我们的数据和代码将在GitHub上公开提供。|
|**2024-06-18**|**Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents**|Zehao Wang et.al.|[2406.12806](http://arxiv.org/abs/2406.12806)|null|**背景**：配置设置对于调整软件行为以满足特定性能需求至关重要，但错误配置普遍存在。由于配置项众多且复杂，识别影响系统性能的配置是一项挑战。本研究提出PerfSense，这是一个轻量级框架，利用大型语言模型（LLMs）高效地识别性能关键配置，同时保持低开销。PerfSense利用LLM代理模拟开发者和性能工程师之间的交互，采用先进的提示链技术和检索增强生成（RAG）等技术。  **方法与成果**：我们在七个开源Java系统上的评估显示，PerfSense在分类性能敏感配置方面的平均准确率为64.77%，优于基于LLM的基线（50.36%）和先前的最佳方法（61.75%）。特别是，我们的提示链技术提高了召回率10%至30%，而保持了相似的精确度。进一步的手动分析362个误分类案例，发现常见问题包括LLMs对需求的理解偏差（占26.8%）。  **结论**：PerfSense显著减少了手动分类性能关键配置的工作量，并为未来的LLM基于代码分析研究提供了有价值的观点。|
|**2024-06-18**|**Supporting Human Raters with the Detection of Harmful Content using Large Language Models**|Kurt Thomas et.al.|[2406.12800](http://arxiv.org/abs/2406.12800)|null|本文探讨了利用大型语言模型（LLMs）自动或辅助人类审阅者检测有害内容的可能性，如仇恨言论、骚扰、极端主义和选举误导。通过50,000条评论的数据集，我们发现LLMs在与人类判断相比时能达到90%的准确率。我们提出五种设计模式，以整合LLMs与人工评级，例如预筛选非暴力内容、检测人类评级可能的错误，或者提供关键上下文以支持人工评级。我们展示了如何使用一个优化的提示来支持这些设计模式。在实际应用的试点中，我们的方法在优化人力资源效率方面实现了41.5%的提升，同时在检测违规内容的精确度和召回率上分别提高了9%至11%。|
|**2024-06-18**|**ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools**|Team GLM et.al.|[2406.12793](http://arxiv.org/abs/2406.12793)|null|我们介绍ChatGLM，这是一个随时间不断发展的大型语言模型系列。本报告主要关注GLM-4语言系列，包括GLM-4、GLM-4-Air和GLM-4-9B，它们代表了我们当前最强大的模型，集成了前三代ChatGLM的所有经验和教训。这些模型经过了十万亿次训练，主要涵盖中文和英语，以及少量来自24种语言的语料库，侧重于中英文的对齐。高质量的对齐是通过多阶段的后训练过程实现的，包括监督微调和学习人类反馈。评估显示，GLM-4在通用指标如MMLU、GSM8K、MATH、BBH、GPQA和HumanEval上接近或优于GPT-4；在IFEval指令跟随任务中的表现接近GPT-4 Turbo；在长文本任务上与GPT-4 Turbo（128K）和Claude 3相当；在中文对齐方面，GLM-4优于GPT-4，根据AlignBench衡量。GLM-4 All Tools模型进一步进行了对齐，以理解用户意图并能自主决定何时使用哪种工具，如Web浏览器、Python解释器、文本转图像模型和自定义函数，以有效地完成复杂任务。在实际应用中，它在诸如通过网络浏览获取信息和使用Python解释器解题等任务上与GPT-4 All Tools相匹配甚至超越。到目前为止，我们已经开源了一系列模型，包括ChatGLM-6B（三代）、GLM-4-9B（128K、1M）、GLM-4V-9B、WebGLM和CodeGeeX，在2023年仅Hugging Face上就有超过1000万次下载。这些开源模型可通过<https://github.com/THUDM>和<https://huggingface.co/THUDM>访问。|
|**2024-06-18**|**UBENCH: Benchmarking Uncertainty in Large Language Models with Multiple Choice Questions**|Xunzhi Wang et.al.|[2406.12784](http://arxiv.org/abs/2406.12784)|null|随着大型语言模型（LLMs）的迅速发展，它们在实际应用中展现出显著的效果。然而，由于低可解释性，这些模型在未预见情况下常会出现错误，限制了其价值。尽管已有许多研究致力于构建全面的评估体系，但先前的基准测试主要关注问题解决能力，对响应的不确定性评估不足，可能导致不稳定性。当前的方法在衡量LLM可靠性时资源消耗大，且难以测试黑盒模型。  为解决这些问题，我们提出了UBENCH，一个全面的LLM可靠性评估基准。它包含3,978个涵盖知识、语言理解、推理能力的多选题。实验结果显示，UBENCH达到了最先进的性能，并且其单次采样方法显著节省了计算资源，相较于需要多次采样的基线方法更为高效。此外，我们利用UBENCH评估了15种流行LLM的可靠性，发现GLM4表现出色，紧随其后的是GPT-4。我们还探究了Chain-of-Thought提示、角色扮演提示、选项顺序和温度对LLM可靠性的影响，分析了它们对不同模型的不同作用。|
|**2024-06-17**|**LLaNA: Large Language and NeRF Assistant**|Andrea Amaduzzi et.al.|[2406.11840](http://arxiv.org/abs/2406.11840)|null|多模态大型语言模型（MLLM）在理解和处理图像和3D数据方面表现出色，但它们在全面捕捉物体的外观和几何特性上存在局限。近期，神经辐射场（Neural Radiance Fields，简称NeRF）作为一种新兴的表示方式，通过一个简单的多层感知器（Multi-Layer Perceptron，MLP）的权重编码了物体的几何结构和高度逼真的外观，引起了广泛关注。本文探讨了将NeRF整合到MLLM中的可行性和效果。我们开发了LLaNA，这是首个通用的NeRF-语言助手，能够执行新任务，如NeRF描述和问答。我们的方法直接处理NeRF MLP的权重，无需渲染图像或构建3D数据结构，就能提取有关代表对象的信息。此外，我们创建了一个无须人工干预的NeRF文本标注数据集，用于各种NeRF-语言任务，并据此建立了一个评估方法来衡量我们的模型对NeRF理解能力。实验结果表明，处理NeRF权重的方法在与从NeRF中提取2D或3D表示进行比较时表现更优。|
|**2024-06-17**|**mDPO: Conditional Preference Optimization for Multimodal Large Language Models**|Fei Wang et.al.|[2406.11839](http://arxiv.org/abs/2406.11839)|null|### 背景  直接偏好优化（DPO）已被证明是大型语言模型（LLM）校准的有效手段。最近的研究尝试将DPO应用于多模态场景，但发现实现持续改进颇具挑战。通过对比实验，我们发现了多模态偏好优化中的无条件偏好问题，即模型忽视了图像条件。为解决这个问题，我们提出了mDPO，一个旨在防止语言偏好过度优先的多模态DPO目标，同时优化图像偏好。此外，我们引入了奖励锚点，确保选择的响应奖励保持正向，从而避免相对偏好优化固有的可能性降低问题。  ### 任务  我们在两个不同规模的多模态LLM以及三个常用基准上进行了实验，结果显示，mDPO有效解决了多模态偏好优化中的无条件偏好问题，并显著提高了模型性能，特别是在减少幻觉方面。|
|**2024-06-17**|**Unveiling Encoder-Free Vision-Language Models**|Haiwen Diao et.al.|[2406.11832](http://arxiv.org/abs/2406.11832)|**[link](https://github.com/baaivision/eve)**|**当前的视觉语言模型（VLM）主要依赖于视觉编码器来提取视觉特征，然后利用大型语言模型（LLMs）处理视觉语言任务。然而，视觉编码器在抽象视觉表示方面设定了强烈的先验，如分辨率、比例和语义倾向，这可能限制了VLM的灵活性和效率。直接训练无编码器的纯VLM仍然具有挑战性，且鲜有探索。实证研究显示，这种直接训练方法会导致收敛缓慢和性能差距较大。本文旨在弥合编码器依赖型和无编码器模型之间的差距，提出了一种简单而有效的纯VLM训练策略。具体来说，我们通过深入实验揭示了高效训练无编码器VLM的关键要素：（1）在统一的解码器内融合视觉与语言表示；（2）通过额外监督提升视觉识别能力。基于这些策略，我们开发了EVE，一个无编码器的视觉语言模型，既能高效训练也能快速推理。值得注意的是，仅使用3500万公开可用的数据，EVE就能在多个视觉语言基准上与类似容量的编码器依赖型VLM匹敌，甚至超越了训练过程神秘、数据未公开的Fuyu-8B模型。我们相信，EVE为跨模态开发纯粹的解码器架构提供了一个透明且高效的路径。我们的代码和模型已公开在：https://github.com/baaivision/EVE。**|
|**2024-06-17**|**Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models**|Bingqi Ma et.al.|[2406.11831](http://arxiv.org/abs/2406.11831)|null|大型语言模型（LLMs）基于解码器-only变压器在文本理解方面表现出色，但如何将这些先进的LLMs应用于文本到图像的扩散模型仍是一个待探索的问题。我们发现直接使用LLM作为提示编码器会显著降低生成图像时的提示跟随能力。主要存在两个问题：一是LLM的下一个词预测训练与扩散模型对区分性提示特征的需求不匹配；二是解码器架构固有的位置偏见。为解决这些问题，我们提出了一种新框架，通过精心设计的使用指南，增强LLM的文本表示能力，消除其内在的定位偏见，从而灵活地将最先进的LLMs融入文本到图像生成模型。此外，我们还提供了一种融合多个LLMs的方法。鉴于Transformer架构的卓越性能和扩展能力，我们进一步设计了基于该框架的LLM-Infused Diffusion Transformer（LI-DiT）。我们进行了广泛的实验，验证了LI-DiT在不同模型规模和数据量下的性能。得益于LLMs的内在能力及我们的创新设计，LI-DiT的提示理解性能轻松超越开源的最新模型，以及包括Stable Diffusion 3、DALL-E 3和Midjourney V6在内的主流闭源商业模型。强大的LI-DiT-10B将在进一步优化和安全检查后提供。|
|**2024-06-17**|**WPO: Enhancing RLHF with Weighted Preference Optimization**|Wenxuan Zhou et.al.|[2406.11827](http://arxiv.org/abs/2406.11827)|**[link](https://github.com/wzhouad/wpo)**|**强化学习从人类反馈（RLHF）是调整大型语言模型（LLMs）以更好地符合人类价值观的有前景方法。由于成本效益和可扩展性，离线偏好优化——通过其他模型获取偏好数据——被广泛采用。然而，离线偏好优化常受采样策略与目标策略之间分布差异的影响，导致优化效果不理想。为此，我们提出了一种创新策略——加权偏好优化（WPO），旨在通过调整偏好评分对，使离线数据更接近于当前策略，从而缓解这一问题。这种方法不仅解决了分布差距难题，还提升了优化过程，无需额外成本。  我们在Alpaca Eval 2和MT-bench等指令跟随基准上验证了我们的方法。WPO在Alpaca Eval 2上的性能比直接偏好优化（DPO）提高了5.6%。基于Llama-3-8B-Instruct，WPO甚至建立了显著的长度控制胜率，达到48.6%，在80亿参数模型排行榜上成为最强劲的模型。我们将在<https://github.com/wzhouad/WPO>上开源代码和模型。**|
|**2024-06-17**|**Embodied Instruction Following in Unknown Environments**|Zhenyu Wu et.al.|[2406.11818](http://arxiv.org/abs/2406.11818)|null|在自主家庭服务系统中，使实体代理能根据自然语言完成复杂的人类指令至关重要。传统方法仅能在所有互动对象都提供给代理的已知环境中执行指令，直接将现有方法应用于未知环境通常会产生操作不存在物体的不可行计划。相反，我们提出了一种针对未知环境的复杂任务实体指令跟随（Embodied Instruction Following，EIF）方法，该方法使代理能够有效地探索环境，利用现有物体生成可执行计划，以达成抽象指令。具体来说，我们构建了一个包括高层任务规划器和低层探索控制器的多模态大语言模型的层次化实体指令跟随框架。然后，我们通过动态区域注意力构建场景的语义表示地图，以展示已知的视觉线索，使任务规划和场景探索与人类指令目标保持一致。对于任务规划器，根据任务完成过程和已知视觉线索，我们生成步骤式的可行计划。对于探索控制器，根据生成的步骤计划和已知视觉线索预测最优的导航或物体交互策略。实验结果表明，我们的方法在大型房屋级场景中的204个复杂人类指令（如做早餐和整理房间）上实现了45.09%的成功率。|
|**2024-06-17**|**VideoLLM-online: Online Video Large Language Model for Streaming Video**|Joya Chen et.al.|[2406.11816](http://arxiv.org/abs/2406.11816)|null|## 翻译  近期的大型语言模型已经增强了视觉功能，能够理解图像、视频和融合了视觉与语言的内容。然而，这些大模odels的训练方法通常将视频视为预先剪辑好的片段，这使得它们在处理连续视频流时效果不佳且效率低下。为此，我们在本文中提出了一种新颖的“Learning-In-Video-Stream”（LIVE）框架，旨在实现实时、长序列、与视频流同步的对话，适用于连续视频输入。LIVE框架包括以下三个方面：（1）一个设计用于处理连续流式输入的语言建模目标；（2）一种数据生成策略，将离线时间标注转换为适合流式对话的格式；（3）一个优化的推理管道，以提高在实际视频流中的响应速度。基于Llama-2/Llama-3，我们构建了VideoLLM-online模型，并通过它展示了在处理视频流对话方面的显著优势，例如，在A100 GPU上，该模型能在5分钟视频片段中实现超过10帧每秒的流式对话。此外，VideoLLM-online还在公开的离线视频基准测试（如识别、captioning和预测）上展现出最先进的性能。我们已将代码、模型、数据和演示发布在https://showlab.github.io/videollm-online供人使用。|
|**2024-06-17**|**How Do Large Language Models Acquire Factual Knowledge During Pretraining?**|Hoyeon Chang et.al.|[2406.11813](http://arxiv.org/abs/2406.11813)|null|尽管近期研究表明大型语言模型（LLMs）能够存储大量事实知识，但它们如何在预训练过程中获取这些知识的机制尚不明确。本研究针对这一缺口，探讨了LLMs在预训练期间如何获取和保持事实知识。研究发现了一些关键洞见：首先，出乎意料的是，更多的训练数据对模型获取和保持事实知识的能力并无显著提升。其次，训练步数与记忆遗忘和事实知识泛化之间存在幂律关系，使用重复训练数据的模型遗忘速度更快。第三，增大批量大小可以提高模型抵抗遗忘的能力。总的来说，我们的观察表明，LLMs在预训练中的事实知识获取是通过逐步增加每一步中预训练数据中事实知识出现的概率。然而，这种增加随后会因遗忘而稀释。基于这种理解，我们能够解释一些最近观察到的LLM行为，如长尾知识上的性能不佳，以及去重预训练语料库的好处。|
|**2024-06-17**|**RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content**|Joao Monteiro et.al.|[2406.11811](http://arxiv.org/abs/2406.11811)|null|## 背景  大型语言模型（LLMs）在训练过程中大量依赖自动从互联网抓取的数据，其中包括包含大量通用知识的百科全书（如维基百科），也可能与用于评估LLMs的基准数据集重叠。因此，如果测试集可能已泄露到训练集中，对模型的评估可能会产生误导性的结论。为了推动语言模型的公正评估，我们提出了一种新的测试数据集——RepLiQA，适用于问答和主题检索任务。RepLiQA是一个包含五个分片的测试集，其中四个在本论文发布前未公开或通过LLM API提供。RepLiQA的每个样本由以下四部分组成：（1）由人类标注员创作的虚构场景描述文档（例如新闻文章），这些内容不会出现在互联网上；（2）关于文档主题的问题；（3）直接源自文档信息的正确答案；（4）包含答案的文档段落。这意味着只有当模型能在提供的文档中找到相关内容时，才能生成准确的答案。  我们进行了一项大规模基准测试，包括多个最先进的LLM，以揭示不同类型的和规模的模型在条件语言建模设置下的性能差异。RepLiQA的已发布分片可在以下链接找到：https://huggingface.co/datasets/ServiceNow/repliqa。|
|**2024-06-17**|**Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations**|Rima Hazra et.al.|[2406.11801](http://arxiv.org/abs/2406.11801)|**[link](https://github.com/declare-lab/safety-arithmetic)**|**随着大型语言模型（LLMs）在翻译和问答等应用中的日益重要，确保它们与人类价值观的正确导向变得至关重要。然而，当前的对齐方法在处理动态用户意图和复杂目标时存在困难，使得模型容易生成有害内容。为此，我们提出了一种无需训练的框架——安全算术（Safety Arithmetic），旨在提升LLMs在不同场景下的安全性，包括基础模型、监督微调模型（SFT）和编辑后的模型。安全算术包含两部分：有害内容消除（Harm Direction Removal）以避免不良输出，以及安全对齐（Safety Alignment）以促进安全响应。此外，我们还发布了NoIntentEdit数据集，它揭示了可能导致模型安全风险的编辑实例。实验结果显示，安全算术显著增强了安全措施，减少了过度安全的问题，同时保持了模型的实用性，相较于现有方法在保障内容生成的安全性方面表现出色。**|
|**2024-06-14**|**Quantifying Variance in Evaluation Benchmarks**|Lovish Madaan et.al.|[2406.10229](http://arxiv.org/abs/2406.10229)|null|评价基准是衡量大型语言模型（LLMs）能力的关键，也是推动这些能力进步的驱动力。最初设计用于评估预训练模型的性能（或缺乏），现在它们也被广泛用于决定不同的训练选择之间。然而，尽管被广泛应用，我们很少量化评价基准的方差，这决定了性能差异的含义。本文定义并测量了一系列旨在衡量评价基准方差的指标，包括初始化时的随机种子方差和训练过程中的单调性。通过对大量模型（包括公开可用的和从头训练的模型）进行研究，我们提供了各种方差度量的实证估计，并为实践者提供了考虑和建议。我们还评估了连续和离散性能度量的实用性和权衡，并探索了更好地理解和减少方差的方法。我们发现，对于较小规模（约70亿参数）的模型，如将多模态多任务学习（MMLU）任务框架为完成任务，可以常常降低方差；而受到人类测试文献启发的更复杂方法（如项目分析和项目反应理论）在显著减少方差方面效果有限。总的来说，我们的工作揭示了评价基准的方差特性，提出了针对LLMs的特定技术来减少方差，并普遍鼓励实践者在比较模型时仔细考虑方差因素。|
|**2024-06-14**|**Semantic Membership Inference Attack against Large Language Models**|Hamid Mozaffari et.al.|[2406.10218](http://arxiv.org/abs/2406.10218)|null|## 背景 成员身份泄露攻击（Membership Inference Attacks，MIA）的目标是识别特定数据点是否被纳入了目标模型的训练集。本文提出了一种新颖的方法——语义成员身份泄露攻击（Semantic Membership Inference Attack，SMIA），通过利用输入的语义内容及其扰动，提升MIA的性能。SMIA训练一个神经网络来分析目标模型对扰动输入的行为，从而捕捉成员样本与非成员样本之间输出概率分布的差异。我们在Pythia和GPT-Neo模型家族，以及Wikipedia数据集上进行了全面的评估。实验结果显示，SMIA明显优于现有攻击手段，例如在Pythia-12B上的AUC-ROC值达到了67.39%，而第二好的攻击方法仅为58.90%。|
|**2024-06-14**|**Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs**|Rui Yang et.al.|[2406.10216](http://arxiv.org/abs/2406.10216)|null|在强化学习从人类反馈（RLHF）框架中，利用基于人类偏好数据的奖励模型已证实能有效调整大型语言模型（LLMs）以符合人类意图。然而，当前奖励模型对未见过的提示和响应的泛化能力有限，可能导致所谓的过度优化问题，即奖励优化过度导致实际性能下降。尽管先前的研究倾向于约束策略优化，我们的研究提出了一种新方法，通过正则化隐藏状态来增强奖励模型应对分布变化的泛化能力。具体来说，我们保留基础模型的语言模型头，并结合一系列文本生成损失，旨在保持隐藏状态的文本生成能力，同时在相同的隐藏状态后学习一个奖励头。实验结果表明，引入的正则化技术显著提高了在各种泛化任务中的奖励模型准确性，并有效缓解了RLHF中的过度优化问题，提供了一个更可靠、更稳健的偏好学习范式。|
|**2024-06-14**|**Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs**|Abhimanyu Hans et.al.|[2406.10209](http://arxiv.org/abs/2406.10209)|**[link](https://github.com/ahans30/goldfish-loss)**|**## 背景 大型语言模型能够记住并重复其训练数据，这带来了隐私和版权问题。为了减轻这种记忆，我们提出了一种对下一步 token 训练目标的微妙修改，称为“金鱼损失”。在训练过程中，随机选择一部分令牌不参与损失计算。模型不会记住这些被丢弃的令牌，从而防止了完整训练序列的逐字复制。我们在数十亿规模的 Llama-2 模型上进行了大量实验，包括预训练和从头开始训练，结果显示，我们的方法显著减少了可提取的记忆，而对下游基准的影响微乎其微。**|
|**2024-06-14**|**TRIP-PAL: Travel Planning with Guarantees by Combining Large Language Models and Automated Planners**|Tomas de la Rosa et.al.|[2406.10196](http://arxiv.org/abs/2406.10196)|null|**摘要：**  旅行规划是一个复杂的任务，它涉及根据约束条件生成一系列与访问地点相关的行动，同时最大化用户的满意度。传统方法通常会将问题转化为特定形式的语言表达，从网络资源中提取相关信息，并使用合适的求解器来生成有效解决方案。然而，近期的基于大型语言模型（LLMs）的方法直接从用户请求中输出计划，利用丰富的旅行领域知识提供景点和可能路线等高层次信息。尽管如此，当前最先进的模型往往产生不连贯、未能完全满足约束的计划，且无法保证生成高质量方案。我们提出TRIP-PAL，一种融合LLMs和自动化规划器的混合方法：（1）LLMs获取并转换旅行信息和用户需求，将其转化为可输入规划器的数据结构；（2）自动化规划器负责生成满足约束并优化用户效用的旅行计划。我们在不同旅行场景中的实验表明，TRIP-PAL在生成旅行计划方面优于纯LLM方法。|
|**2024-06-14**|**Detecting and Evaluating Medical Hallucinations in Large Vision Language Models**|Jiawei Chen et.al.|[2406.10185](http://arxiv.org/abs/2406.10185)|null|随着大型视觉语言模型（LVLM）在医疗领域的应用日益增长，如医学图像问答和报告生成，它们从基础大语言模型（LLMs）那里继承了强大的功能，但同时也带来了令人担忧的幻觉问题，这在医疗这样对错误容限极低的环境中尤为重要。然而，目前尚无专门针对医疗领域的幻觉检测和评估方法或基准。为了填补这一空白，我们推出了Med-HallMark，这是首个专为医疗多模态领域设计的幻觉检测和评估基准。Med-HallMark支持多任务幻觉检测，提供多元化的幻觉数据，并采用分级幻觉分类。此外，我们提出了MediHall Score，这是一种新的医疗评估指标，通过分层评分系统评估LVLM的幻觉，考虑其严重程度和类型，从而实现对潜在临床影响的细致评估。我们还展示了MediHallDetector，一种专为精确幻觉检测设计的医疗LVLM，它采用了多任务训练方法。通过广泛的实验，我们在我们的基准上为流行的LVLM设立了基线。实验结果表明，MediHall Score提供了比传统指标更深入理解幻觉影响的能力，并显示了MediHallDetector的提升性能。我们期望这项工作能显著提高LVLM在医疗应用中的可靠性。所有相关资源将在不久后发布。|
|**2024-06-14**|**Practical offloading for fine-tuning LLM on commodity GPU via learned subspace projectors**|Siyuan Chen et.al.|[2406.10181](http://arxiv.org/abs/2406.10181)|null|在大语言模型（LLMs）的微调过程中，由于内存需求通常超过单个GPU的容量，解决这一内存挑战的一个常见方法是将计算和数据从GPU迁移到CPU。然而，这受到普通硬件带宽限制的制约，影响了CPU与GPU之间的通信效率。本文提出了一种名为LSP_Offload的框架，通过学习式的子空间投影器，实现在 commodity 硬件上接近原生速度的大规模语言模型微调。我们的数据驱动方法涉及学习一个高效的稀疏压缩器，以最小化通信并保持最小精度损失。此外，我们引入了一种创新的层级通信调度策略，以最大化通信与计算之间的并行性。因此，我们的框架能够在4GB笔记本GPU上微调13亿参数的模型，在配备24GB内存的NVIDIA RTX 4090 GPU上微调70亿参数的模型，仅比无内存限制的微调慢31%。与最先进的离线框架相比，我们的方法提高了微调吞吐量，最高可达3.33倍，当达到相同准确度时，减少了端到端微调时间的33.1%至62.5%。|
|**2024-06-14**|**Datasets for Multilingual Answer Sentence Selection**|Matteo Gabburo et.al.|[2406.10172](http://arxiv.org/abs/2406.10172)|null|**摘要：**  在设计高效的检索式问答（Question Answering，QA）系统中，答案句子选择（Answer Sentence Selection，AS2）是一个关键任务。然而，由于缺乏标注数据，大多数AS2领域的进展主要集中在英语上。这导致了非英语环境下QA系统的性能与英语系统之间的差距。本论文针对这一问题，我们开发了新的高质量多语言（法语、德语、意大利语、葡萄牙语和西班牙语）AS2数据集，通过使用大型语言模型（Large Language Model，LLM）对现有的英文AS2数据集（如ASNQ、WikiQA和TREC-QA）进行监督自动机器翻译（Automatic Machine Translation，AMT）。我们通过多种实验和不同Transformer架构的评估，验证了我们的方法以及翻译数据集的质量。结果显示，我们的数据集对于构建健壮的多语言AS2模型至关重要，显著缩小了非英语与英语环境下的性能差距。|
|**2024-06-14**|**Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models**|Carson Denison et.al.|[2406.10162](http://arxiv.org/abs/2406.10162)|**[link](https://github.com/anthropics/sycophancy-to-subterfuge-paper)**|**在强化学习中，当人工智能系统学会因训练目标不明确而获得不期望的行为时，就会出现规格游戏现象。这种行为可能从简单的奉承行为发展到更复杂且危险的奖励篡改，即模型直接修改其自身的奖励机制。然而，发现这些复杂行为可能超出探索的范畴。本论文探讨大型语言模型（LLMs）是否会在学习常见规格游戏策略后，泛化到执行更为罕见和明显的行为，包括奖励篡改。我们构建了一个逐步升级的可游戏环境系列，并发现针对早期阶段环境的训练会导致在后续环境中出现更多的规格游戏。令人惊讶的是，一小部分但非零的LLMs，在经历了完整训练课程后，能够零样本地直接修改其奖励函数。重新训练LLMs以避免早期阶段的游戏行为可以减轻但不能完全消除后期环境中的奖励篡改。此外，对可游戏环境进行无害性训练并不能阻止奖励篡改。这些结果表明，LLMs能够从常见的规格游戏策略中泛化到更恶劣的奖励篡改行为，并且要消除这种行为可能并非易事。**|
|**2024-06-14**|**BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack**|Yuri Kuratov et.al.|[2406.10149](http://arxiv.org/abs/2406.10149)|null|近年来，大型语言模型（LLMs）的输入上下文长度显著增加。然而，现有的评估方法未能充分衡量模型处理长篇文本中的事实推理能力。为此，我们提出了BABILong基准测试，旨在测试模型在分布式长文档中跨事实推理的能力。BABILong包括20个多样化的推理任务，如事实链、简单归纳、演绎、计数以及处理列表/集合等。这些任务本身就具有挑战性，而当所需事实分散在长篇自然文本中时，难度进一步提升。我们的评估显示，流行的LLMs实际上只利用了10%-20%的上下文信息，且随着推理复杂性的提高，性能急剧下降。对于替代的上下文推理方法，检索增强生成策略在单事实问题回答上的准确率仅为60%，与上下文长度无关。在上下文扩展方法中，循环记忆Transformer展现出最高性能，可处理长达1100万个令牌的长度。BABILong基准测试可以扩展到任意长度，以支持评估具有更强能力的新模型，并提供了长达100万令牌的分隔。|
|**2024-06-13**|**VideoGPT+: Integrating Image and Video Encoders for Enhanced Video Understanding**|Muhammad Maaz et.al.|[2406.09418](http://arxiv.org/abs/2406.09418)|**[link](https://github.com/mbzuai-oryx/videogpt-plus)**|**在基于语言模型的进展基础上，大型多模态模型（LMMs）在视频理解方面取得了显著进步。然而，现有的视频LMMs依赖于图像或视频编码器处理视觉输入，这些编码器各自存在局限性。图像编码器擅长捕捉帧序列中的丰富空间细节，但缺乏明确的时间上下文；而视频编码器提供时间上下文，但常常受限于计算资源，导致只能处理低分辨率的稀疏帧，从而影响了对空间和上下文的理解。因此，我们提出VideoGPT+，它结合了图像编码器（用于详细的空间理解）和视频编码器（用于全局时序上下文建模）的优势。该模型通过将视频划分为小段，并对来自两者特征的提取应用自适应池化策略，以提高性能。我们的架构在多个视频基准上表现出色，包括VCGBench、MVBench和零样本问答任务。此外，我们开发了一个112K的视频指令集，通过新颖的半自动标注管道进一步提升模型性能。为了全面评估视频LMMs，我们还提出了VCGBench-Diverse，它涵盖了18个广泛视频类别，如生活方式、体育、科学、游戏和监控视频，共4,354个问题-答案对。这个基准测试评估现有LMMs在密集视频描述、空间和时间理解以及复杂推理方面的泛化能力，确保在各种视频类型和动态下的全面评估。代码可在https://github.com/mbzuai-oryx/VideoGPT-plus找到。**|
|**2024-06-13**|**Explore the Limits of Omni-modal Pretraining at Scale**|Yiyuan Zhang et.al.|[2406.09412](http://arxiv.org/abs/2406.09412)|**[link](https://github.com/invictus717/MiCo)**|**我们提议构建全模态智能，旨在理解各种模态并学习通用表示。为此，我们提出了一种可扩展的预训练范式，称为多模态上下文（MiCo）。这种方法能够在预训练过程中同时增加模态数量、数据量以及模型参数的数量。通过MiCo，预训练模型在多项任务上展现出显著的多模态学习能力：一是针对10种不同模态的单模态感知基准，二是包括检索、问答和captioning在内的25项跨模态理解任务，三是18个多模态大语言模型基准。我们的模型创造了37项最新的最高性能记录。我们期望这项研究能推动全模态智能的发展。相关代码和模型已在<https://github.com/invictus717/MiCo>开源。**|
|**2024-06-13**|**Aligning Vision Models with Human Aesthetics in Retrieval: Benchmarks and Algorithms**|Miaosen Zhang et.al.|[2406.09397](http://arxiv.org/abs/2406.09397)|null|现代视觉模型在大规模嘈杂数据集上进行训练，虽然展现出强大能力，但在遵循用户意图、如视觉美感、特定风格和责任输出方面可能存在问题。本文关注视觉美学领域，目标是使视觉模型与人类审美标准在检索系统中保持一致。高级检索系统通常采用基于低级特征（如饱和度）的审美模型作为重排器或过滤器，但面对风格、文化或知识背景时性能有限。我们发现利用大型语言模型（LLM）的推理能力，通过改写搜索查询并扩展审美期望，可以弥补这一不足。  因此，我们提出了一种基于偏好的强化学习方法，该方法针对视觉模型进行微调，以提取LLM推理和审美模型的知识，从而更好地使视觉模型符合人类审美。由于缺乏专门用于评估检索系统的基准，我们利用强大的多模态大模型（LMM）来评价美感表现。考虑到美感评估的主观性，我们还提出了一个名为HPIR的新数据集，用于衡量与人类审美的契合度。实验结果显示，我们的方法显著提升了视觉模型的美感行为，从多个指标来看。我们相信，提出的算法可以作为一种通用实践，用于使视觉模型与人类价值观相一致。|
|**2024-06-13**|**Too Many Frames, not all Useful:Efficient Strategies for Long-Form Video QA**|Jongwoo Park et.al.|[2406.09396](http://arxiv.org/abs/2406.09396)|null|长期视频通常包含大量冗余信息，跨越较长的时间间隔，且包含多个松散关联的事件或实体。因此，在进行长视频问答（LVQA）时，生成正确答案所需的所有信息往往只需一小部分帧就足以提供。近期的研究试图利用大型语言模型（LLMs）在LVQA基准上取得卓越性能，但这些模型依赖于视觉语言模型（VLMs）将视频中的所有视觉内容转换成自然语言。传统做法通常是均匀采样大量帧并独立为其生成描述，这既不高效也不免有冗余。针对这一问题，我们探索了关键帧选择和顺序感知的描述方法，以显著减少这些冗余。  为此，我们提出了两个创新方法：层次关键帧选择器和顺序视觉语言模型。我们的最终框架称为LVNet，在三个基准LVQA数据集上实现了最先进的性能。我们将公开我们的代码。|
|**2024-06-13**|**Needle In A Video Haystack: A Scalable Synthetic Framework for Benchmarking Video MLLMs**|Zijia Zhao et.al.|[2406.09367](http://arxiv.org/abs/2406.09367)|**[link](https://github.com/joez17/videoniah)**|**视频理解是大规模多模态语言模型（MLLMs）的关键下一步。为了检验视频理解的特定方面，现有的视频基准通常需要精心选择与目标能力匹配的视频，并对查询-响应对进行繁琐的标注，以匹配视频内容。这个过程既具有挑战性又资源密集。本文提出VideoNIAH（视频针 haystack），一个通过合成视频生成的基准构建框架。VideoNIAH通过将不相关的图像/文本“针”插入原始视频中，将测试视频内容与它们的查询-响应分离。它仅基于这些针生成注释，确保视频来源的多样性和查询-响应的丰富性。此外，通过插入多个针，VideoNIAH严格评估模型的时序理解能力。我们利用VideoNIAH构建了视频基准VNBench，包括检索、排序和计数等任务。VNBench能够高效地评估视频模型的精细理解能力和时空建模能力，同时支持长距离依赖性的评估。我们还对近期的视频为中心的多模态大型语言模型进行了评估，包括开源和专有模型，提供了全面的分析。尽管专有模型相对于开源模型具有显著优势，但所有现有视频模型在长距离依赖任务上的性能仍然不佳。VideoNIAH是一个简单且高度可扩展的基准构建框架，我们相信它将激发未来视频基准工作的创新。代码和数据已在https://github.com/joez17/VideoNIAH上提供。**|
|**2024-06-13**|**ElicitationGPT: Text Elicitation Mechanisms via Language Models**|Yifan Wu et.al.|[2406.09363](http://arxiv.org/abs/2406.09363)|null|该论文探讨了如何利用无需领域知识的查询来大型语言模型（如ChatGPT）对获取的文本预测进行评分，以评估其与实际状态的一致性。这种方法是激励信息收集和机器学习模型训练的关键组成部分。研究通过在同行评审数据集上进行实验，比较自动的模型评分与人工导师给出的评分，旨在实证评估这些机制与人类偏好的一致性。|
|**2024-06-13**|**DiscreteSLU: A Large Language Model with Self-Supervised Discrete Speech Units for Spoken Language Understanding**|Suwon Shon et.al.|[2406.09345](http://arxiv.org/abs/2406.09345)|null|## 背景  将预训练的文本型大型语言模型（LLMs）与语音输入相结合，已经赋予了这些模型执行多样化语音任务的能力，包括指令跟随。这种整合需要结合语音编码器、语音适配器和LLM，它们分别针对不同的任务进行训练。我们提议使用离散语音单元（DSU），而非连续值的语音编码输出，通过语音适配器将DSU转换到LLM的嵌入空间。我们通过无监督的语音编码器生成DSU，然后运用k-means聚类方法。提出的模型在处理来自见/未见过领域以及口语问答中的指令跟随任务时表现出稳健性能。我们还研究了来自不同自监督语音编码器层的DSU类型，以及梅尔频率倒谱系数（MFCC）。实验结果表明，在口语问答的指令调优任务中，ASR任务和数据集的重要性可能较低。|
|**2024-06-13**|**REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space**|Tomer Ashuach et.al.|[2406.09325](http://arxiv.org/abs/2406.09325)|null|大型语言模型（LLMs）可能无意中记住并泄露训练数据中的敏感或个人识别信息（PII），引发隐私问题。当前的解决方案包括昂贵的数据清洗，或者通过遗忘和模型编辑来过滤模型，但这些方法可能被提取攻击绕过。我们提出了一种新颖的模型编辑方法，名为REVS，用于从LLMs中消除敏感信息。REVS识别并修改与每条敏感信息相关的少量神经元。通过将这些神经元投影到词汇空间（去嵌入），我们定位驱动其生成的关键部分。然后，我们根据去嵌入矩阵的伪逆计算模型编辑，并应用它来降低目标敏感数据的生成概率。为了充分评估我们的方法在真正敏感信息上的效果，我们创建了两个数据集：一个是GPT-J固有的电子邮件数据集，另一个是我们调整模型使其记忆的合成社会保障号码数据集。与最先进的模型编辑方法相比，REVS在消除敏感信息和抵抗提取攻击方面表现出色，同时保持模型的完整性。代码和演示笔记本可在<https://technion-cs-nlp.github.io/REVS>获取。|
|**2024-06-13**|**Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs**|Zhao Xu et.al.|[2406.09324](http://arxiv.org/abs/2406.09324)|**[link](https://github.com/usail-hkust/bag_of_tricks_for_llm_jailbreaking)**|**尽管大型语言模型（LLMs）在零样本任务执行方面展现出显著能力，但它们易受破解攻击，可能被操纵产生有害输出。近期的研究开始将破解攻击分为令牌级和提示级。然而，先前的工作主要忽视了破解攻击的多样关键因素，大部分研究聚焦于LLM的漏洞，而对防御增强的LLMs探索不足。为了改进这一状况，我们评估了不同攻击设置对LLM性能的影响，并提议建立一个基准测试框架，以促进标准化评估。我们从目标级和攻击级两个角度，详细考察了实施针对LLMs的破解攻击的八个关键因素。我们在两个常用数据集上对六种防御方法进行了七种代表性的破解攻击，总计约320个实验，使用A800-80G GPU耗时大约5万小时。实验结果强调了对防御增强的LLMs进行标准化评估的必要性。我们的代码已开源：https://github.com/usail-hkust/Bag_of_Tricks_for_LLM_Jailbreaking。**|
|**2024-06-13**|**JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models**|Delong Ran et.al.|[2406.09321](http://arxiv.org/abs/2406.09321)|**[link](https://github.com/thuccslab/jailbreakeval)**|**本文探讨了针对大型语言模型（LLMs）的越狱攻击研究中的评估难题。目前，对于攻击是否成功缺乏统一标准，不同的评估方法如人工标注或特定方式提示GPT-4存在，各有优缺点，对人类价值观的体现和研究成本产生影响。我们的研究分析了近九十项2023年5月至2024年4月期间发布的越狱攻击相关研究，提出了一种详细的评估方法分类体系，深入剖析了各种评估器的优缺点及其应用现状。为了推动后续研究，我们开发并推出了JailbreakEval工具包，它是一个用户友好的平台，集成了多种知名的评估器，用户只需一个命令即可获取结果。此外，JailbreakEval支持用户在统一框架内定制自定义评估流程，简化了开发和比较过程。总之，我们期望JailbreakEval能促进越狱攻击评价的标准化，成为社区内越狱研究评估的催化剂。**|
|**2024-06-12**|**Improving LLMs for Recommendation with Out-Of-Vocabulary Tokens**|Ting-Ji Huang et.al.|[2406.08477](http://arxiv.org/abs/2406.08477)|null|在推荐系统中，通过向量表示用户和项目对于多种任务至关重要。最近的研究尝试将大型语言模型（LLMs）应用于问答形式的推荐，使用词汇表内的标记（如“item”、“20”、“24”）来表示实际的用户和项目。然而，由于LLMs通常是在自然语言任务上预训练的，这些词汇表内的标记在表达独特用户和项目方面能力有限，即使经过推荐任务的微调，也会削弱推荐性能。本文探讨如何有效在LLM基的推荐系统中处理用户和项目的标记。  我们强调了出词汇表（OOV）标记的作用，它们除了词汇表内的标记外，还能捕捉用户/项目之间的关联性和多样性。通过分析历史用户-项目交互的表示学习，我们使具有相似特性的用户/项目组合共享相同的OOV标记。此外，将这些OOV标记整合到LLM的词汇表中，有助于更好地区分用户和项目，增强在下游任务微调时对用户-项目关系的捕捉。  我们的提出的框架在各种下游推荐任务上超越了现有最先进的方法。|
|**2024-06-12**|**Real2Code: Reconstruct Articulated Objects via Code Generation**|Zhao Mandi et.al.|[2406.08474](http://arxiv.org/abs/2406.08474)|null|我们提出了一种新颖的方法——Real2Code，旨在通过代码生成来重建可动物体。给定物体的视觉观测，我们首先利用图像分割模型和形状补全模型重构其部件几何结构。接着，我们将物体部件表示为带有方向的边界框，然后输入到一个经过微调的大语言模型（LLM）中，预测关节活动的代码表示。通过利用预训练的视觉和语言模型，我们的方法能够优雅地扩展到具有更多可动部件的对象，并能从合成训练数据中泛化到现实世界中的不规则环境物体。实验结果表明，Real2Code在重建精度上显著优于现有最先进的方法，并且是首个能够超越训练集中对象结构复杂性的方法，能够重建多达10个可动部件的物体。当与立体重建模型结合时，Real2Code还能从少量多视图RGB图像中泛化到现实世界的物体，无需深度或相机信息。|
|**2024-06-12**|**Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing**|Zhangchen Xu et.al.|[2406.08464](http://arxiv.org/abs/2406.08464)|null|高质量的指令数据对于调整大型语言模型至关重要。尽管像Llama-3-Instruct这样的模型公开了权重，但它们的对齐数据仍然保密，这限制了人工智能的普及。现有的开源数据生成方法受限于高昂的人力成本和有限的提示范围，难以有效扩展，可能影响公共对齐数据集的多样性和质量。能否通过直接从已对齐的大型语言模型中提取，大规模合成高质指令数据呢？我们提出了一种自我合成方法，称为Magpie。我们的关键观察是，由于Llama-3-Instruct等已对齐的模型具有自回归特性，当我们仅输入左侧模板到用户消息预留位置时，它们可以生成用户查询。我们利用这种方法提示Llama-3-Instruct，生成了400万个指令及其对应的响应。我们对提取的数据进行了全面分析，并选择了30万个高质量实例。为了比较Magpie数据与其他公共指令数据集，我们分别使用每个数据集对Llama-3-8B-Base进行微调，并评估微调后模型的性能。结果显示，在某些任务中，仅使用Magpie进行微调的模型在性能上与官方经过1000万个数据点监督微调（SFT）和后续反馈学习增强的Llama-3-8B-Instruct相当。我们还展示了仅使用Magpie进行SFT可以超越先前用于SFT和偏好优化（如UltraFeedback的直接偏好优化）的公共数据集。这种优势在AlpacaEval、ArenaHard和WildBench等对齐基准测试中表现明显。|
|**2024-06-12**|**TasTe: Teaching Large Language Models to Translate through Self-Reflection**|Yutong Wang et.al.|[2406.08434](http://arxiv.org/abs/2406.08434)|**[link](https://github.com/yutongwang1216/reflectionllmmt)**|**大型语言模型在自然语言处理任务中展现出卓越性能，特别是通过指令调优后，在机器翻译（Machine Translation, MT）等下游任务中的表现有所提升。然而，这些方法未能达到与监督神经机器翻译（Supervised Neural Machine Translation, NMT）系统相当的翻译质量。原因可能是当前使用的简单提示无法充分利用模型的指令跟随能力。为此，我们提出了TasTe框架，即“通过自我反思进行翻译”。该框架包括两个推理阶段：第一阶段，模型被引导生成初步翻译并同时对其自身进行评估；第二阶段，模型根据评估结果对初步翻译进行细化。在WMT22基准的四种语言方向上，我们的方法显示出与现有技术相比的有效性。这项工作展示了一种有前景的方法，能够释放大型语言模型的潜力，并增强其在机器翻译领域的性能。相关代码和数据已在https://github.com/YutongWang1216/ReflectionLLMMT上开源。**|
|**2024-06-12**|**Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL**|Zijin Hong et.al.|[2406.08426](http://arxiv.org/abs/2406.08426)|null|文本转SQL生成准确的SQL查询以响应自然语言问题是一个长期存在的挑战，它涉及用户问题理解、数据库模式理解以及SQL生成等多个复杂环节。传统的文本转SQL系统依赖于人工工程和深度神经网络。随着预训练语言模型（PLMs）的发展和在该任务中的应用，性能得到了显著提升。然而，随着数据库复杂度增加和用户问题难度增大，PLMs有限的理解能力可能导致错误的SQL生成，这促使研究人员寻求更高级和定制化的优化方法，限制了PLM基础系统的广泛应用。最近，大型语言模型（LLMs）因其在自然语言理解上的强大能力而备受瞩目。因此，整合LLM的实现为文本转SQL研究带来了独特的机遇、挑战和解决方案。本综述全面概述了基于LLM的文本转SQL。首先，我们概述当前面临的挑战和文本转SQL的发展历程。接着，详细介绍用于评估文本转SQL系统的数据集和评价指标。然后，我们系统分析了近期在LLM支持下的文本转SQL进展。最后，我们讨论了该领域尚存的挑战，并对未来研究方向提出期待。|
|**2024-06-12**|**OmniCorpus: An Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text**|Qingyun Li et.al.|[2406.08418](http://arxiv.org/abs/2406.08418)|**[link](https://github.com/opengvlab/omnicorpus)**|**该论文介绍了一种名为OmniCorpus的大型图像-文本交错数据集，规模达到100亿级别。这个数据集通过高效的引擎筛选和提取了大量高质量文档，包含86亿张图片和1,696万亿个文本令牌，相较于同类数据（如MMC4、OBELICS），OmniCorpus具有以下优势：1）规模扩大15倍，同时保持了良好的数据质量；2）来源更为多样，包括英文和非英文网站，以及视频为主的网站；3）灵活性更强，可以从图像-文本交错格式轻松转换为纯文本语料库或图像-文本对。通过全面分析和实验，论文验证了OmniCorpus的数据质量、可用性和有效性，旨在为未来的多模态模型研究提供坚实的数据基础。相关的代码和数据已在https://github.com/OpenGVLab/OmniCorpus上公开。**|
|**2024-06-12**|**Discovering Preference Optimization Algorithms with and for Large Language Models**|Chris Lu et.al.|[2406.08414](http://arxiv.org/abs/2406.08414)|**[link](https://github.com/luchris429/DiscoPOP)**|****中文翻译：**  离线偏好优化是提升和控制大型语言模型（LLM）输出质量的重要方法。传统上，偏好优化被视为基于人工设计的凸损失函数的离线监督学习任务。然而，这些方法受限于人类创造力，未能充分探索可能的损失函数的巨大搜索空间。为此，我们提出了一种利用LLM进行目标发现的方法，以自动发现新的最先进的偏好优化算法，无需（专家）人工干预。具体来说，我们通过迭代地提示LLM，根据先前的性能评估提出并实现新的偏好优化损失函数。这个过程导致了未知且高效的优化算法的发现。其中最好的一个被命名为“发现偏好优化”（DiscoPOP），这是一种新颖的算法，它巧妙地融合了逻辑和指数损失。实验结果表明，DiscoPOP在性能上达到了最新水平，并成功地应用于未见过的任务上。**|
|**2024-06-12**|**Memory Is All You Need: An Overview of Compute-in-Memory Architectures for Accelerating Large Language Model Inference**|Christopher Wolters et.al.|[2406.08413](http://arxiv.org/abs/2406.08413)|null|## 背景  大型语言模型（LLMs）近期在自然语言处理领域取得了显著进步，使得机器能够生成逼真的文本并进行有意义的对话。然而，随着计算和内存需求的急剧增长，尤其是当LLMs超越单个GPU的处理能力时，对速度、效率和可访问性的需求也随之增加。同时，计算机性能和内存能力的发展并未跟上步伐，尤其是在摩尔定律放缓的背景下。内存访问成本远高于计算，这给大规模扩展带来了挑战，即所谓的“内存墙”。在这个时候，计算在内存（Compute-in-Memory, CIM）技术为AI推理提供了加速可能，通过在内存中直接执行模拟计算，有望降低延迟和功耗。通过紧密集成内存和计算元件，CIM消除了冯诺依曼瓶颈，减少了数据传输，提高了能源效率。  本综述论文概述了基于变压器的模型，探讨了各种CIM架构，并研究了它们如何应对现代人工智能计算系统面临的紧迫挑战。我们详细讨论了与变压器相关的运算及其硬件加速策略，同时指出相关CIM设计中的挑战、趋势和洞察。|
|**2024-06-12**|**Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models**|Chun-Yi Kuan et.al.|[2406.08402](http://arxiv.org/abs/2406.08402)|**[link](https://github.com/kuan2jiu99/audio-hallucination)**|**## 背景 大型音频语言模型（LALMs）通过整合音频感知能力，增强了传统的大规模语言模型，使其能够处理音频相关任务。先前的研究主要集中在评估LALMs在各种任务上的性能，但对它们的可靠性，特别是关于对象幻觉等问题的关注不足。我们的研究中，我们提出方法来评估公开可用的LALMs在对象幻觉方面的程度。结果表明，LALMs在理解音频内容方面与专门的音频captioning模型相当，但在回答区分性问题时表现不佳，尤其是那些需要识别音频片段中特定物体声音的问题。这揭示了当前LALMs的一个关键弱点：它们对区分性查询的理解不足。此外，我们还探讨了提示工程如何提升LALMs在区分性问题上的性能。**|
|**2024-06-12**|**cPAPERS: A Dataset of Situated and Multimodal Interactive Conversations in Scientific Papers**|Anirudh Sundar et.al.|[2406.08398](http://arxiv.org/abs/2406.08398)|null|## 背景 在情境化和多模态交互对话（SIMMC）的新兴研究领域中，科学论文的互动是一个重要方向。由于科学论文主要由文本、公式、图表和表格构成，SIMMC方法需要针对这些组成部分进行专门设计，以支持科研人员所需的深度探究和互动。本研究提出了一种名为“对话式论文”（cPAPERS）的数据集，它包含了来自arXiv上可用的科学文档的学术论文评论中的问答对，这些问答与论文组件及其引用相关。我们介绍了数据收集策略，通过OpenReview收集这些问题-答案对，并与LaTeX源文件中的上下文信息关联起来。此外，我们展示了使用大型语言模型（LLMs）的一系列基线方法，包括零样本和微调配置，来处理cPAPERS数据集。|
|**2024-06-11**|**Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena**|Aidar Myrzakhan et.al.|[2406.07545](http://arxiv.org/abs/2406.07545)|**[link](https://github.com/vila-lab/open-llm-leaderboard)**|**### 背景  多项选择题（MCQ）常用于评估大型语言模型（LLMs）。通常，LLM会根据调整后的概率，如长度因素，选择最可能的答案。然而，LLMs可能存在固有的偏见，例如对A、B、C、D等选项ID的偏好，这可能影响答案预测。先前的研究通过在少数测试样本上随机打乱选项，并将其应用到新样本上，试图减少这种“选择偏差”。此外，MCQ的另一个问题是“彩票式猜测”，即LLM并未真正学习知识，而是凭运气猜对答案，这对小型LLMs尤为严重。  为解决这些问题，一个更全面的方法是转向开放式问题，这能从根本上消除选择偏差和随机猜测。但转向开放式问题也带来了挑战：一是如何识别合适的开放性问题，二是如何验证LLM对开放式问题的回答与人类标注的真实答案之间的准确性。本研究旨在解决这些难题，并建立一个新的LLM评估基准，通过完全的开放式问题来衡量模型性能，例如GPT-4o/4/3.5、Claude 3、Gemini等。  ### 任务  我们创建了Open-LLM-Leaderboard，这是一个新的评价平台，旨在跟踪各种LLM的表现，揭示它们的真实能力。我们的代码和数据集已开源，可在此链接获取：https://github.com/VILA-Lab/Open-LLM-Leaderboard。**|
|**2024-06-11**|**QuickLLaMA: Query-aware Inference Acceleration for Large Language Models**|Jingyao Li et.al.|[2406.07528](http://arxiv.org/abs/2406.07528)|**[link](https://github.com/dvlab-research/q-llm)**|**大型语言模型（LLMs）在理解和处理长序列方面的能力对于各领域的发展至关重要。然而，它们在捕捉序列中的长期依赖关系以深入理解语义方面仍然存在挑战。为此，我们提出了Query-aware Inference for LLMs（Q-LLM），这是一种旨在模仿人类认知处理大规模序列的系统。通过聚焦于与给定查询相关的内存数据，Q-LLM能够在固定窗口大小内准确捕捉相关信息，并为查询提供精确的答案，无需额外训练，可无缝集成到任何LLMs中。使用LLaMA3（QuickLLaMA）的Q-LLM能在30秒内阅读《哈利·波特》，并能准确回答问题。相较于当前最先进的LLaMA3，Q-LLM的性能提升了7.17%，而在Mistral上，它在 $\infty$ -bench上的表现提升了3.26%。在“针锋相对”任务中，Q-LLM在广泛认可的基准上，相对于当前最佳成绩，Mistral上的提升达到了7.0%，在LLaMA3上实现了100%的准确率。我们的代码已在https://github.com/dvlab-research/Q-LLM上开源。**|
|**2024-06-11**|**Beyond Model Collapse: Scaling Up with Synthesized Data Requires Reinforcement**|Yunzhen Feng et.al.|[2406.07515](http://arxiv.org/abs/2406.07515)|null|随着生成模型合成数据的兴起，越来越多地被用于大型语言模型的微调，这引发了对模型崩溃（即微调性能下降）的关注。由于人类和机器都较容易分辨好样本和坏样本，而非生成高质量样本，我们探讨了如何利用反馈来防止模型在合成数据上出现崩溃。我们理论分析了一个高斯混合分类模型在基于反馈增强的合成数据训练下的最优性能，并提供了有限样本情况下的实验证据。我们在两个实际问题上展示了这些理论预测：使用变压器计算矩阵特征值和利用大型语言模型进行新闻摘要，这两种情况下模型在生成数据上都会经历崩溃。我们发现，通过从反馈增强的合成数据中训练，无论是修剪错误预测还是选择最佳猜测，都能防止模型崩溃，证实了像RLHF（Reinforcement Learning with Human Feedback）这样的流行方法的有效性。|
|**2024-06-11**|**THaLLE: Text Hyperlocally Augmented Large Language Extension -- Technical Report**|KBTG Labs et.al.|[2406.07505](http://arxiv.org/abs/2406.07505)|null|## 背景  近期大型语言模型（LLMs）的进步在科技领域展现了新功能和机遇。然而，非常大的LLMs的实际应用受到其高计算成本的制约，这与其相对有限的人类能力相比，收益并不明显。尽管小型、更实用的LLMs在金融分析方面展现出潜力，但它们尚未完全掌握，如它们在模拟特许金融分析师（CFA）考试中的接近通过表现所示。本文中，我们展示了Financial Analyst Extension（FAE）对我们的Text Hyperlocally Augmented Large Language Extension（THaLLE）系列的扩展，这一系列80亿参数的LLMs在模拟CFA考试中始终表现出最高性能，与同类规模的模型相比。我们详细记录了用于优化的微调技术，以供后续研究参考。此外，我们引入Flare CFA，这是一个公开可用的金融顾问评估数据集，用于检验LLMs在财务顾问角色中的能力。|
|**2024-06-11**|**Image Textualization: An Automatic Framework for Creating Accurate and Detailed Image Descriptions**|Renjie Pi et.al.|[2406.07502](http://arxiv.org/abs/2406.07502)|**[link](https://github.com/sterzhang/image-textualization)**|**## 背景  图像描述数据集对于推动图像理解、文本到图像生成和文本图像检索等应用至关重要。当前，这些数据集主要来自两个途径：一是从网络上抓取图像与文字对，但这类描述往往质量较低且存在噪声；二是人工标注，如COCO等，通常描述简洁，缺乏详细信息。尽管详细的图像描述可以通过人类标注获得，但高昂的标注成本限制了其可行性。这些局限性促使我们寻求更有效和可扩展的方法来生成准确而详尽的图像描述。  本文提出了一种创新框架，称为“图像文本化”（Image Textualization，简称IT），它通过协同利用现有的多模态大型语言模型（Multimodal Large Language Models，MLLMs）和视觉专家模型，有效地将视觉信息转化为文本，从而自动生成高质量的图像描述。针对当前缺乏详尽描述的基准问题，我们还提出了多个评价基准，以全面评估我们的框架生成的图像描述质量。  此外，我们展示了在IT精心编纂的描述训练下，LLaVA-7B模型的图像描述生成能力得到了提升，能够生成更丰富的描述，输出长度和细节显著增加，同时减少了幻觉现象。**|
|**2024-06-11**|**TextGrad: Automatic "Differentiation" via Text**|Mert Yuksekgonul et.al.|[2406.07496](http://arxiv.org/abs/2406.07496)|**[link](https://github.com/zou-group/textgrad)**|**人工智能正经历一场范式转变，通过大型语言模型（LLMs）和其他复杂组件的协同工作取得了突破。当前，为复合人工智能系统设计原则化的自动化优化方法成为一项关键新挑战。神经网络在早期面临类似问题时，通过反向传播和自动微分实现了重大革新。受此启发，我们提出了TextGrad，这是一个强大的框架，它通过文本实现自动“微分”，将LLMs提供的丰富、通用的自然语言建议回传到复合AI系统的各个组件中。TextGrad遵循PyTorch的语法和抽象，易于使用且灵活，用户仅需提供目标函数，无需调整框架组件或提示，即可无缝应用。  TextGrad适用于多种任务，从问答和分子优化到放射治疗计划设计。在无需修改框架的情况下，它显著提升了GPT-4o在Google证明性问题回答中的零-shot准确率，从51%提升至55%；在优化LeetCode难题解法上实现了20%的相对性能提升；改进了推理提示，设计出具有理想体外亲和力的新药候选分子；以及设计出具有高特异性的放射治疗方案。TextGrad为下一代AI系统的发展奠定了基础，推动了复合AI技术的加速发展。**|
|**2024-06-12**|**CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization**|Frederic Kirstein et.al.|[2406.07494](http://arxiv.org/abs/2406.07494)|null|该文章综述了2019年至2024年间发表的1262篇独特的研究论文，集中在Transformer架构在英文对话摘要生成方面的研究。文章详细探讨了对话摘要中存在的主要挑战，如语言理解、结构处理、理解能力、说话者识别、重要性判断和事实准确性，并与相应的技术，如图解方法、额外训练任务和规划策略进行了关联。尽管在某些方面（如语言）取得了显著进展，但如理解力、真实性与重要性评估等挑战仍然存在，提供了丰富的研究空间。  文章还分析了评估这些方法的方式，涵盖了对话子领域（如会议、医疗）的常用数据集，以及自动评价指标（如ROUGE）和人类评估的普遍实践。然而，发现跨领域的数据集相对有限，且报告的人类评估往往缺乏足够的内审员一致性信息和标注指南细节。此外，文章讨论了大语言模型的最新探索可能带来的影响，指出尽管它们可能会改变相关性和难度，但描述的挑战分类体系仍然具有价值。|
|**2024-06-11**|**PITCH: Productivity and Mental Well-being Coaching through Daily Conversational Interaction**|Adnan Abbas et.al.|[2406.07485](http://arxiv.org/abs/2406.07485)|null|高效的计划制定对生产力和心理健康至关重要，但人们往往难以制定实际的计划并反思自己的效率。利用人工智能的发展，对话助手作为一种有前景的工具，旨在通过对话方式将计划外化，强化决心，促进专注行动，从而正面影响生产力和心理健康。我们的研究目标是设计一个对话助手，通过自然对话的社交互动性，提供深入的问题和反思提示，以提高计划执行度。尽管先前的研究显示了这些代理的效益，但许多干预措施仍保持静态，可能导致用户参与度随时间下降。为了弥补这一不足，我们提出了一种新颖的旋转和上下文感知的提示策略，每天为用户提供多样的干预手段。我们的系统PITCH利用大语言模型（LLMs）来促进日常计划的外部化和反思。本研究旨在探究与对话代理一起外化任务对生产力和心理健康的影响，以及旋转策略在保持用户参与度方面的有效性。|
|**2024-06-11**|**Advancing Annotation of Stance in Social Media Posts: A Comparative Analysis of Large Language Models and Crowd Sourcing**|Mao Li et.al.|[2406.07483](http://arxiv.org/abs/2406.07483)|null|在快速发展的自然语言处理领域，大型语言模型（LLMs）在社交媒体帖子的自动文本标注方面展现出浓厚兴趣。本文研究了八种开源和专有LLMs在立场标注任务中的性能，将其与人类（通过众包）的判断进行基准测试。我们探究了何时LLMs可能与人类判断产生分歧的情况。研究发现，文本中表达立场的明确程度对LLMs判断与人类一致性至关重要。当人类注释者表现良好时，LLMs也表现出色；反之，LLMs的失败往往对应于人类难以达成一致的情境。因此，我们建议结合人类专业知识的精确度与LLMs预测的规模，提出一种全面的方法。这项研究强调了提高自动化立场检测准确性和全面性的必要性，旨在推动这些技术在更高效、无偏见的社会媒体分析中得到提升。|
|**2024-06-11**|**VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs**|Zesen Cheng et.al.|[2406.07476](http://arxiv.org/abs/2406.07476)|**[link](https://github.com/damo-nlp-sg/videollama2)**|**本文介绍VideoLLaMA 2，一套专为提升视频和音频定向任务中的空间-时间建模及音频理解能力而设计的视频大型语言模型（Video-LLMs）。它在前一代的基础上增添了定制的时空卷积（STC）连接器，有效地捕捉视频数据的复杂空间和时间动态。此外，我们通过联合训练融入了音频分支，增强了模型的多模态理解能力，使其能无缝融合音频线索。在多项评估中，如多选视频问答（MC-VQA）、开放性视频问答（OE-VQA）和视频captioning（VC）任务上，VideoLLaMA 2表现出与开源模型相当的竞争实力，并在某些基准上接近专有模型。在音频仅用（AQA）和音频-视频问答（OE-AVQA）任务上，VideoLLaMA 2也显示出对现有模型的合理改进。这些进步凸显了VideoLLaMA 2在多模态理解方面的卓越性能，为智能视频分析系统树立了新标准。所有模型均公开以促进进一步研究。**|
|**2024-06-10**|**Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation**|Peize Sun et.al.|[2406.06525](http://arxiv.org/abs/2406.06525)|**[link](https://github.com/foundationvision/llamagen)**|**我们提出LlamaGen，这是一种全新的图像生成模型家族，它将大型语言模型的原始“下一个词预测”范式应用于视觉生成领域。这表明，如果适当扩展，未经视觉特性的先验知识增强的纯自回归模型（如Llama）也能达到最先进的图像生成性能。我们的研究探索了图像分词器的设计空间、图像生成模型的可扩展性以及训练数据质量，结果如下：(1) 一种具有16倍下采样的图像分词器，其在ImageNet基准上的重构质量为0.94，代码书利用率高达97%。(2) 一系列从111百万到31亿参数的类条件图像生成模型，在ImageNet 256x256基准上实现了2.18的FID分数，超越了流行的扩散模型，如LDM和DiT。(3) 一个7.75亿参数的文本条件图像生成模型，通过两阶段训练在LAION-COCO和高审美质量图像上，显示出良好的视觉质量和文本一致性性能。(4) 我们验证了大语言模型服务框架在优化图像生成模型推理速度方面的有效性，实现了326%至414%的速度提升。我们开源所有模型和代码，以促进视觉生成和多模态基础模型的开放源代码社区的发展。**|
|**2024-06-10**|**UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor**|Shivani Upadhyay et.al.|[2406.06519](http://arxiv.org/abs/2406.06519)|**[link](https://github.com/castorini/umbrela)**|**## 翻译  大量相关性判断对于检索系统的有效训练和精确评估至关重要。传统上，这些判断由人工评定员完成，过程昂贵且耗时。微软Bing的Thomas等人最近的一项研究表明，大型语言模型（LLMs）能够准确地进行相关性评估，提供与人类相当的判断。遗憾的是，他们的研究并未公开可供重复使用的软件工具。我们的工作介绍了一个开源工具包——UMBRELA（全称为“UMBRELA是Bing RELevance Assessor的递归缩写”），它基于OpenAI的GPT-4模型复现了Thomas等人的结果，并为原论文增添了更多细节。我们在TREC 2019年至2023年的深度学习任务中发现，LLM生成的相关性判断与高效多阶段检索系统生成的排名高度相关。该工具包设计为易于扩展，可以融入现有的多阶段检索和评估流程，为研究检索评估方法的研究者提供了宝贵的资源。UMBRELA将在TREC 2024年的RAG任务中用于辅助相关性评估，我们期望它成为该领域进一步创新的基础。UMBRELA的代码库可于https://github.com/castorini/umbrela获取。**|
|**2024-06-10**|**NarrativeBridge: Enhancing Video Captioning with Causal-Temporal Narrative**|Asmar Nadeem et.al.|[2406.06499](http://arxiv.org/abs/2406.06499)|null|当前的视频字幕基准和模型在表征因果时间叙事方面存在不足，这种叙事是通过因果关系连接的一系列事件，随时间发展，由人物或主体驱动。这种缺乏叙事性限制了模型生成捕捉视频内容内在因果和时间动态的文本描述的能力。为填补这一空白，我们提出NarrativeBridge，它包括以下两个组成部分：（1）一个由大型语言模型通过少量提示生成的新型因果时间叙事（CTN）字幕基准，该基准明确地在视频描述中编码因果关系，通过自动评估确保质量和相关性；（2）一个专门的因果网络（CEN）架构，具有独立的编码器以分别捕获因果动态，从而实现有效的学习和生成具有因果时间叙事的字幕。实验结果表明，CEN在表达视频内容的因果和时间方面比第二好的模型（GIT）更准确：在MSVD和MSR-VTT数据集上的CIDEr分数分别为17.88和17.44。提出的框架能够理解和生成具有复杂因果时间叙事结构的细微文本描述，这是视频字幕生成的一个关键局限性。有关项目详情，请访问<https://narrativebridge.github.io/>。|
|**2024-06-10**|**Towards a Personal Health Large Language Model**|Justin Cosentino et.al.|[2406.06474](http://arxiv.org/abs/2406.06474)|null|在健康领域，大部分大型语言模型（LLM）的研究集中在临床任务上。然而，移动和可穿戴设备提供的丰富、长期的个人健康监测数据往往被忽视。本文介绍了一种名为Personal Health Large Language Model（PH-LLM）的新模型，它是Gemini的定制版，专为理解和处理数值时间序列的个人健康数据而设计。我们创建并整理了三个测试集，考察了PH-LLM在以下方面的性能：1）从睡眠模式、身体活动和生理反应中生成个性化见解和建议；2）专业知识领域的专家水平；3）预测自我报告的睡眠结果。我们与领域专家合作构建了857个案例研究，以评估实际的睡眠和健身场景。通过针对特定领域的评分标准进行全面评估，我们发现Gemini Ultra 1.0和PH-LLM在健身方面与专家表现无统计差异，尽管在睡眠方面专家仍占优势，但Fine-tune后的PH-LLM在利用相关领域知识和个人化睡眠信息方面表现出显著提升。我们还通过多项选择的睡眠医学和健身考试评估了PH-LLM的专业知识，其得分分别为79%和88%，超过了人类专家样本的平均分。最后，我们训练PH-LLM预测来自可穿戴设备文本和多模态编码数据的自我报告睡眠质量结果，并证明了多模态编码对于达到专门区分模型的性能至关重要。尽管在个人健康这个关键安全领域还需要进一步发展和评估，但这些结果展示了Gemini模型的广泛知识和能力，以及将生理数据应用于个人健康应用，如PH-LLM中的做法。|
|**2024-06-10**|**AID: Adapting Image2Video Diffusion Models for Instruction-guided Video Prediction**|Zhen Xing et.al.|[2406.06465](http://arxiv.org/abs/2406.06465)|null|文本引导的视频预测（TVP）任务旨在根据初始帧和指令预测后续帧的运动，这对于虚拟现实、机器人技术和内容创作等领域具有广泛的应用。尽管先前的方法通过改编Stable Diffusion在该任务上取得了重大进展，但它们在帧一致性与时间稳定性方面仍存在问题，主要受限于视频数据集的规模。我们观察到，预训练的Image2Video扩散模型对视频动态有良好的先验知识，但缺乏文本控制。因此，将Image2Video模型转移，同时注入指令控制以生成可控制的视频，既具有意义又颇具挑战。  为了实现这一目标，我们提出了多模态大型语言模型（MLLM），用于根据初始帧和文本指令预测未来的视频状态。特别地，我们设计了双查询Transformer（DQFormer）架构，它将指令和帧信息整合到条件嵌入中，用于未来帧的预测。此外，我们开发了长短期时序适配器和空间适配器，能够在少量训练成本下快速将通用视频扩散模型适应特定场景。  实验结果表明，我们的方法在Something Something V2、Epic Kitchen-100、Bridge Data和UCF-101四个数据集上显著优于现有技术。特别是在Bridge数据集和SSv2上，AID分别实现了91.2%和55.5%的FVD改进，这证明了其在不同领域的有效性。更多示例可在我们的网站<https://chenhsing.github.io/AID>找到。|
|**2024-06-10**|**Transforming Wearable Data into Health Insights using Large Language Model Agents**|Mike A. Merrill et.al.|[2406.06464](http://arxiv.org/abs/2406.06464)|null|尽管可穿戴健康追踪器日益普及，睡眠和运动对健康的重要性不言而喻，但从这些数据中提取具有行动价值的个性化见解仍是一个挑战。这需要对大量数据进行非结构化分析。随着大型语言模型（LLM）的兴起，它们能够利用工具理解和与世界互动，为大规模个性化分析带来了希望。然而，在个人健康领域的LLM应用尚待开发。本文介绍了一种名为Personal Health Insights Agent（PHIA）的系统，它利用最新的代码生成和信息检索工具来分析和解释行为健康数据。我们构建了两个超过4000个健康洞察问题的基准问答数据集。根据650小时的人类和专家评估，PHIA能准确回答84%以上的事实性数值问题，以及超过83%的众包开放性问题。这项工作对于推动大众行为健康进步具有重要意义，可能使个人能够解读自己的可穿戴数据，开辟了一个以数据驱动洞察为指导的个性化健康方案的新时代，使得健康保健更加便捷且个性化。|
|**2024-06-11**|**Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies**|Junlin Wang et.al.|[2406.06461](http://arxiv.org/abs/2406.06461)|null|这篇论文指出，尽管已经提出了多种推理策略来评估大型语言模型的能力，但传统的评价方法仅关注性能指标，忽视了一个关键因素：额外计算资源带来的增效。这可能导致对策略效率的片面理解。为此，论文提出了一种框架，将计算预算纳入评估，以提供一个既考虑性能指标又考虑计算成本的更全面比较。通过这种预算意识的视角，研究发现复杂的推理策略在没有显著算法创新的情况下，往往由于分配了更多的计算资源而超越了简单的基线。例如，当给予链式思考自洽性（chain-of-thought self-consistency）类似级别的计算资源，它常常能优于文献中提出的推理策略。然而，在这种规模敏感的视角下，某些策略如多代理辩论或多反思在增加计算预算时可能会表现得更差。|
|**2024-06-10**|**Evaluating the Retrieval Component in LLM-Based Question Answering Systems**|Ashkan Alinejad et.al.|[2406.06458](http://arxiv.org/abs/2406.06458)|null|## 背景  大规模语言模型（LLMs）驱动的问答系统在依赖检索组件时，能够获取领域特定信息并降低产生不准确回复或错误信息的风险。尽管信息检索领域的评估方法早已存在，但如何评估LLMs驱动的聊天机器人中的检索器性能仍是一个挑战。本研究提出了一种简单的基准方法，用于评价基于检索增强生成（Retrieval-Augmented Generation，RAG）的聊天机器人中的检索器。  ## 任务  我们的研究发现，这种方法能更全面地反映检索器的性能，并与整个问答系统的整体表现更为一致。尽管传统的精确度（precision）、召回率（recall）和F1分数等指标可能无法完全揭示LLMs的能力，因为它们可能会在检索器不完美时仍提供准确答案，但我们的评估方法考虑到了LLMs的优势，即它们能够忽略无关上下文，同时也能处理可能存在的错误和虚构内容。|
|**2024-06-10**|**A Large Language Model Pipeline for Breast Cancer Oncology**|Tristen Pool et.al.|[2406.06455](http://arxiv.org/abs/2406.06455)|null|大型语言模型在众多领域展现出创新潜力，但在癌症治疗方面的应用仍需进一步开发。研究者使用一种新颖的Langchain提示工程管道，对最先进的OpenAI模型进行了微调，数据集包括临床数据和临床指南文本，专注于乳腺癌患者辅助放疗和化疗两个关键治疗因素。结果显示，模型在分类这两个治疗手段时达到了高精度（0.85+）。通过观察人类肿瘤学家的治疗质量数据，建立了一个置信区间，估计模型在预测治疗方案时必须比原始肿瘤学家表现得更好，才能在总体上成为更好的解决方案的比例为8.2%至13.3%。由于癌症治疗决策结果的不确定性，未来可能需要进行临床试验来验证这一阈值。考虑到美国85%的癌症患者在地方社区设施接受治疗，这类模型有可能显著扩大优质护理的可及性，其效果至少接近人类肿瘤学家。|
|**2024-06-10**|**Insights from Social Shaping Theory: The Appropriation of Large Language Models in an Undergraduate Programming Course**|Aadarsh Padiyath et.al.|[2406.06451](http://arxiv.org/abs/2406.06451)|null|大型语言模型（LLMs）在代码生成、调试和解释方面的性能引发了许多研究者和教育工作者对本科编程教育的关注，他们期待这些模型能革新编程教学。然而，关于如何以及为何在编程教育中使用LLMs的决策可能不仅仅基于技术评估。本研究以社会塑造技术理论为指导框架，探讨了学生对LLMs的社会感知如何影响他们的使用行为。我们通过分析一份匿名的课程结束时的调查问卷（n=158）、中期自我效能问卷（n=158）、10位学生的深度访谈、自我报告的LLM在作业中的使用情况，以及期中考试成绩，发现学生的LLM使用与其对未来职业的期望和对同伴使用的感知有关。此外，我们发现早期自我报告的LLM使用与较低的自我效能和中期考试成绩相关，而学生对过度依赖LLM的感知，而非实际使用，与课程后期的自我效能下降有关。|
|**2024-06-07**|**3D-GRAND: Towards Better Grounding and Less Hallucination for 3D-LLMs**|Jianing Yang et.al.|[2406.05132](http://arxiv.org/abs/2406.05132)|**[link](https://github.com/sled-group/3D-GRAND)**|在这个研究中，语言与三维感知的融合对于构建理解和互动于物理世界的实体代理和机器人至关重要。尽管大型语言模型（LLMs）在语言理解和生成方面表现出色，但在适应三维环境（3D-LLMs）方面仍处于初级阶段，主要挑战在于缺乏大规模的密集地将语言与三维场景关联的数据集。为此，我们提出了3D-GRAND，这是一个开创性的大型数据集，包含40,087个家庭场景，配对有620万条详尽的场景-语言指令。实验结果显示，使用3D-GRAND进行指令调优显著提高了3D-LLMs的定位能力，并减少了错误的想象。我们还设计了3D-POPE基准，用于系统性评估3D-LLMs中的幻觉问题，以促进未来模型的公平比较。  我们的实验揭示了数据集规模与3D-LLM性能之间的关联，强调了大型三维文本数据集在推动体感AI研究中的关键作用。值得注意的是，初步迹象表明，通过在大型合成数据上训练的模型可能在现实世界3D扫描中表现良好，这展示了模拟到实际的迁移学习潜力。通过3D-GRAND和3D-POPE，我们旨在为体感AI社区提供必要的资源和洞见，推动更可靠、更扎实的3D-LLMs的发展。项目网站：https://3d-grand.github.io|
|**2024-06-07**|**An Empirical Study on Parameter-Efficient Fine-Tuning for MultiModal Large Language Models**|Xiongtao Zhou et.al.|[2406.05130](http://arxiv.org/abs/2406.05130)|null|这篇论文关注的是大型多模态语言模型（MLLMs）的参数高效微调（PEFT）。由于这些模型通常具有数十亿参数，全面调整变得困难。研究目标是找出在参数受限情况下提升MLLM性能的有效方法。通过实验使用四种流行的PEFT技术对开源MLLMs的LLM组件进行微调，论文进行了详尽的分析，内容包括不同方法对模型、参数位置、微调数据规模、模型稳定性、泛化能力以及幻觉的影响。研究涵盖了两种类型的七项数据集：未见过的和已见过的。结果显示，适配器是最有效的PEFT方法，而连接器层的微调在大多数情况下能提高性能。研究代码和数据可在<https://github.com/alenai97/PEFT-MLLM.git>获取。|
|**2024-06-07**|**Towards Semantic Equivalence of Tokenization in Multimodal LLM**|Shengqiong Wu et.al.|[2406.05127](http://arxiv.org/abs/2406.05127)|null|### 背景  多模态大型语言模型（MLLMs）在处理视觉语言任务方面展现出卓越性能。MLLM的核心在于视觉 tokenization，即如何有效地将输入的视觉信号转化为对语言模型有益的特征表示。然而，现有的视觉tokenizer在保持视觉与语言的语义一致性上存在问题，它们过于碎片化视觉输入，破坏了视觉内容的语义完整性。为解决这一问题，本文提出了一种新颖的动态语义等效视觉tokenizer（SeTok），它通过动态聚类算法将视觉特征组织成语义单元，根据图像复杂性灵活决定token的数量。这种生成的视觉tokens能有效保持语义完整性，同时捕捉低频和高频视觉特征。  ### 任务  我们提出了一种名为Setokim的新型MLLM，它结合了SeTok。实验结果表明，Setokim在各种任务上表现出显著的优势。关于更多详情，可以访问项目网页：https://chocowu.github.io/SeTok-web/。|
|**2024-06-07**|**LINX: A Language Driven Generative System for Goal-Oriented Automated Data Exploration**|Tavor Lipman et.al.|[2406.05107](http://arxiv.org/abs/2406.05107)|null|## 翻译  数据探索是一个复杂的过程，用户通过逐步执行一系列查询来审视数据集。有时，用户会探索新数据以熟悉它，但更多时候，探索过程是围绕特定分析目标或问题进行的。为了帮助用户有效探索，已提出自动化数据探索（Automated Data Exploration，ADE）系统，它们旨在自动生成展示数据有趣特性的完整探索流程。然而，现有的ADE系统常受限于预定义的优化函数，导致对同一数据集始终产生相同的探索序列，这在有明确目标的探索中显得不足。为此，本文提出LINX，一个结合自然语言接口的生成式系统，专注于面向目标的数据探索。  LINX接受输入数据集和用自然语言描述的分析目标，生成与用户需求相关的个性化探索会话。系统利用大型语言模型解析输入的分析目标，并据此生成期望输出探索会话的规范。这些规范随后被传递给基于约束深度强化学习（Constrained Deep Reinforcement Learning，CDRL）的新型模块化ADE引擎，使其能根据指定指令调整输出。为了验证LINX的效果，我们创建了一个新的面向目标探索的基准数据集，并进行了深入的用户研究。实验结果表明，LINX生成的探索笔记本在相关性和实用性上显著优于现有解决方案，包括ChatGPT、无目标导向的ADE以及商业系统。|
|**2024-06-07**|**Multi-Head RAG: Solving Multi-Aspect Problems with LLMs**|Maciej Besta et.al.|[2406.05085](http://arxiv.org/abs/2406.05085)|**[link](https://github.com/spcl/mrag)**|**## 背景  **增强型检索生成（Retrieval Augmented Generation, RAG）**通过将文档内容融入大语言模型（Large Language Models, LLMs）的上下文中，提高了其响应的准确性和相关性。然而，现有的RAG方法并未充分处理那些可能需要检索包含不同内容的多文档查询。这类问题在现实中很常见，但挑战在于，这些文档的嵌入在向量空间中可能相距较远，难以一次性获取。本文提出了一种新的方案——**多头检索增强生成（Multi-Head RAG, MRAG）**，它以一种简单而强大的方式解决这个问题：利用Transformer的多头注意力层的激活作为检索键，而非解码层。这个想法的驱动力在于，不同的注意力头能够学习捕捉数据的不同方面。通过利用这些激活，我们得到的嵌入能代表数据项和查询的多种特性，从而提升复杂查询的检索精度。  **贡献**  我们提供了评估方法、度量标准、合成数据集以及实际应用案例，来展示MRAG的有效性。与标准RAG基线相比，MRAG在相关性方面的提升可高达20%。MRAG可以无缝融入现有的RAG框架，如RAGAS，以及各类数据存储系统。  总结，本文旨在改进现有RAG模型，以更好地处理涉及多角度信息检索的复杂查询任务。**|
|**2024-06-07**|**Are Large Language Models More Empathetic than Humans?**|Anuradha Welivita et.al.|[2406.05063](http://arxiv.org/abs/2406.05063)|null|随着大型语言模型（LLMs）的兴起，研究它们是否能在情感识别和共情回应方面超越人类已成为研究焦点。本论文开展了一项深入研究，对比了包括GPT-4、LLaMA-2-70B-Chat、Gemini-1.0-Pro和Mixtral-8x7B-Instruct在内的四款最先进的LLMs与人类在共情回应能力上的表现。我们通过一项涉及1,000名参与者的双盲用户研究，对2,000个精心挑选的情感对话提示进行了分析，这些提示涵盖了32种不同正负情绪的广泛范围。研究结果显示，LLMs的共情回应能力在统计学上优于人类。GPT-4表现出最强烈的共情，其“好”等级别的回复比人类基准提高了约31%。紧随其后的是LLaMA-2，提升了约24%，Mixtral-8x7B提升了约21%，Gemini-Pro提升了约10%。我们还对回复评级进行了更详细的分析，发现某些LLMs在回应特定情绪方面明显优于其他模型。提出的评估框架提供了一种可扩展且适应性强的方法，用于评估新LLMs的共情能力，避免了未来研究重复这项研究的必要性。|
|**2024-06-07**|**Robustness Assessment of Mathematical Reasoning in the Presence of Missing and Contradictory Conditions**|Shi-Yu Tian et.al.|[2406.05055](http://arxiv.org/abs/2406.05055)|null|大型语言模型在推理任务上表现出色，通过少量示例提示可以进一步提升性能。然而，当前的评估主要集中在精心构建的基准上，忽视了现实世界中存在缺失和矛盾条件的推理问题，即所谓的不明确问题。我们的观察表明，现有的少量提示方法在这种情况下效果不佳，往往给出过度自信的答案或错误推断。为了深入研究这个问题，我们创建了一个名为“带有缺失和矛盾条件的问题”（PMC）的基准，并引入了两个新指标来评估少量提示方法在处理这类问题时的表现。使用PMC基准的分析揭示了在解决明确问题的数学推理性能与识别不明确问题能力之间存在权衡。针对PMC带来的挑战，我们提出了一种新颖的少量提示方法，称为SMT-LIB提示（SLP）。这种方法利用SMT-LIB语言描述问题，而不是直接求解，然后采用双重检查求解策略验证解决方案的满足性和唯一性，从而提供最终反馈。实验结果全面展示了我们的SLP方法在处理带有缺失和矛盾条件的问题时，相较于现有方法具有显著优势。我们将开源我们的基准和代码，以促进未来的研究。|
|**2024-06-07**|**Hints-In-Browser: Benchmarking Language Models for Programming Feedback Generation**|Nachiket Kotalwar et.al.|[2406.05053](http://arxiv.org/abs/2406.05053)|null|### 概述  生成式人工智能和大型语言模型在编程教育中的潜力巨大，它们能够为学习者提供个性化的反馈和提示。当前的研究主要集中在提升生成反馈的质量，以达到人类导师的水平。然而，在实际教育部署中，除了质量外，成本、时间及数据隐私也是关键考量因素。本论文旨在对语言模型在编程反馈生成方面的性能进行全面评估，包括质量、成本、速度和数据隐私等多个维度。我们特别关注利用最新的在浏览器内推理技术，这有助于直接降低成本并保护数据隐私。  为了优化适合浏览器内运行的小型模型的反馈质量，我们开发了一种基于GPT-4生成的合成数据的微调流程。我们将展示如何使用WebLLM的浏览器内推理引擎来优化Llama3-8B和Phi3-3.8B的4位量化模型在三个不同Python编程数据集上的效果。我们承诺会公开全部实现、web应用和数据集，以促进在浏览器语言模型领域的进一步研究。|
|**2024-06-07**|**Bootstrapping Referring Multi-Object Tracking**|Yani Zhang et.al.|[2406.05039](http://arxiv.org/abs/2406.05039)|**[link](https://github.com/zyn213/temprmot)**|## 背景 当前的多对象引用跟踪（RMOT）任务通常依赖于手动标注的数据集和静态规则，这限制了多样性和实施范围。为了解决这个问题，我们的研究主要关注通过引入更多区分性语言词汇来推动RMOT任务的发展。为此，我们首先对Refer-KITTI数据集进行了扩展，创建了Refer-KITTI-V2，它从最初的2,719个手动标注开始，解决了类别不平衡问题，并增加了更多关键词，使其更贴近现实场景，相较于Refer-KITTI有所进步。我们进一步利用大型语言模型扩充这些标注，总计达到9,758个，生成了617个不同的词汇，超越了先前的RMOT基准。  此外，我们还改进了RMOT的端到端框架，采用了一个简单而优雅的时序推进策略，该策略在性能上优于先前的方法。相关源代码和数据集已可在<https://github.com/zyn213/TempRMOT>获取。|
|**2024-06-07**|**Scenarios and Approaches for Situated Natural Language Explanations**|Pengshuo Qiu et.al.|[2406.05035](http://arxiv.org/abs/2406.05035)|null|大型语言模型（LLMs）能够生成适应不同用户情境的自然语言解释（NLE）。然而，对于这种适应性的量化评估尚存空白。为此，我们创建了一个基准数据集——基于情境的解释（Situation-Based Explanation，SBE）数据集，包含100个需要解释的事物（explanandum）。每个事物都配对了针对教师、学生和专业人士等不同受众群体的解释，以便评估模型在满足这些多元化群体信息需求和背景下的解释精准度，如学生、教师和家长。每种“事例-受众”组合都附有人类撰写的参考解释，用于计算分数，以量化模型如何根据情境调整解释。我们在不同规模的预训练语言模型上测试了三种提示方法：规则基础提示、元提示和上下文学习提示。研究发现：1）模型可以通过生成提示产生更精确地符合目标情境的解释；2）明确提示“你是一个有用的助手”并非针对情境化NLE任务的必要技术；3）上下文学习提示仅能帮助模型学习演示模板，但无助于提升其推理性能。SBE数据集和我们的分析为今后生成适应情境的自然语言解释的研究提供了基础。|
|**2024-06-06**|**Verbalized Machine Learning: Revisiting Machine Learning with Language Models**|Tim Z. Xiao et.al.|[2406.04344](http://arxiv.org/abs/2406.04344)|null|受大型语言模型（LLMs）取得的巨大进展启发，我们提出了口头化机器学习（VML）框架。与传统的机器学习模型，通常在连续参数空间中优化不同，VML将参数空间限制为人可理解的自然语言。这种约束促使我们从新角度看待函数逼近问题，即将带有文本提示的LLM视为由文本提示参数化的函数。我们借此视角重新审视了经典机器学习任务，如回归和分类，发现这些问题可以通过LLM参数化的学习器和优化器来解决。VML的主要优势包括：（1）易于编码先验知识：关于问题和假设类的先验知识可以以自然语言形式编码并输入给LLM参数化的学习器；（2）自动模型选择：优化器可以根据数据和口头化先验知识自动选择具体的模型类别，并在训练过程中更新模型类别；（3）可解释的学习者更新：LLM参数化的优化器可以解释每次学习者更新的原因。我们进行了多项实验评估VML的有效性，希望它能成为增强机器学习可解释性和信任度的桥梁。|
|**2024-06-06**|**RoboMamba: Multimodal State Space Model for Efficient Robot Reasoning and Manipulation**|Jiaming Liu et.al.|[2406.04339](http://arxiv.org/abs/2406.04339)|null|在机器人操作的核心目标中，让模型理解视觉场景并执行动作是一个基本任务。尽管现有的机器人多模态大型语言模型（MLLM）能够处理一些基础任务，但它们在两个方面仍面临挑战：1）处理复杂任务的推理能力不足；2）对于MLLM的微调和推理存在高计算成本。近期提出的基于状态空间模型（SSM）的Mamba展示了在非平凡序列建模方面的潜力，具有线性推理复杂度。在此启发下，我们开发了RoboMamba，一个端到端的机器人MLLM，它利用Mamba模型结合机器人推理和动作能力，同时保持高效的微调和推理效率。  首先，我们将视觉编码器与Mamba集成，通过联合训练使视觉数据与语言嵌入对齐，赋予模型视觉常识和与机器人相关的推理能力。为了进一步提升RoboMamba的动作姿态预测能力，我们探索了一种高效的微调策略，仅使用简单的策略头。实验表明，一旦RoboMamba具备足够的推理能力，只需极少的微调参数（模型的0.1%）和时间（20分钟），就能习得操纵技能。在实验中，RoboMamba在通用和机器人评估基准上展现出卓越的推理能力。同时，我们的模型在模拟和真实世界实验中实现了姿态预测的出色表现，其推理速度比现有机器人MLLM快7倍。项目的网页链接为：<https://sites.google.com/view/robomamba-web>。|
|**2024-06-06**|**Coherent Zero-Shot Visual Instruction Generation**|Quynh Phung et.al.|[2406.04337](http://arxiv.org/abs/2406.04337)|null|尽管文本到图像合成技术取得了进步，特别是在扩散模型方面，但生成需要物体在连续步骤中保持一致表示和平滑状态转换的视觉指令仍然是一项艰巨挑战。本文提出了一种无需训练的框架，巧妙地结合了文本理解与图像生成，以确保视觉指令既美观又具有连贯性和准确性。通过测试多步骤指令，并与多个基线进行比较，我们验证了这种方法的有效性。实验结果显示，我们的方法能够生成连贯且视觉上吸引人的指令。|
|**2024-06-06**|**DeepStack: Deeply Stacking Visual Tokens is Surprisingly Simple and Effective for LMMs**|Lingchen Meng et.al.|[2406.04334](http://arxiv.org/abs/2406.04334)|null|大多数大型多模态模型（LMMs）通过将视觉令牌作为序列输入到大型语言模型（LLMs）的第一层来实现。这种方法虽然直观，但会显著增加计算和内存开销，因为模型需要处理更多的输入层令牌。本文提出了一种新的架构DeepStack，用于LMMs。在LMM的视觉和语言Transformer的N层中，我们将视觉令牌分为N组，并从底层逐层向上馈送到对应的Transformer层。令人惊讶的是，这种简单的方法极大地增强了LMM在跨层视觉令牌交互方面的建模能力，同时成本几乎不变。我们分别将DeepStack应用于LMM的语言和视觉Transformer，并通过广泛实证结果验证了DeepStack LMM的有效性。  使用相同的上下文长度，我们的DeepStack 7B和13B参数模型在9个基准测试上平均超越同类模型2.7分和2.9分。仅使用五分之一的上下文长度，DeepStack的表现接近于使用完整上下文长度的模型。这些提升在高分辨率任务中尤为明显，例如，与LLaVA-1.5-7B相比，TextVQA、DocVQA和InfoVQA上的性能分别提高了4.2分、11.0分和4.0分。此外，我们还将DeepStack应用到视觉Transformer层，这带来了与LLaVA-1.5-7B相当的平均改进，为3.8分。|
|**2024-06-06**|**PaCE: Parsimonious Concept Engineering for Large Language Models**|Jinqi Luo et.al.|[2406.04331](http://arxiv.org/abs/2406.04331)|**[link](https://github.com/peterljq/parsimonious-concept-engineering)**|**大型语言模型（LLMs）被广泛应用于各种任务，尽管它们能够生成类似人类的回复，但也会产生不良输出，如潜在有害信息、种族或性别歧视性言论以及错误的信息。为了减少这些问题，研究人员开发了对齐方法，如微调、提示工程和表示工程。然而，现有方法面临挑战：一些需要针对每个对齐任务进行昂贵的微调；一些未能充分消除不良概念，对齐效果不佳；一些则删除了良性的概念，降低了LLMs的语言能力。为此，我们提出了名为Parsimonious Concept Engineering（PaCE）的新型激活工程框架，旨在解决这些问题。  首先，我们构建了一个大规模的概念字典，它在激活空间中表示每个原子对应一个语义概念。接着，对于给定的任何对齐任务，我们会使用一个概念分区器高效地标记这些概念为良性或不良。在推理阶段，我们利用稀疏编码方法，根据概念字典分解LLM的激活，将其准确表示为良性成分和不良成分的线性组合。通过移除不良成分，我们能够调整LLMs的行为以符合对齐目标。  我们在回应净化、真实性增强和情感修订等任务上进行了实验，并发现PaCE在实现对齐性能的同时，保持了良好的语言能力，达到了当前最先进的水平。**|
|**2024-06-06**|**Step-aware Preference Optimization: Aligning Preference with Denoising Performance at Each Step**|Zhanhao Liang et.al.|[2406.04314](http://arxiv.org/abs/2406.04314)|null|## 背景  近期，Direct Preference Optimization (DPO) 已成功扩展到调整文本到图像的扩散模型，使其与人类偏好保持一致。不同于大多数现有 DPO 方法假设所有扩散步骤都与最终生成图像保持一致的偏好顺序，我们认为这种假设忽略了每个步骤特有的去噪性能，因此应该为每一步定制偏好标签。为此，我们提出了一种新颖的后训练方法——Step-aware Preference Optimization (SPO)，它独立评估并调整每个步骤的去噪性能，利用步级感知偏好模型和步级重采样器来确保准确的步级监督。  在SPO中，我们在每个去噪步骤中会创建一个图像池，寻找合适的胜者-败者对，并且关键在于，我们会从池中随机选择一个图像作为下一次去噪步骤的起点。这个步级重采样过程保证了每次胜者-败者对都来自同一原始图像，使得比较独立于前一步。为了评估每个步骤的偏好，我们训练了一个专门的步级感知偏好模型，适用于模糊和清晰的图像。在Stable Diffusion v1.5和SDXL等实验中，SPO 显著优于最新的Diffusion-DPO，尤其是在处理复杂、详细的提示时，能更好地生成图像并提升美学效果，同时在训练效率上超过20倍。代码和模型可在此链接获取：[https://rockeycoss.github.io/spo.github.io/](https://rockeycoss.github.io/spo.github.io/)。|
|**2024-06-06**|**Semantically Diverse Language Generation for Uncertainty Estimation in Language Models**|Lukas Aichberger et.al.|[2406.04306](http://arxiv.org/abs/2406.04306)|**[link](https://github.com/ml-jku/SDLG)**|**大型语言模型（LLMs）在生成文本时可能会出现幻觉，这阻碍了社会和工业中的各种应用，因为它们会降低LLMs的可信度。当前的LLMs采用自回归方式生成文本，即预测并添加文本标记。当LLMs对生成的下一个标记的语义含义不确定时，很可能会产生幻觉。因此，人们认为幻觉源于预测不确定性。我们提出了“语义多样性语言生成”（Semantically Diverse Language Generation，SDLG），用于量化LLMs的预测不确定性。SDLG引导LLM生成语义多样但又合理的初始文本替代方案，从而提供了精确的aleatoric语义不确定性测量，能够检测初始文本是否可能出现幻觉。  实验在问答任务上表明，SDLG始终优于现有方法，并且在计算效率上最为高效，为LLMs的不确定性估计设定了新的标准。**|
|**2024-06-06**|**Text-to-Drive: Diverse Driving Behavior Synthesis via Large Language Models**|Phat Nguyen et.al.|[2406.04300](http://arxiv.org/abs/2406.04300)|null|在模拟训练和评估关键安全系统，如自动驾驶车辆时，通过模拟生成各种场景至关重要。然而，模型其他车辆的轨迹以模拟复杂且有意义的近距离交互任务成本高昂。利用语言描述来生成驾驶行为是一种有前景的方法，它提供了一种可扩展且直观的人类操作方式，能够模拟广泛驾驶互动。但大型标注的语言-轨迹数据稀缺是这一方法面临的挑战。为此，我们提出了Text-to-Drive（T2D），这是一种利用大型语言模型（LLMs）合成多样化驾驶行为的技术。我们的方法采用知识驱动两阶段策略：首先，利用LLMs的内置知识生成丰富多样的驾驶行为语言描述；接着，利用其推理能力在模拟器中实现这些行为。T2D的核心是使用LLM构建状态图，将低级状态映射到高级抽象，从而简化了诸如总结低级观测、评估策略与行为描述的一致性以及设计辅助奖励等下游任务，无需人工监督。通过我们的知识驱动方法，我们证明T2D能生成比其他基准更丰富的轨迹，并提供一个自然语言界面，允许用户交互式地融入人类偏好。更多示例请访问我们的网站：<https://text-to-drive.github.io/>|
|**2024-06-07**|**What Languages are Easy to Language-Model? A Perspective from Learning Probabilistic Regular Languages**|Nadav Borenstein et.al.|[2406.04289](http://arxiv.org/abs/2406.04289)|null|## 背景  大型语言模型能够学习什么？根据定义，语言模型（LM）是字符串的分布。因此，可以将这个问题转化为评估字符串分布类的学习能力。尽管先前的研究主要关注理论限制，但我们关注的是实际可学习性。不同于以往的实证工作，我们评估神经语言模型在其“主场”——学习概率语言——上的表现，而不是作为形式语言的分类器。具体来说，我们研究递归语言模型（RLM）由循环神经网络（RNN）和Transformer LM学习的可行性。我们通过实验测试RLM的可学习性，考察其与RLM的复杂参数以及神经LM隐藏层大小的关系。实验结果显示，RLM的秩（对应于其条件分布对数似然线性空间的大小）和采样字符串的预期长度是RNN和Transformer LM可学习性的强且显著预测因素。其他一些预测指标也达到了显著性，但RNN和Transformer之间存在不同的模式。|
|**2024-06-06**|**Characterizing Similarities and Divergences in Conversational Tones in Humans and LLMs by Sampling with People**|Dun-Ming Huang et.al.|[2406.04278](http://arxiv.org/abs/2406.04278)|**[link](https://github.com/jacobyn/SamplingTonesACL)**|**## 翻译后的中文摘要  对话语气在人际交流中至关重要。随着大型语言模型（LLMs）的日益普及，研究它们与人类交流语气的差异变得尤为重要。然而，当前关于对话模式的研究往往依赖于预先存在的分类体系或文本语料库，这些可能存在实验者偏见，并可能无法充分反映研究领域中的真实世界分布。受认知科学方法的启发，我们提出一种迭代方法，通过交替进行两项任务来同时揭示语气和句子：（1）参与者判断给定句子的语气，（2）另一参与者根据该语气生成句子。我们在人类参与者和GPT-4之间进行了100轮这样的互动，从而获得了一组包含句子和常见对话语气的数据。我们还进行了额外实验，让人类和GPT-4对所有句子标注所有语气。基于1,339名人类参与者、33,370次人类评价以及29,900个GPT-4查询的数据，我们展示了如何使用这种方法创建一个可解释的几何表示，以展示人类和GPT-4之间的对话语气关系。这项工作展示了机器学习和认知科学理念如何结合，以解决人机交互中的挑战。**|
|**2024-06-05**|**Wings: Learning Multimodal LLMs without Text-only Forgetting**|Yi-Kai Zhang et.al.|[2406.03496](http://arxiv.org/abs/2406.03496)|null|## 任务  多模态大型语言模型（MLLMs）起源于预训练的通用语言模型，首先将图像与文本对齐，然后在混合模态输入上进行微调。然而，MLLM在处理仅包含文本的指令时会出现灾难性的遗忘，这些文本指令并未包含图像，这些问题在初始的语言模型阶段就已经存在。本文提出Wings，一个新型的MLLM，它在文本对话和多模态理解方面表现出色。通过分析MLLM在多模态指令中的注意力，我们发现文本遗忘与从图像前向图像后的注意力转移有关。因此，我们构建了额外模块作为增强学习器，以补偿这种注意力转移。视觉和文本学习器作为“翅膀”式的补充，平行连接在每个注意力块内，起初图像和文本输入由视觉学习器与主注意力协同工作，平衡对视觉元素的关注。随后，文本学习器通过注意力路由的方式与视觉学习器的输出协作整合。我们设计了低秩残差注意力（LoRRA）机制以保证学习器的高效运行。  实验结果表明，Wings在文本对话和视觉问答任务上优于同等规模的MLLM。在我们新构建的交错图像-文本（IIT）基准测试中，Wings在从文本为主到多模态为主的问答任务中展现出卓越性能。|
|**2024-06-06**|**Seq1F1B: Efficient Sequence-Level Pipeline Parallelism for Large Language Model Training**|Ao Sun et.al.|[2406.03488](http://arxiv.org/abs/2406.03488)|**[link](https://github.com/maydomine/seq1f1b)**|大型语言模型（LLMs）的兴起在很大程度上依赖于分布式训练策略，其中管道并行性起着关键作用。随着LLMs的训练序列长度扩展到32k甚至128k，当前的管道并行方法面临严重瓶颈，如高内存占用和显著的管道延迟，这极大地限制了模型的可扩展性和训练吞吐量。为了提高内存效率和训练效率，我们提出了一种针对长序列训练LLMs的高效序列级一次前向一次后向（1F1B）管道调度方法，称为Seq1F1B。Seq1F1B将批级别可调度单元分解为更细的序列级单元，从而减小延迟并降低内存需求。  考虑到如果均匀分割序列，Seq1F1B可能会产生轻微的额外延迟，我们设计了一种基于计算的策略来划分输入序列，以缓解这个副作用。与竞争性的管道基线方法，如Megatron的1F1B管道并行相比，我们的方法在保持更高训练吞吐量的同时，内存占用更低。值得注意的是，Seq1F1B能够在不使用重新计算策略的情况下，有效地在64个NVIDIA A100 GPU上训练一个具有300亿参数的LLM，处理长达64k的序列，这是现有方法无法实现的。我们的代码基于Megatron-LM，并已开源：https://github.com/MayDomine/Seq1F1B.git。|
|**2024-06-05**|**Analyzing LLM Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends**|Sanjana Ramprasad et.al.|[2406.03487](http://arxiv.org/abs/2406.03487)|null|### 翻译  近期的大型语言模型（LLMs）的进步显著提升了摘要生成系统的性能，但它们在真实性方面的问题引起了关注。尽管之前的研究广泛评估了新闻领域的LLMs，对话摘要的评价主要集中在基于BART的模型上，这在我们理解它们的可信度方面留下了空白。本研究旨在评估LLMs在对话摘要中的真实性，通过人类标注，并着重于识别和分类句级不一致。我们特别关注GPT-4和Alpaca-13B这两款主流模型。我们的评估揭示了错误定义的微妙之处：LLMs常常生成看似合理的推断，这些推断依赖于对话中的间接证据，而缺乏直接证据，这在旧模型中较少见。我们提出了一种改进的错误分类体系，引入了“情境推理”类别来归类这些LLM行为，并公开了相关数据集。利用我们的分类体系，我们比较了LLMs与老式微调模型之间的行为差异。此外，我们系统地评估了自动错误检测方法在LLM摘要上的效果，发现它们在识别这类细微错误时表现不佳。为此，我们提出了两种基于提示的精细错误检测方法，这两种方法优于现有指标，特别是在识别“情境推理”错误时。|
|**2024-06-05**|**BIPED: Pedagogically Informed Tutoring System for ESL Education**|Soonwoo Kwon et.al.|[2406.03486](http://arxiv.org/abs/2406.03486)|null|大型语言模型（LLMs）显示出巨大的潜力，能够作为经济且易于获取的英语第二语言（L2）学习者对话式智能辅导系统（CITS）。然而，现有的CITS往往只能教授简单概念，或者在教学深度上无法满足不同学习策略的需求。为了开发一个更具教育学导向、能教授复杂概念的CITS，我们构建了一个双语教育指导对话数据集（BIPED），包含一对一的人类英语辅导互动。通过对辅导对话的后处理分析，我们提炼出一套包含34种教师行为和9种学生行为的对话动作词典，并将其用于进一步标注收集的数据。根据先预测合适的教师行为再生成相应回复的两步框架，我们利用GPT-4和SOLAR-KO分别实现了两个CITS模型。实验结果表明，这些实施的模型不仅模仿了人类教师的风格，还运用了丰富且与上下文相适应的教学策略。|
|**2024-06-05**|**Does your data spark joy? Performance gains from domain upsampling at the end of training**|Cody Blakeney et.al.|[2406.03476](http://arxiv.org/abs/2406.03476)|null|随着大型语言模型（LLMs）的预训练数据集规模增长到万亿级别的tokens，这些数据集主要由大规模的CommonCrawl网络爬虫内容以及较小的领域特定数据组成。由于在大计算量（FLOPs）下训练以揭示模型在困难和新兴基准上的显著变化成本高昂，如何在通用网络抓取的多样性和领域特定信息密度之间找到最优平衡成为一个问题。本文展示了如何利用这些较小的领域特定数据，在训练后期对其进行上采样，从而在诸如MMLU、GSM8K和HumanEval等基准上提升性能。对于一个训练了1万亿（T）令牌的70亿参数模型，这种简单方法可使其性能提高6.90分、8.26分和6.17分，与训练时间两倍的Llama-2（7B）模型相当。我们研究了在训练后期领域上采样的持续时间，从5%到30%，发现10%到20%的比例最为合适，以平衡一般语言建模能力与特定任务的优化。此外，我们还利用领域上采样来大规模分析单个数据集对不同基准的增益，通过在这一阶段移除它们进行实验。这种方法极大地降低了实验成本，使得能够以预训练运行的十分之一左右的成本探索不同预训练数据集的影响。|
|**2024-06-05**|**AD-H: Autonomous Driving with Hierarchical Agents**|Zaibin Zhang et.al.|[2406.03474](http://arxiv.org/abs/2406.03474)|null|鉴于多模态大语言模型（MLLM）的强大功能，近期的研究聚焦于使用MLLM驱动的自动驾驶系统在大规模动态环境中。然而，常见的方法直接将高级指令转化为低级车辆控制信号，这违背了MLLM的本质生成模式，未能充分利用其潜在能力。因此，这些方法的一般化能力受到训练数据集的极大限制。为解决这个问题，我们提出通过中层语言驱动命令来连接高级指令和低级控制信号，它们比高级指令更细致，但比控制信号更通用且可解释，从而有效弥合两者之间的鸿沟。我们通过一个名为AD-H的分层多代理驾驶系统实现这一理念，包括一个用于高层推理的MLLM规划器和一个轻量级控制器进行低层执行。这种分层设计使MLLM摆脱了低级控制信号解码，充分释放了其在高层感知、推理和规划方面的涌现能力。  我们构建了一个带有动作层次注释的新数据集。全面的闭环评估显示，我们的AD-H系统具有多项关键优势。首先，AD-H在驾驶性能上显著优于现有方法，甚至展现出在车辆操作过程中自我纠正的能力，这是训练数据未涵盖的场景。其次，AD-H在长程指令和新环境条件下表现出色，明显超越当前最先进的方法。我们将公开我们的数据和代码，可通过<https://github.com/zhangzaibin/AD-H>获取。|
|**2024-06-05**|**What is the Best Way for ChatGPT to Translate Poetry?**|Shanshan Wang et.al.|[2406.03450](http://arxiv.org/abs/2406.03450)|null|本文研究了大型语言模型如ChatGPT在英语-中文诗歌翻译任务中的性能，通过定向提示和小样本场景分析以优化其表现。尽管初期结果令人鼓舞，但研究发现ChatGPT的翻译存在持续问题。为此，我们提出了“解释辅助诗歌机器翻译”（EAPMT）方法，该方法利用诗歌的单语解释作为翻译过程的指导。同时，我们改进了现有的评估标准，以更好地适应现代诗歌翻译的微妙之处。我们邀请专业诗人进行评估，并结合GPT-4的评价，结果显示，我们的EAPMT方法在与传统ChatGPT翻译方法以及现有在线系统的比较中表现出色。论文验证了我们方法的有效性，并为文学翻译的机器辅助提供了新颖视角。|
|**2024-06-05**|**Pre-trained Large Language Models Use Fourier Features to Compute Addition**|Tianyi Zhou et.al.|[2406.03445](http://arxiv.org/abs/2406.03445)|null|## 翻译  预训练的大型语言模型（LLMs）在数学推理方面表现出色，但它们如何执行基本的算术运算，如加法，仍不清楚。本文揭示了预训练的LLMs通过傅里叶特征进行加法——这些是隐藏状态中的维度，通过一组在频域中稀疏分布的特征来表示数字。在模型中，多层感知器（MLP）层和注意力层以互补的方式使用傅里叶特征：MLP层主要使用低频特征近似答案的大小，而注意力层主要通过高频特征执行模运算（例如判断答案是否为偶数）。预训练对于这种机制至关重要：从头开始训练的模型仅利用低频特征，导致准确性较低。将预训练的词嵌入引入到随机初始化的模型中可以恢复其性能。总的来说，我们的分析表明，适当的预训练表示（如傅里叶特征）能够解锁Transformer学习算法任务精确机制的能力。|
|**2024-06-05**|**Cycles of Thought: Measuring LLM Confidence through Stable Explanations**|Evan Becker et.al.|[2406.03441](http://arxiv.org/abs/2406.03441)|null|在许多高风险的机器学习应用中，模型需要能够表明其对预测的不确定性至关重要。尽管大型语言模型（LLMs）在各种基准上的准确度可达到甚至超过人类水平，但它们对错误响应的过度自信仍是已知的问题。传统的方法在直接应用于LLMs时可能面临计算成本和封闭源模型的挑战。近期提出了一些黑盒方法，但它们往往依赖于诸如自我表述的信心等启发式。我们提出了一种框架，通过分析模型生成答案的解释分布来衡量LLMs的不确定性。尽管利用解释本身并非新颖，但我们将其视为测试时间分类器，通过计算最可能的分类器后验答案分布，以此进行不确定性评估。  我们展示了使用解释蕴含作为分类器似然性的一种特定框架实例，如何在五个不同的数据集上改进了信心分数指标（特别是AUROC和AURC）。我们的结果表明，该框架既具有理论依据，又是有效量化LLMs不确定性的方式。|
|**2024-06-05**|**Interactive Text-to-Image Retrieval with Large Language Models: A Plug-and-Play Approach**|Saehyung Lee et.al.|[2406.03411](http://arxiv.org/abs/2406.03411)|**[link](https://github.com/saehyung-lee/plugir)**|**该论文主要关注的是交互式文本到图像检索任务中的对话形式上下文查询问题。我们的方法论，名为PlugIR，通过两种方式有效地利用大型语言模型（LLMs）的一般指令跟随能力。首先，通过重述对话形式的上下文，我们消除了在现有视觉对话数据上微调检索模型的需求，从而能够使用任意黑盒模型。其次，我们设计了一个LLM提问者，根据当前上下文中候选图像的信息，生成关于目标图像属性的非冗余问题。这种方法减少了生成问题的噪声和冗余。除了我们的方法，我们还提出了一种新的评估指标，称为最佳对数排名积分（BRI），以全面评估交互式检索系统。PlugIR在多个基准测试中表现出优于零次设置和 Fine-tuned 基准的性能。此外， PlugIR 的两个组成部分可以根据不同情况灵活单独或结合应用。我们的代码已开源在：https://github.com/Saehyung-Lee/PlugIR。**|
|**2024-06-04**|**Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks**|Tianyu He et.al.|[2406.02550](http://arxiv.org/abs/2406.02550)|**[link](https://github.com/ablghtianyi/ICL_Modular_Arithmetic)**|**这篇工作研究了大型语言模型在一组模块化算术任务中出现的上下文学习和技能组合现象。我们关注的是有限数量的一次性模运算函数 $z = a \times x + b \times y \;(\text{mod}\; p)$，这些函数由向量 $(a, b) \in \mathbb{Z}_p^2$ 标记。部分任务被用作预训练，其余用于分布外测试。实验表明，GPT风格的Transformer随着预训练任务数量增加，其在分布内和分布外的泛化能力会经历转变。最小型能实现分布外泛化的模型需要两个Transformer块；而对于更深的模型，分布外泛化阶段是“瞬态”的，需要早期停止。最后，我们对预训练模型进行了可解释性分析，揭示了两种阶段中高度结构化的表示，并讨论了学习到的算法。**|
|**2024-06-04**|**Leveraging Visual Tokens for Extended Text Contexts in Multi-Modal Learning**|Alex Jinpeng Wang et.al.|[2406.02547](http://arxiv.org/abs/2406.02547)|**[link](https://github.com/showlab/VisInContext)**|**这段研究并未介绍最先进的多模态大语言模型（MLLM），而是提出了一种创新方法，旨在有效提升长序列在多模态模型中的处理。我们提出了“Visualized In-Context Text Processing”（VisInContext）技术，通过视觉令牌来处理长文本，从而显著降低GPU内存使用和浮点运算（FLOPs）在训练和推理阶段的需求。例如，对于一个560亿参数的混合 Experts（MOE）模型，我们的方法将预训练中的上下文文本长度扩展到了2048个tokens，而计算量几乎保持不变。实验结果显示，使用VisInContext训练的模型在常见的基于实例的少量数据评估下游任务中表现出色。此外，VisInContext与现有技术相结合，能增强对文档的理解能力，特别适用于文档问答和连续文档检索，显示出巨大的潜力。**|
|**2024-06-04**|**To Believe or Not to Believe Your LLM**|Yasin Abbasi Yadkori et.al.|[2406.02543](http://arxiv.org/abs/2406.02543)|null|我们研究大型语言模型（LLMs）中的不确定性量化，目标是识别对给定查询的响应时的不确定性程度。我们同时考虑了两种类型的不确定性：一种是知识性不确定性（例如对事实或语言真理的未知），另一种是不可消除的随机性（如可能的答案多样性）。特别是，我们提出了一种信息论指标，能够可靠地区分出只有知识性不确定性较大的情况，这时模型的输出是不可靠的。这个条件仅依赖于通过特殊迭代提示基于先前响应得到的模型输出来计算。这种量化方法可以检测单答和多答情况下是否存在虚构（即知识性不确定性高）的情况，这与许多标准的不确定性量化策略（如以响应的对数似然性作为阈值）不同，后者无法识别多答情况下的虚构。  我们进行了一系列实验，展示了我们的方法的优势。此外，我们的研究还揭示了LLM如何通过迭代提示放大对给定输出的概率分配，这可能具有独立的兴趣价值。|
|**2024-06-04**|**Loki: Low-Rank Keys for Efficient Sparse Attention**|Prajwal Singhania et.al.|[2406.02542](http://arxiv.org/abs/2406.02542)|null|针对大型语言模型的推理计算成本高昂，特别是当使用长序列时，自注意力机制是主要开销。为了解决这个问题，近期的研究提出了一些稀疏注意力近似方法。本文中，我们通过分析发现，注意力块中的键向量实际上处于一个远低于原始维度的空间。这一观察促使我们提出Loki，一种新的稀疏注意力方法。Loki根据在低维空间计算的注意力得分，对KV缓存中的令牌进行排序和选择。实验结果表明，Loki能够比其他流行近似方法更好地保持模型的效能，同时由于减少了数据移动（加载/存储）和计算成本，加速了注意力计算。|
|**2024-06-04**|**Parrot: Multilingual Visual Instruction Tuning**|Hai-Long Sun et.al.|[2406.02539](http://arxiv.org/abs/2406.02539)|null|随着GPT-4V等多模态大型语言模型的快速发展，人工智能朝着通用人工智能迈出了重要一步。当前的方法主要依赖于监督微调（SFT）来同步视觉编码器与语言模型，从而赋予它们多模态能力。然而，这种做法可能导致随着训练的进行，语言模型处理多种语言的能力逐渐减弱。我们发现，以英语为中心的不平衡SFT数据集会导致非英语语言性能显著下降，原因在于SFT过程中未能有效连接视觉编码器和多语言令牌。为此，我们提出Parrot，一种利用文本引导在语言层面驱动视觉令牌对齐的新方法。Parrot通过让视觉令牌根据不同的语言输入进行条件化，并借助混合专家（MoE）促进多语言令牌的对齐。特别是，为了增强非英语视觉令牌的对齐，我们计算初始视觉特征与文本嵌入之间的跨注意力，然后将其输入到MoE路由器，选择最相关的专家。选定的专家会将初始视觉令牌转化为特定语言的视觉令牌。鉴于目前缺乏评估多语言能力的标准基准，我们还创建并公开了一个大规模多语言多模态基准（MMMB），包括6种语言、15个类别和12,000个问题。Parrot不仅在MMMB和MMM Benchmark上展现出最先进的性能，还在广泛的多模态任务中表现出色。我们将提供Parrot的源代码和训练数据集供公众使用。|
|**2024-06-04**|**Mitigate Position Bias in Large Language Models via Scaling a Single Dimension**|Yijiong Yu et.al.|[2406.02536](http://arxiv.org/abs/2406.02536)|null|这篇论文主要探讨了大型语言模型（LLMs）在实际应用中的一个现象——位置偏见，也称为"迷失在中间"。这种偏见在长文本情境中尤为明显，即关键信息在提示中的不同位置会显著影响模型的准确性。研究发现，注意力权重是位置偏见的微观表现。此外，论文指出，因果注意力掩码通过创建位置特定的隐藏状态，也对位置偏见有所贡献。  基于这些洞察，作者提出了一种方法来减轻位置偏见，即调整这些位置特定的隐藏状态。实验在多个任务上进行，包括自然问题多文档问答、键值检索、LongBench和时间线重排，涉及RoPE模型、扩展上下文窗口模型和Alibi模型等多种架构。结果显示，我们的方法通过仅修改隐藏状态的一个维度，就能实现性能提升，最高可达15.2%。研究者还提供了代码供进一步使用，代码地址为：https://aka.ms/PositionalHidden。|
|**2024-06-04**|**SpecExec: Massively Parallel Speculative Decoding for Interactive LLM Inference on Consumer Devices**|Ruslan Svirschevski et.al.|[2406.02532](http://arxiv.org/abs/2406.02532)|**[link](https://github.com/yandex-research/specexec)**|随着大型语言模型的广泛应用，高效运行它们变得至关重要。近期的研究通过推测性解码实现了显著的速度提升。然而，大多数工作都是针对数据中心硬件进行设计。本研究反问：我们能在消费级设备上多快地运行LLMs？消费者级GPU已无法容纳最大的模型（500亿参数以上），因此需要将参数卸载到RAM或SSD。当使用卸载参数的方式运行时，推理引擎可以同时处理数百乃至数千个令牌的批次，使其非常适合推测性解码。我们提出SpecExec（推测性执行），这是一种简单的并行解码方法，适用于主流LLM家族，能生成每轮目标模型迭代高达20个令牌的预测。它利用现代LLMs中概率分布的高波动性和模型输出概率之间的高度一致性。SpecExec通过从草稿模型获取最可能的令牌延续，构建一个目标模型的“缓存”树，然后在一个单次遍历中验证。  使用SpecExec，我们在消费级GPU上实现了500亿参数LLM的推理，配合RAM卸载，4位量化下的速度达到4-6个令牌/秒，而16位权重下的速度为2-3个令牌/秒。|
|**2024-06-04**|**Scalable MatMul-free Language Modeling**|Rui-Jie Zhu et.al.|[2406.02528](http://arxiv.org/abs/2406.02528)|**[link](https://github.com/ridgerchu/matmulfreellm)**|**## 翻译  在大型语言模型（LLMs）中，矩阵乘法（MatMul）通常占据主要计算开销。随着LLMs的规模扩大，其嵌入维度和上下文长度也随之增加，这一问题更为显著。本文提出了一种方法，能够在保持强大性能的同时，完全移除LLMs中的MatMul操作，即使是在27亿参数量级的模型上也能实现。实验表明，我们的无MatMul模型在与内存消耗显著更多的状态-of-the-artTransformer相当的条件下表现出色。我们研究了模型的扩展性规律，并发现无MatMul模型与全精度Transformer之间的性能差距随着模型尺寸增大而减小。  此外，我们提供了一个高效的GPU实现，相较于未优化的基线，训练时能减少高达61%的内存使用。在推理阶段，通过优化的内核，我们的模型内存消耗可降低超过10倍。为了准确评估架构效率，我们在FPGA上构建了定制硬件解决方案，利用GPU无法处理的轻量级运算，实现了对十亿参数规模模型的高速处理，使其接近人脑级别的效率。  这项工作不仅展示了LLMs在减小复杂性后仍能保持高效，还指出了未来加速器应优化的运算类型，以适应下一代轻量级LLMs的需求。我们的代码实现已开源至：\url{https://github.com/ridgerchu/matmulfreellm}。**|
|**2024-06-04**|**CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks**|Maciej Besta et.al.|[2406.02524](http://arxiv.org/abs/2406.02524)|**[link](https://github.com/spcl/checkembed)**|大型语言模型（LLMs）正在各个领域带来变革，但验证其答案仍然是一个重大挑战，尤其是在处理复杂、开放性的任务，如知识整合、摘要和提取。本文提出了一种名为CheckEmbed的精确、可扩展且简便的LLM验证方法。CheckEmbed的核心理念是：通过利用如GPT文本嵌入大模型获取的答案级嵌入来比较LLM的回答。这将复杂的文本答案转化为单一的嵌入，简化了对比过程，实现快速而有意义的验证。我们构建了一个全面的验证管道，该管道实现了CheckEmbed的理念，并提供了评估LLM答案真实性的度量，如嵌入热力图及其总结。我们展示了如何利用这些指标设计实际的引擎，以决定LLM答案是否令人满意。在实际文档分析任务中，如术语提取和文档摘要，我们的方法表现出显著的准确性提升、成本效益和运行时间性能，相较于BERTScore或SelfCheckGPT等基于token、句子和事实级别的方案。|
|**2024-06-04**|**RoboCasa: Large-Scale Simulation of Everyday Tasks for Generalist Robots**|Soroush Nasiriany et.al.|[2406.02523](http://arxiv.org/abs/2406.02523)|null|## 翻译  人工智能的最新进展在很大程度上依赖于规模的扩大。然而，在机器人领域，大规模机器人数据集的获取是一个瓶颈。我们主张利用逼真的物理模拟来提升环境、任务和数据集的规模，以支持机器人学习方法。为此，我们介绍RoboCasa，这是一个大型的仿真框架，旨在训练能够在日常环境中通用的机器人。RoboCasa的特点是拥有丰富且多样化的厨房场景，包括超过150个类别的一千多件3D模型资产和数十种可交互的家具和电器。  我们通过生成式AI工具进一步增强模拟的真实性和多样性，如使用文本到3D模型的技术生成对象资产，以及通过文本到图像模型生成环境纹理。我们设计了100项任务，包括由大型语言模型指导的复合任务，用于系统性评估。为了促进学习，我们提供了高质量的人类演示，并结合自动轨迹生成方法，以最小的人力成本大幅扩充数据集。  我们的实验表明，在使用合成生成的机器人数据进行大规模模仿学习时，存在明显的规模效应，并显示出利用模拟数据在现实世界任务中的巨大潜力。相关视频和开源代码已在https://robocasa.ai/网站上提供。|
|**2024-05-31**|**Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis**|Chaoyou Fu et.al.|[2405.21075](http://arxiv.org/abs/2405.21075)|null|在人工智能的追求中，多模态大型语言模型（MLLMs）已成为近期进步的核心。然而，对它们处理序列视觉数据的能力的关注尚显不足。为此，我们在本文中提出Video-MME，这是首个全面评估MLLMs在视频分析性能的多模态评估基准。我们的工作有四个关键特性：1）视频类型多样，涵盖6个主要视觉领域和30个子领域，确保广泛的应用场景泛化能力；2）时间维度的跨度，包括短、中、长期视频，从11秒到1小时，以检验模型对复杂情境动态的适应性；3）数据模态的广度，结合视频帧以外的多种输入，如字幕和音频，揭示MLLMs的全方位能力；4）高质量的标注，由专家严格手动标记，以保证精确且可靠的模型评估。我们精心挑选并手动注解了900段视频，总时长达到256小时，生成了2,700个问题-答案对。通过Video-MME，我们对包括GPT-4系列、Gemini 1.5 Pro在内的多个最先进的MLLM，以及开源图像模型InternVL-Chat-V1.5和视频模型LLaVA-NeXT-Video进行了深入评估。实验结果显示，Gemini 1.5 Pro是表现最佳的商业模型，明显优于开源模型。我们的数据集和发现强调了改进处理更长序列和多模态数据的必要性。项目网页链接：https://video-mme.github.io|
|**2024-05-31**|**Grammar-Aligned Decoding**|Kanghee Park et.al.|[2405.21047](http://arxiv.org/abs/2405.21047)|null|大型语言模型（LLMs）在生成高度结构化的输出时面临挑战，如程序代码、数学公式或规范的标记。约束解码方法通过限制每次输出可能的令牌，确保输出符合特定规则来缓解这个问题，例如在语法约束解码（GCD）中，LLM的输出必须遵循给定的语法规则。然而，研究表明，这种约束解码可能会扭曲模型的分布，导致生成的输出虽然语法正确，但其概率并不直接反映LLM本身的概率分配，从而质量不高。我们称之为“与语法约束对齐的解码”（Grammar-Aligned Decoding，GAD），并提出了一种名为“自适应采样与近似期望未来”（Adaptive Sampling with Approximate Expected Futures，ASAp）的解码算法。  ASAp算法旨在保证输出的语法性，并理论上产生与LLM在给定语法约束条件下的条件概率相符的结果。该算法利用先前的样本输出来稳健地估算不同输出前缀的未来语法可能性。我们在代码生成和结构化自然语言处理任务上的实验表明，ASAp经常能够生成比现有GCD技术更符合LLM分布且仍遵守所需语法限制的输出，从而提高了整体质量。|
|**2024-05-31**|**Direct Alignment of Language Models via Quality-Aware Self-Refinement**|Runsheng Yu et.al.|[2405.21040](http://arxiv.org/abs/2405.21040)|null|强化学习从人类反馈（RLHF）是调整大型语言模型（LLMs）行为以符合人类偏好的常用方法。最近，直接策略优化（DPO）作为一种替代方案兴起，它不再依赖LLM奖励模型，从而减少了额外的内存和训练时间。然而，DPO忽视了正向和负向响应的相对质量，可能导致训练结果不理想。为解决这个问题，我们探讨利用LLM内部知识在即时微调过程中获取响应的质量，并优化损失函数。我们设计了一种细化函数，利用LLM的知识来估计正向和负向响应的品质。实验表明，在轻度假设下，构建的细化函数能够帮助自我调整损失函数。我们将这个细化功能整合到DPO及其变体身份策略优化（IPO）中。实验证明，这些改进后的模型在各种评估者上表现出优于DPO和IPO的性能。|
|**2024-05-31**|**Standards for Belief Representations in LLMs**|Daniel A. Herrmann et.al.|[2405.21030](http://arxiv.org/abs/2405.21030)|null|随着大型语言模型（LLMs）在各个领域展现出非凡能力，计算机科学家们正在寻求理解它们的认知过程，特别是关于LLMs如何（如果有的话）内部构建对世界的信念。然而，目前尚缺乏一个统一的理论框架来支撑对LLM中信念的研究。本文试图填补这一空白，提出了一套条件，使LLM中的表示能够被视为信念似的。我们指出，尽管在LLMs中测量信念的项目与决策理论和形式认识论中的信念测量在许多方面有相似之处，但也存在差异，这些差异应影响我们的测量方法。因此，借鉴哲学洞察和机器学习的当代实践，我们确立了四个标准：准确性、一致性、统一性和实用性。这四个标准结合了理论考量与实际限制，为全面理解LLM中的信念表示奠定了基础。我们引用实证工作的成果，揭示了单独使用某些标准时识别信念表示的局限性。|
|**2024-05-31**|**LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models**|Elias Stengel-Eskin et.al.|[2405.21028](http://arxiv.org/abs/2405.21028)|**[link](https://github.com/esteng/pragmatic_calibration)**|**当回答问题时，语言模型不仅能提供答案，还能传达对答案正确性的信心程度。这包括明确的分数标记，如给出数字，以及隐含的信心标志，如权威语气或提供额外知识。然而，当前大多数模型往往过于自信。为了校准这些信心度，我们提出了一种实用的、考虑听众的微调方法（LACIE），它不仅关注答案是否正确，还关注答案是否会被听众接受。我们将校准视为偏好优化，通过双代理游戏创建数据，让一个演讲者模型的输出接受模拟听者的评判。然后，我们使用LACIE对三个语言模型（Mistral-7B、Llama3-8B和Llama3-70B）进行微调，并显示经过微调的模型在模拟听者面前有更好的校准。重要的是，这些趋势也适用于人类听众，帮助他们更准确地预测模型的正确性：我们在人机评估中发现，经过LACIE训练的模型接受的错误答案减少了47%，而正确答案的接受率保持不变。此外，LACIE泛化到另一个数据集上，在使用TriviaQA训练后，TruthfulQA上的真实性大幅提高。我们的分析表明，LACIE导致了正确和错误示例之间的信心度更好地分离。定性上，我们发现经过LACIE训练的模型会更加谨慎，并在回答正确时通过使用权威语气或提供细节来隐性地表示确定性。最后，LACIE微调导致模型对于可能错误的答案更倾向于放弃（例如说“我不知道”）。**|
|**2024-05-31**|**Improved Techniques for Optimization-Based Jailbreaking on Large Language Models**|Xiaojun Jia et.al.|[2405.21018](http://arxiv.org/abs/2405.21018)|**[link](https://github.com/jiaxiaojunqaq/i-gcg)**|**随着大型语言模型（LLMs）的快速发展，其安全校准成为广泛应用的关键。针对这些模型的破解（即“jailbreaking”）活动日益增多，其中贪婪坐标梯度（GCG）攻击因其成效显著而受到关注。然而，GCG的攻击效率仍有提升空间。本文提出了一系列改进的优化基线破解技术，以提升GCG的性能。首先，我们注意到单个目标模板“Sure”极大地限制了GCG的攻击效果，因此我们建议采用包含有害自我暗示和/或指导的多样化目标模板，以误导模型。在优化策略上，我们建议在GCG中实施自动多坐标更新，以加速收敛，并引入从简单到复杂（easy-to-hard）的初始化技巧。将这些改进整合，我们开发出一种高效的方法—— $\mathcal{I}$ -GCG。实验在一系列基准测试，如NeurIPS 2023 红队挑战中进行，结果显示，我们的改进技术能够帮助GCG超越现有破解攻击，实现接近100%的攻击成功率。代码已发布在https://github.com/jiaxiaojunQAQ/I-GCG。**|
|**2024-05-31**|**DeCo: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models**|Linli Yao et.al.|[2405.20985](http://arxiv.org/abs/2405.20985)|null|该研究关注于多模态语言模型（MLLMs）中的投影器模块，因为它们在连接视觉和语言模态、促进跨模态对齐方面发挥关键作用。然而，目前对于投影器在视觉-语言对齐方面的效果评估仍显不足，通常只能通过下游任务的性能间接推断。为此，本研究通过分析MLLM中的视觉-语言语义流，来解读投影器的工作机制。  具体来说，研究者追踪从生成的语言标记到原始视觉编码块以及投影器产生的中间输出之间的语义相关性流。发现压缩型投影器（如QFormer）倾向于将视觉块抽象成有限的几个概念，如物体或属性，导致“双重抽象”现象：首先，投影器参照预定义查询令牌进行视觉语义抽象，然后，基于文本指令的大语言模型进一步提取。这种双重抽象在训练过程中效率不高，并可能导致视觉语义信息的累积缺失。  为解决这个问题，研究提出“解耦压缩与抽象（DeCo）”的关键洞察，即在投影层面上将视觉令牌数量压缩，而让大语言模型完全负责视觉语义抽象。因此，研究人员采用了一种简单的压缩器——二维自适应池化，以无参数的方式降低视觉块的尺寸。实验结果显示，DeCo在性能和效率上都优于传统的压缩投影器。它在MLLM基准、视觉定位和开放性视觉问答任务中分别取得了0.9%、7.1%和2.9%的性能提升，同时拥有更少的可训练参数和更快的收敛速度。|
|**2024-05-31**|**Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training**|Feiteng Fang et.al.|[2405.20978](http://arxiv.org/abs/2405.20978)|null|大型语言模型（LLMs）展现出强大功能，但面临挑战，如虚构、过时知识和难以追溯的推理过程。为解决这些问题，检索增强生成（RAG）作为一种有前景的方法崭露头角，它结合外部数据库的知识。然而，不适当的检索段落可能妨碍LLMs生成全面且高质量的回答。先前关于RAG中检索噪声稳健性的研究往往局限于有限的噪声类型，这与现实世界的检索环境不符，限制了实际应用。本研究首先探讨了检索噪声，并将其分为三种不同的类别，反映真实环境。我们分析了这些不同类型的检索噪声对LLMs稳健性的影响。  接着，我们提出了一种新颖的RAG方法，称为检索增强自适应对抗训练（RAAT）。RAAT利用自适应对抗训练来动态调整模型的训练流程以应对检索噪声，并采用多任务学习确保模型能够识别嘈杂的上下文。大量的实验表明，在各种噪声条件下，使用RAAT训练的LLaMA-2 7B模型在F1和EM分数上显示出显著提升。为了便于复现，我们已在https://github.com/calubkk/RAAT上发布了我们的代码和数据。|
|**2024-05-31**|**SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales**|Tianyang Xu et.al.|[2405.20974](http://arxiv.org/abs/2405.20974)|**[link](https://github.com/xu1868/sayself)**|**大型语言模型（LLMs）常常产生不准确或虚假的信息，并且通常无法表明其信心水平，这限制了它们的广泛应用。先前的研究试图通过直接提示或自我一致性提示来提取LLMs的信心，或者构建特定数据集进行监督微调。基于提示的方法性能较差，而基于训练的方法又局限于二元或不精确的整体信心估计。本文提出了一种先进的方法——SaySelf，这是一个训练框架，旨在教导LLMs提供更精确的细粒度信心估计。  此外，SaySelf还推动LLMs生成自我反思的解释，明确指出它们在参数知识上的空白并解释不确定性。这是通过让LLM以自然语言的形式自动总结特定知识中的不确定性来实现的。这种总结是基于对多个采样推理链的不一致性分析，生成的数据用于监督微调。为了进一步校准信心估计，我们采用了精心设计的强化学习，奖励准确、高置信度的预测，同时惩罚错误输出中的过度自信。  实验结果表明，无论是在分布内还是分布外的数据集上，SaySelf都能有效减少信心校准误差，同时保持任务性能。生成的自我反思理由也被证明是合理的，能进一步促进校准。代码已公开在：\url{https://github.com/xu1868/SaySelf}。**|
|**2024-05-31**|**LCQ: Low-Rank Codebook based Quantization for Large Language Models**|Wen-Pu Cai et.al.|[2405.20973](http://arxiv.org/abs/2405.20973)|null|## 背景  大型语言模型（LLMs）在众多任务上展现出优异性能，但它们的存储和计算成本高成为部署的一大挑战。为了压缩模型并降低成本，权重量化技术被广泛应用。目前，大多数针对LLMs的量化方法使用秩一码本，然而在高压缩比下，这会导致显著的精度损失。本文提出了一种新颖的权重量化方法，称为低秩码本量化（LCQ），旨在解决这一问题。  ## 方法  LCQ采用低秩码本进行量化，其秩可以大于一。这种方法旨在通过利用更高的秩来保持或提升模型的精度，同时控制额外的存储开销几乎为零。实验表明，与现有方法相比，LCQ在保持良好准确性的前提下，能够实现更优的压缩效果。  ## 结论  综上所述，本文介绍了一种创新的低秩码本量化方法，它有望在不显著增加存储成本的情况下，提升大型语言模型在实际应用中的性能和效率，为高效部署这些模型提供了新的解决方案。|
|**2024-05-30**|**MotionLLM: Understanding Human Behaviors from Human Motions and Videos**|Ling-Hao Chen et.al.|[2405.20340](http://arxiv.org/abs/2405.20340)|null|这项研究关注于多模态（视频和动作模态）下的人类行为理解，通过大型语言模型（LLMs）的强大功能。与专为单模态（视频或动作）设计的最新LLMs不同，我们认为理解人类行为需要对视频和动作序列（如SMPL序列）进行联合建模，以有效捕捉精细的身体部位动态和语义。为此，我们提出MotionLLM，这是一个简洁而有效的框架，用于人类动作理解、描述和推理。MotionLLM采用了一体化的视频-动作训练策略，利用现有粗粒度的视频-文本数据和精细动作-文本数据的优势，以获取丰富的空间-时间洞察。此外，我们还创建了一个大规模的MoVid数据集，包含了多样化的视频、动作、caption和指令。我们还提出了MoVid-Bench，它具有精心的手动标注，以更好地评估在视频和动作上的人类行为理解能力。实验结果充分展示了MotionLLM在caption生成、空间-时间理解以及推理能力方面的优越性。|
|**2024-05-30**|**Visual Perception by Large Language Model's Weights**|Feipeng Ma et.al.|[2405.20339](http://arxiv.org/abs/2405.20339)|null|这篇论文的背景是现有的多模态大型语言模型（MLLMs）采用了一种方法，即将视觉信息与语言模型的输入空间对齐，然后将视觉令牌与文本令牌合并，形成统一的序列输入给语言模型。然而，这种方法由于增加了由视觉令牌导致的输入序列长度，计算成本较高。为此，论文提出了一种新颖的参数空间对齐范式，通过将视觉信息表示为模型权重来处理。对于每个输入图像，首先使用视觉编码器提取特征，然后将这些特征转换为感知权重，并将其与语言模型的权重融合。这样，语言模型的输入无需视觉令牌，从而缩短了输入序列，显著提高了效率。  基于这一理念，论文提出了VLoRA模型，其中包含一个感知权重生成器。该生成器设计成能够将视觉特征转化为具有低秩特性的感知权重，类似于LoRA（低秩自适应训练）。实验结果表明，尽管VLoRA在多种多模态任务的基准上表现出与现有MLLMs相当的性能，但其在训练和推理阶段的计算成本显著降低。论文承诺开源代码和模型。|
|**2024-05-30**|**Xwin-LM: Strong and Scalable Alignment Practice for LLMs**|Bolin Ni et.al.|[2405.20335](http://arxiv.org/abs/2405.20335)|**[link](https://github.com/xwin-lm/xwin-lm)**|**本文介绍Xwin-LM，一个专为大型语言模型（LLMs）设计的全面对齐方法套件。它涵盖了监督微调（SFT）、奖励建模（RM）、拒绝采样微调（RS）和直接偏好优化（DPO）等多种关键技术。主要组成部分包括：(1) 使用高质量指令数据进行初始微调的Xwin-LM-SFT；(2) 由GPT-4精心标注的大型多轮偏好数据集Xwin-Pair；(3) 在7B、13B和70B参数规模上训练的Xwin-RM奖励模型；(4) 每个提示关联64个独特响应的多wise偏好数据集Xwin-Set，这些响应由Xwin-LM-SFT生成并由Xwin-RM评分；(5) 使用Xwin-Set中最高得分响应进行微调的Xwin-LM-RS模型；(6) 通过DPO算法在Xwin-Set上进一步优化的Xwin-LM-DPO模型。我们在AlpacaEval和MT-bench上的评估显示了整个管道的稳定且显著改进，证明了Xwin-LM的强大和可扩展性。我们将在https://github.com/Xwin-LM/Xwin-LM的仓库中持续更新，以促进社区研究。**|
|**2024-05-31**|**ParSEL: Parameterized Shape Editing with Language**|Aditya Ganeshan et.al.|[2405.20319](http://arxiv.org/abs/2405.20319)|null|本文提出了一种名为ParSEL的系统，它旨在通过自然语言实现高质量3D资产的可控编辑。面对自然语言在精确操控上的局限性，ParSEL接收一个分割的3D网格和编辑请求，生成一个参数化的编辑程序。用户可以调整程序参数，精细地探索形状变化，控制编辑幅度。系统利用大型语言模型（LLMs）来理解初始编辑指令，但发现它们在推断完整编辑程序时常常不足，产生的结果可能违反形状逻辑。为此，我们设计了分析性编辑传播（Analytical Edit Propagation，AEP）算法，它从初始编辑种子开始，通过计算机代数系统进行几何分析，寻找与潜在用户编辑兼容的分析性编辑操作，以生成完整的编辑程序。实验表明，相较于其他方案，ParSEL通过自然语言请求有效地实现了对3D对象的可控编辑。|
|**2024-05-30**|**CausalQuest: Collecting Natural Causal Questions for AI Agents**|Roberto Ceraolo et.al.|[2405.20318](http://arxiv.org/abs/2405.20318)|**[link](https://github.com/roberto-ceraolo/causal-quest)**|**人类天生就有寻求因果关系的驱动力，无论是出于好奇心还是特定目标。为了开发能处理这种人类本性追求的AI代理，我们急需一个全面的自然因果问题数据集。然而，现有的数据集要么包含人工制造的问题，无法反映实际AI应用场景，要么在特定来源的问题覆盖上有限。为此，我们提出了CausalQuest，这是一个源自社交网络、搜索引擎和AI助手的13,500个自然出现的问题的数据集。我们定义了因果问题，并建立了更细致的分类体系。通过人类标注员和大型语言模型的协作，我们对数据集进行了精心标注。研究发现，42%的人类提问实际上是关于因果的，大部分是想了解给定结果背后的原因。利用这个数据集，我们训练了高效的二分类器（高达28.5亿参数），用于识别因果问题，实现了高性能，F1分数高达0.877。最后，我们提出了一系列丰富的未来研究方向，这些都可以基于我们的数据和模型进行扩展。**|
|**2024-05-30**|**ANAH: Analytical Annotation of Hallucinations in Large Language Models**|Ziwei Ji et.al.|[2405.20315](http://arxiv.org/abs/2405.20315)|**[link](https://github.com/open-compass/anah)**|**### 背景  大型语言模型（LLMs）的“幻觉”问题对于其广泛应用至关重要。然而，对这一问题的细致测量在社区中并未得到充分探索。为此，我们提出了一项名为 $\textbf{ANAH}$ 的双语数据集，专注于生成式问答中的LLM幻觉分析。ANAH中的每个答案句子都经过严谨标注，包括参考片段检索、幻觉类型的判断以及错误内容的修正。该数据集包含约12,000个句级注释，涵盖了大约4,300个LLM响应，涉及超过700个主题，通过人机交互式流程构建而成。由于幻觉注释的精细粒度，我们可以定量确认LLMs的幻觉问题随着答案的扩展而逐渐增加，并利用ANAH来训练和评估幻觉标注器。  ### 任务  我们构建了大约12,000条句子级别的注释，针对约4,300个LLM生成的回答，涵盖了超过700个主题。这个名为ANAH的数据集通过人类参与的流程精心设计，旨在提供关于生成式问答中LLMs幻觉的详尽分析。通过细致的幻觉标注，我们能够量化地验证LLMs在生成答案时幻觉问题的累积，并利用ANAH来训练和评估幻觉识别能力。我们的实验深入研究了生成式和区分性标注器，并发现尽管开源LLMs在精细幻觉标注方面面临挑战，但使用ANAH训练的生成式标注器能够超越所有开源模型，甚至接近GPT-3.5的表现，并展现出在未见过问题上的良好泛化能力。**|
|**2024-05-30**|**Sequence-Augmented SE(3)-Flow Matching For Conditional Protein Backbone Generation**|Guillaume Huguet et.al.|[2405.20313](http://arxiv.org/abs/2405.20313)|null|蛋白质在几乎所有的生物过程中发挥关键作用，其多样化的功能源于复杂的三维结构，而这些结构又由氨基酸序列决定。在这篇论文中，我们利用氨基酸序列丰富的生物学归纳偏置，提出了一种新的序列条件的SE(3)等变流匹配模型——FoldFlow-2，用于蛋白质结构生成。与FoldFlow家族的先前模型相比，FoldFlow-2引入了新颖的架构特性，包括用于编码序列的蛋白质大语言模型、结合结构和序列表示的新多模态融合主干，以及基于几何变换器的解码器。为了增加生成样本的多样性和新颖性——这对新药设计至关重要——我们在比先前工作使用的PDB数据集大一个数量级的新数据集上大规模训练FoldFlow-2，该数据集包含了已知的PDB蛋白质和通过过滤获得的高质量合成结构。此外，我们展示了如何通过引入强化微调（Reinforced Finetuning，简称ReFT）目标，使FoldFlow-2能够适应任意奖励，如提高二级结构多样性。  实验结果表明，FoldFlow-2超越了现有基于蛋白质结构的生成模型的状态，无论在无条件生成还是在设计性、多样性和新颖性方面，都优于RFDiffusion，且在蛋白质长度的各类任务上表现出良好的泛化能力，特别是在等温构象采样任务上。最后，我们展示了一个经过微调的FoldFlow-2在诸如VHH纳米抗体骨架设计等具有挑战性的条件设计任务上取得了进展。|
|**2024-05-30**|**Large Language Models Can Self-Improve At Web Agent Tasks**|Ajay Patel et.al.|[2405.20309](http://arxiv.org/abs/2405.20309)|**[link](https://github.com/AjayP13/webdreamer)**|在复杂的环境中，如网络浏览器，训练模型作为能够有效导航和执行动作的代理通常具有挑战性，主要受限于缺乏训练数据。近年来，大型语言模型（LLMs）显示出通过自然语言提示以零样本或少量样本来在新环境中导航的能力。研究还表明，LLMs可以通过自我改进（即在其自身生成的数据上微调）来超越基础性能。本研究旨在探究LLMs在长时序任务的复杂环境——WebArena基准中，通过自我改进能否提升其表现。WebArena要求代理自主浏览网页并执行操作以达成特定目标。我们使用三种不同的合成训练数据混合进行微调，并发现经过自我改进后，模型在WebArena基准上的任务完成率提高了31%。此外，我们还提出了新的评估指标，用于更全面地评估我们的微调代理模型的行为性能、鲁棒性、能力以及轨迹质量，这些指标超越了当前仅依赖于整体基准分数的评估方式。|
|**2024-05-30**|**Group Robust Preference Optimization in Reward-free RLHF**|Shyam Sundhar Ramesh et.al.|[2405.20304](http://arxiv.org/abs/2405.20304)|**[link](https://github.com/rsshyam/Group-robust-preference-optimization)**|**## 翻译  针对大型语言模型（LLMs）的特定任务进行适应时，通常需要通过基于人类反馈的强化学习（RLHF）和多元标签者群体（如不同性别、种族、公司团队等）的偏好数据进行微调。然而，传统方法倾向于采用“一刀切”的策略，即假设并优化单一的偏好模型，对各群体的独特特性和需求不够敏感。为此，我们提出了一种新颖的群体鲁棒偏好优化（GRPO）方法，旨在稳健地使LLMs适应各个群体的偏好。GRPO方法基于无奖励直接偏好优化，但区别于以往，它目标是寻找一个能最大化最差群体性能的鲁棒策略。为了实现这一目标，GRPO会动态且逐次调整不同群体的权重，优先关注累积损失较高的群体。我们在理论上探讨了GRPO的可行性，并分析了其在对数线性策略类别下的收敛性。通过使用来自不同群体的全局意见数据对LLMs进行GRPO微调，我们显著提高了最差群体的表现，减少了群体间损失的不平衡，同时提高了概率准确性，相较于非鲁棒基线，这些改进效果显著。**|
|**2024-05-30**|**Who Writes the Review, Human or AI?**|Panagiotis C. Theocharopoulos et.al.|[2405.20285](http://arxiv.org/abs/2405.20285)|null|随着人工智能在自然语言处理中的广泛应用，人们关注如何识别不同领域的AI生成文本。本研究旨在探讨这个问题，通过提出一种方法来准确区分人工智能生成的和人类撰写的书评。我们的方法利用迁移学习，让模型能够在不同主题间识别生成文本，同时提高其识别写作风格和词汇变化的能力。我们构建了一个数据集，包含真实的书评和使用Vicuna开源语言模型生成的模拟评论，以评估所提方法的有效性。实验结果显示，识别文本原创来源是可行的，准确率达到96.86%。我们的工作聚焦于大型语言模型在文本识别方面的性能与局限性研究，这对于未来有效管理此类模型以及确保人类创作内容的完整性和真实性具有重要意义。|
|**2024-05-29**|**X-VILA: Cross-Modality Alignment for Large Language Model**|Hanrong Ye et.al.|[2405.19335](http://arxiv.org/abs/2405.19335)|null|我们提出X-VILA，一种旨在增强大型语言模型（LLMs）功能的多模态模型，它融合了图像、视频和音频模态。通过将各模态特定的编码器与LLM输入对齐，并将扩散解码器与LLM输出对齐，X-VILA实现了跨模态理解、推理和生成。为了支持这种跨模态对齐，我们开发了一个有效的任意模态指令跟随数据集。然而，我们发现当前的跨模态对齐方法存在一个关键问题，导致视觉信息丢失。为此，我们设计了视觉对齐机制，包括一个视觉嵌入高速公路模块，以解决这一问题。此外，我们还提供了一种资源高效的训练策略，使得X-VILA在任意模态对话任务上表现出色，大幅超越先前的方法。令人惊讶的是，即使在缺乏类似训练数据的情况下，X-VILA在不同模态间也展现出涌现特性。该项目将开源。|
|**2024-05-29**|**LLMs Meet Multimodal Generation and Editing: A Survey**|Yingqing He et.al.|[2405.19334](http://arxiv.org/abs/2405.19334)|**[link](https://github.com/yingqinghe/awesome-llms-meet-multimodal-generation)**|**随着大型语言模型（LLMs）的最新进展，人们越来越关注将它们与多模态学习相结合。当前的多模态大语言模型（MLLMs）调查主要集中在理解上。这篇综述详细探讨了跨图像、视频、3D和音频等领域的多模态生成，特别强调了这些领域中的里程碑式工作及其技术进步。我们深入研究了这些方法的关键技术组件，以及在相关研究中使用的多模态数据集。此外，我们还剖析了借助现有生成模型进行人类-计算机交互的工具增强型多模态代理。最后，我们全面讨论了人工智能安全的进步，并探索了新兴应用和未来前景。我们的工作提供了一个系统而深入的多模态生成概述，有望推动生成内容的人工智能（AIGC）和世界模型的发展。所有相关的论文列表可在<https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation>找到。**|
|**2024-05-29**|**Multi-Modal Generative Embedding Model**|Feipeng Ma et.al.|[2405.19333](http://arxiv.org/abs/2405.19333)|null|在大多数多模态任务中，问题可以归结为生成或嵌入。现有的模型通常通过将语言模块分解为一个用于生成的文本解码器和一个用于嵌入的文本编码器来处理这两种问题。为了探索多模态方法的简约性，本工作试图仅使用一个模型来处理每种模态。为此，我们提出了一种多模态生成嵌入模型（MM-GEM），它将生成和嵌入目标整合到一个大型语言模型中。同时，我们设计了PoolAggregator，以提高效率并实现细粒度的嵌入和生成能力。  令人惊讶的是，这两个目标之间并没有显著冲突。例如，基于ViT-Large和TinyLlama的MM-GEM在诸如跨模态检索和零样本分类等多模态嵌入模型基准上表现出良好的性能，同时具备良好的图像描述能力。此外，MM-GEM能够无缝执行区域级别的图像描述生成和检索任务。另外，MM-GEM中的先进文本模型对于长文本和图像检索的Recall@1指标带来了超过5%的提升。|
|**2024-05-29**|**Self-Exploring Language Models: Active Preference Elicitation for Online Alignment**|Shenao Zhang et.al.|[2405.19332](http://arxiv.org/abs/2405.19332)|**[link](https://github.com/shenao-zhang/selm)**|****摘要：**  偏好优化，特别是在人类反馈强化学习（RLHF）的驱动下，已经在使大型语言模型（LLMs）遵循人类意愿方面取得了显著成就。相较于使用固定数据集的离线对齐，通过人或人工智能对模型生成的反馈通常能够通过迭代过程提升奖励模型的能力和LLMs的一致性。然而，要实现全局准确的奖励模型，需要系统地探索生成各种各样的响应，以涵盖自然语言的广阔空间。仅依赖标准奖励最大化LLMs的随机采样是不足以满足这一需求的。  为解决这个问题，我们提出了一种双层目标，乐观地倾向于可能具有高奖励的响应，以此来主动探索分布外区域。通过解决内层问题，利用重新参数化的奖励函数，我们提出了名为Self-Exploring Language Models（SELM）的算法。它消除了对单独奖励模型（RM）的需求，并通过一个直观的目标对LLMs进行迭代更新。与直接偏好优化（DPO）相比，SELM的目标降低了对未见过的过度延伸的无差别偏好，提高了探索效率。  我们的实验结果显示，在Zephyr-7B-SFT和Llama-3-8B-Instruct模型上进行微调后，SELM在MT-Bench和AlpacaEval 2.0等指令跟随基准以及不同设置下的各种标准学术基准上表现出显著的性能提升。我们的代码和模型已可在<https://github.com/shenao-zhang/SELM>获取。**|
|**2024-05-29**|**Normative Modules: A Generative Agent Architecture for Learning Norms that Supports Multi-Agent Cooperation**|Atrisha Sarkar et.al.|[2405.19328](http://arxiv.org/abs/2405.19328)|null|本文提出了一种名为“规范模块”的架构，它针对生成性代理在面对包含现有规范的社会结构时的协作挑战。这些代理通过大型语言模型理解和评估环境，但在处理复杂社会任务时，如何识别并适应规范基础设施成为关键问题。规范模块的核心在于促进均衡选择，借鉴分类机构实现相关均衡的概念，使代理能够通过同伴互动学习环境中不同候选机构中的权威性。通过提升规范能力，代理可以协调制裁行为，进而影响社交环境中的基本行为，从而提高整体福祉。  我们设计了一个支持机构的新环境，并根据两个主要标准来评估该框架：一是代理能否忽略非权威机构，二是代理在多个选项中识别权威机构的能力。实验结果显示，配备了规范模块的代理相比基础代理能实现更稳定的合作效果，这为研究设计考虑规范基础设施的环境和代理开辟了新途径。|
|**2024-05-29**|**MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series**|Ge Zhang et.al.|[2405.19327](http://arxiv.org/abs/2405.19327)|**[link](https://github.com/multimodal-art-projection/map-neo)**|近年来，大型语言模型（LLMs）在各种任务上取得了显著进步。然而，出于商业利益，像GPT、Gemini和Claude这样的最先进模型被封闭在专有接口后，其训练详情并未公开。近期，一些机构开源了类似性能的LLMs，如LLaMA-3，但大多数细节（如中间检查点、预训练语料库和训练代码等）仍未披露。为了提高LLMs的透明度，研究界正在推动真正开放的模型，如Pythia、Amber和OLMo，这些模型提供了更多的信息，促进了对大模型性能、局限性、偏见和风险的科学研究。然而，现有的开放模型在推理、知识和编程任务上的表现仍逊于同等规模的封闭源码模型。  因此，我们开源了MAP-Neo，一个拥有70亿参数的双语语言模型，从头开始在4.5万亿高质量令牌上进行训练。MAP-Neo是首个与现有顶级LLMs性能相当的完全开源的双语模型。此外，我们还公开了所有细节，包括清理后的预训练语料库、数据清洗流程、检查点以及优化的训练和评估框架，以供重现。我们期望MAP-Neo能推动开放研究社区的发展，激发更多创新，促进LLMs的进一步提升。|
|**2024-05-29**|**Reasoning3D -- Grounding and Reasoning in 3D: Fine-Grained Zero-Shot Open-Vocabulary 3D Reasoning Part Segmentation via Large Vision-Language Models**|Tianrun Chen et.al.|[2405.19326](http://arxiv.org/abs/2405.19326)|null|本文提出了一项新的任务：零样本3D推理分割，目标是针对物体的部件搜索和定位，这是一种超越了先前类别特定的3D语义分割、3D实例分割和开放词汇3D分割局限的新范式。我们设计了一个名为Reasoning3D的简单基线方法，它能够理解和执行复杂的命令，对3D网格进行（细致）部分分割，同时具备上下文感知和推理答案的交互式分割能力。特别地，Reasoning3D利用预训练的2D分割网络，该网络由大型语言模型（LLMs）驱动，在零样本情况下解析用户输入查询。已有研究表明，大规模预训练赋予基础模型世界知识的先验，使其能够理解复杂指令，这使得我们在依赖有限3D数据集的情况下也能“分割任何东西”（源效率高）。实验表明，我们的方法具有泛化性，能有效根据隐性文本查询在3D对象（3D网格）中定位和突出显示部分，包括可动3D对象和真实世界的扫描数据。此外，我们的无监督方法便于快速部署，并为未来3D（语义）对象理解领域的研究，如机器人、物体操作、部件组装、自动驾驶应用、增强现实和虚拟现实（AR/VR）、以及医疗应用，提供了一个可行的通用基准。代码、模型权重、部署指南和评估协议可在以下链接获取：http://tianrun-chen.github.io/Reason3D/。|
|**2024-05-29**|**Nearest Neighbor Speculative Decoding for LLM Generation and Attribution**|Minghan Li et.al.|[2405.19325](http://arxiv.org/abs/2405.19325)|null|大型语言模型（LLMs）常常会产生虚构内容且缺乏对生成文本的来源标注。为解决这些问题，半参数化语言模型如kNN-LM通过在非参数数据存储中寻找与给定提示最接近的邻居来改进LM输出。然而，这类模型的推理速度通常较慢，生成的文本流畅度不高。本文提出了一种新颖的半参数化语言建模方法——Nearest Neighbor Speculative Decoding（NEST），它能够将现实世界中的任意长度文本片段融入生成过程，并提供其源头的标注。NEST在每次推理步骤中进行基于令牌的检索，计算出一个半参数混合分布，并从语料库中识别出可能的连续文本段落扩展。它采用一种近似推测解码策略，接受检索到的片段前缀或生成新的令牌。NEST显著提高了基础LM在各种知识密集型任务中的生成质量和来源标注率，超越了传统的kNN-LM方法，并在基于上下文的检索增强方面表现出竞争力。此外，NEST大幅提升了生成速度，当应用于Llama-2-Chat 70B时，推理时间提高了1.8倍。|
|**2024-05-29**|**Are Large Language Models Chameleons?**|Mingmeng Geng et.al.|[2405.19323](http://arxiv.org/abs/2405.19323)|null|大语言模型（LLMs）是否拥有自己的世界观和人格倾向？研究人员进行了超过一百万次的实验，让LLMs回答主观问题。通过将这些模型的响应与欧洲社会调查（ESS）的实际数据进行比较，结果显示提示对偏见和变异性有显著影响，揭示了重大的文化、年龄和性别偏差。文中讨论了评估LLMs与调查数据差异的方法，如计算加权平均值以及一个新提出的基于Jaccard相似性的测量指标。研究者强调，在利用LLMs模拟个体决策或集体行为之前，分析提示的稳健性和变异性至关重要，因为它们的模仿能力充其量只能说是近似的。|
|**2024-05-29**|**Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF**|Shicong Cen et.al.|[2405.19320](http://arxiv.org/abs/2405.19320)|null|**摘要：**  强化学习从人类反馈（RLHF）在调整大型语言模型（LLMs）以符合人类偏好方面展现出巨大潜力。在线和离线RLHF都处于活跃的研究阶段，但关键挑战之一是如何在处理从偏好数据中学习的奖励函数不确定性时。尽管标准强化学习（RL）中乐观主义或悲观主义的原则已广为人知，但在大型语言模型中实现既实用又基于理论的方法尚不成熟，因为构建置信区间的标准技术在处理任意策略参数化时变得难以处理。  本文提出了一种统一的在线和离线RLHF方法——价值激励的偏好优化（VPO）。VPO通过在最大似然估计的奖励函数中添加相应的值函数的正则化，以指示选择乐观主义还是悲观主义，实现了这一目标。此外，VPO直接优化策略，并利用隐式奖励建模，因此其RLHF管道与直接偏好优化更为简单。对于在线和离线设置，VPO提供了理论保证，其收敛速度与标准RL相当。实验在文本摘要和对话任务上验证了VPO的实用性与有效性。|
|**2024-05-28**|**Don't Forget to Connect! Improving RAG with Graph-based Reranking**|Jialin Dong et.al.|[2405.18414](http://arxiv.org/abs/2405.18414)|null|## 背景  检索增强生成（Retrieval Augmented Generation，RAG）通过结合现有文档的上下文显著提升了大语言模型（Large Language Model，LLM）的响应性能。然而，当文档与问题上下文的相关性不明显或存在部分信息时，RAG的效果如何？又该如何处理文档之间的关联性呢？本研究旨在解答RAG生成中的这两个核心问题。我们提出了一种名为G-RAG的方法，它是一个基于图神经网络（Graph Neural Networks，GNNs）的重排器，介于RAG的检索器和阅读器之间。G-RAG结合了文档之间的连接性和语义信息（通过抽象意义表示图），为RAG提供了一个具有上下文感知的排名器。实验结果表明，G-RAG超越了现有的领先方法，同时计算开销更小。此外，我们评估了PaLM 2作为重排器的表现，发现其明显逊色于G-RAG，这强调了即使使用大型语言模型，重排在RAG中的重要性。|
|**2024-05-28**|**Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning**|Yixiao Zhang et.al.|[2405.18386](http://arxiv.org/abs/2405.18386)|**[link](https://github.com/ldzhangyx/instruct-MusicGen)**|**在文本到音乐编辑领域，近期的进步依赖于文本查询来改变音乐风格或调整乐器元素。然而，现有方法要么需要从头训练特定的编辑模型，耗时且资源密集，要么使用大型语言模型预测编辑后的音乐，导致音频重建不够精确。为了结合优点并解决这些问题，我们提出了Instruct-MusicGen，这是一种新颖的方法，它针对预训练的MusicGen模型进行微调，以高效地执行编辑指令，如添加、删除或分离音轨。我们的方法修改了原始MusicGen架构，引入了文本融合模块和音频融合模块，使模型能够同时处理指令文本和音频输入，生成所需的编辑音乐。令人惊讶的是，Instruct-MusicGen仅向原始模型增加了8%的新参数，并在5000步的训练后，其性能超越现有基准，且表现出与专门针对任务训练的模型相当的能力。这一进展不仅提高了文本到音乐编辑的效率，还拓宽了音乐语言模型在动态音乐制作环境中的应用范围。**|
|**2024-05-28**|**OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection for Memory-Efficient LLM Fine-tuning**|Pengxiang Li et.al.|[2405.18380](http://arxiv.org/abs/2405.18380)|**[link](https://github.com/pixeli99/owlore)**|**随着大型语言模型（LLMs）的快速发展，它们在自然语言处理任务中带来了革命性变化。然而，大模型的训练或微调带来了巨大挑战。针对这一问题，低秩适应（LoRA）等参数高效方法崭露头角，但往往牺牲性能。本文提出了一种新的内存高效微调方法——Outlier-weighed Layerwise Sampled Low-Rank Projection（OwLore），它受到LLMs层间异常分布的启发，通过动态采样预训练层而非添加额外适配器来进行微调。我们首先通过Heavy-Tailed Self-Regularization理论（HT-SR）解读异常现象，发现具有更多异常值的层更倾向于呈现长尾分布，训练效果更好。因此，OwLore策略性地为异常值较多的层分配更高的采样概率，以更好地利用预训练模型的知识。  为了进一步减少微调时的内存需求，我们结合梯度低秩投影，使得每一层能以低秩方式高效训练。通过融合低秩优势和最优层别采样策略，OwLore显著优化了LLM剪枝中的内存-性能权衡。我们在多个架构，如LLaMa2、LLaMa3和Mistral上的广泛实验表明，OwLore持续优于基础方法，包括全量微调。例如，在常识推理基准上，OwLore可实现平均1.1%的精度提升，MMLU上提高3.0%，而在MT-Bench上更是有显著的10%提升，同时内存效率更高。特别地，OwLore仅需21GB内存即可对LLaMa2-7B进行微调。**|
|**2024-05-28**|**LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models**|Anthony Sarah et.al.|[2405.18377](http://arxiv.org/abs/2405.18377)|null|现代大型语言模型（LLMs）在自然语言处理、复杂推理、情感分析等任务中的卓越表现推动了它们的广泛应用。然而，这些强大的功能伴随着巨大的内存和计算成本，限制了在大多数硬件平台上的使用。为解决这一问题，我们提出了一种有效的方法，基于LLaMA2-7B进行单次微调后，通过遗传算法搜索找到更小、计算复杂度更低的网络架构。实验表明，对于某些标准基准任务，预训练的LLaMA2-7B模型实际上过于庞大且复杂。我们实现了1.5倍的模型大小缩减和1.3倍的吞吐量提升，同时保持了几乎无损的准确性。相较于某些剪枝或稀疏化技术，我们的方法在效率和效果上更为优越。最后，我们展示了量化与我们的方法相结合的效果，进一步通过量化减少了找到的网络的大小和复杂性。我们相信，本工作提供了一种自动创建可在更廉价和广泛可用硬件平台上使用的LLMs的方法。|
|**2024-05-28**|**Empowering Source-Free Domain Adaptation with MLLM-driven Curriculum Learning**|Dongjie Chen et.al.|[2405.18376](http://arxiv.org/abs/2405.18376)|**[link](https://github.com/Dong-Jie-Chen/RCL)**|**### 背景  源免费领域适应（SFDA）的目标是仅使用未标记的靶域数据来调整预训练的源模型。当前的SFDA方法在有效利用预训练知识和挖掘靶域数据潜力方面面临挑战。多模态大型语言模型（MLLMs）在理解视觉和文本信息方面表现出色，但它们应用于SFDA时存在问题，如指令执行失败、计算需求高以及在适应前性能评估困难。为了缓解这些问题，我们提出了一种新颖的框架——可靠性基于课程学习（RCL），它通过伪标签化整合多个MLLM以促进知识利用，应用于SFDA。  ### 方法  我们的框架包括：1) 可靠知识转移，2) 自我纠正，3) MLLM引导的知识扩展，以及4) 多热掩码精炼，这些方法协同作用，逐步发掘靶域未标记数据的价值。RCL在多个SFDA基准上实现了最先进的（SOTA）性能，例如在DomainNet上提升显著，达到 $\textbf{+9.4\%}$ ，证明了其在增强适应性和鲁棒性方面的有效性，同时无需访问源数据。代码可在https://github.com/Dong-Jie-Chen/RCL获取。**|
|**2024-05-28**|**Thai Winograd Schemas: A Benchmark for Thai Commonsense Reasoning**|Phakphum Artkaew et.al.|[2405.18375](http://arxiv.org/abs/2405.18375)|**[link](https://github.com/PhakphumAdev/Thai-Winograd)**|常识推理是自然语言理解的重要组成部分，为此已开发出多个评估基准。然而，这些基准大多仅限于英语。创建平行基准有助于跨语言评估，从而更好地理解不同语言。本研究介绍了一个泰语版的Winograd Schema集合，这是一个专为测试泰语中的常识推理能力而设计的新数据集。我们通过邀请母语者、专业翻译和严格验证的方法，确保该系列题库能准确反映泰国语言的独特性、习语和文化引用，同时保持模糊性和常识挑战。我们对大型语言模型（如GPT-4和Claude-3-Opus）在这项基准上的性能进行了评估，结果显示尽管在英语上表现优异，但它们在泰语中的性能明显下降，这表明在多语言常识推理方面仍有待进步。|
|**2024-05-28**|**PromptWizard: Task-Aware Agent-driven Prompt Optimization Framework**|Eshaan Agarwal et.al.|[2405.18369](http://arxiv.org/abs/2405.18369)|null|大型语言模型（LLMs）已经在各个领域带来了革命性的变化，展现出卓越的能力。它们成功的关键在于提示的概念，即指导模型生成输出。然而，手动创建提示既耗时又局限于特定领域，因此需要自动化的解决方案。本文介绍PromptWizard，一个新颖的框架，它利用LLMs迭代地合成和优化针对特定任务的提示。与现有方法不同，PromptWizard同时优化提示指令和上下文示例，以最大化模型性能。该框架通过变异指令并引入负例，逐步深化理解并保证多样性。借助一个评判者，PromptWizard进一步改进指令和示例，融入详细的推理步骤，以实现最佳表现。PromptWizard具有计算效率高、适应不同训练数据量场景以及在小型LLM上同样有效的特点。通过对8个数据集的35个任务进行严谨评估，结果显示PromptWizard明显优于现有的提示策略，证明了其在提示优化方面的高效性和可扩展性。|
|**2024-05-28**|**Is a 3D-Tokenized LLM the Key to Reliable Autonomous Driving?**|Yifan Bai et.al.|[2405.18361](http://arxiv.org/abs/2405.18361)|null|随着自动驾驶（AD）任务的快速发展，基于端到端的方法，特别是视觉语言模型（VLM）的应用变得尤为重要。这些模型试图融合强大的逻辑推理和认知能力，以实现全面的端到端规划。然而，现有的VLM方法往往依赖于2D视觉分词器和大型语言模型（LLM），在处理三维几何信息方面存在不足，这对于可靠的规划至关重要。研究表明，2D分词的LLM并不能准确感知三维环境，这引发了关于VLM在自动驾驶中可靠性的质疑。  针对这一问题，我们提出了一种名为Atlas的新方法，它结合了DETR风格的3D感知器作为3D分词器，与单层线性投影器相连，巧妙地利用了三维物理世界的固有特性。这种方法允许高分辨率多视角图像的同时处理和时空建模。尽管简单，但Atlas在NuScenes数据集上的3D检测和自主驾驶规划任务中表现出色，证明了3D分词的LLM对于实现可靠自动驾驶至关重要。我们将开源代码和数据集，以供进一步研究。|
|**2024-05-28**|**Bridging the Gap: Dynamic Learning Strategies for Improving Multilingual Performance in LLMs**|Somnath Kumar et.al.|[2405.18359](http://arxiv.org/abs/2405.18359)|null|大型语言模型（LLMs）正在全球范围内重塑众多领域，但它们在处理非拉丁字母和低资源语言时的包容性和效果仍有待提升。本文针对这一关键挑战，提出了一种无需大量训练或微调的方法来增强多语言LLMs的表现。通过系统地研究和评估各种语言在流行的问题解答（QA）数据集上的性能，我们提出了一系列新颖技术，以释放LLMs在多元语言环境中的真正潜力。我们的方法包括三个核心策略，极大地提高了多语言能力：首先，精心优化适用于多语言LLM的提示，挖掘其潜在能力，显著提升了各语言的表现。其次，我们引入了一种新的混合方法，结合了多语言嵌入的LLM检索增强生成（RAG），实现了更好的多任务性能。最后，我们开发了一种动态学习策略，实现实时根据查询动态选择最合适的提示策略、LLM模型和嵌入模型，从而最大化LLM在不同语言上的效率，超越了最佳静态和随机策略。此外，我们的方法既适用于离线配置调整，也支持在线适应，能够无缝适应新语言和数据集，显著推动了多语言理解和生成在各种语言中的进步。|
|**2024-05-28**|**MMCTAgent: Multi-modal Critical Thinking Agent Framework for Complex Visual Reasoning**|Somnath Kumar et.al.|[2405.18358](http://arxiv.org/abs/2405.18358)|null|## 背景  近期的多模态大型语言模型（MLLM）在视觉与语言融合任务上取得了显著进步。然而，它们在细致的多模态理解、复杂任务解析以及多模态信息推理方面仍存在挑战。本文提出MMCTAgent，一个旨在解决当前MLLM在复杂视觉推理任务中固有局限性的新型多模态批判性思维代理框架。MMCTAgent借鉴了人类认知过程和批判性思考的特点，通过迭代分析多模态信息、拆解问题、规划策略，并实现动态推理。  此外，MMCTAgent还融入了批判性思考元素，如对最终答案的验证和自我反思。它通过一种新颖的方法定义基于视觉的评判者，并确定特定任务的评估标准，从而提升决策能力。在多个图像理解和视频理解基准测试中，我们严谨地评估了MMCTAgent（包括带评判者的版本）的表现，结果表明它在超越基础MLLM和其他工具增强的管道方面表现出色。|
|**2024-05-27**|**Matryoshka Multimodal Models**|Mu Cai et.al.|[2405.17430](http://arxiv.org/abs/2405.17430)|null|## 背景  大型多模态模型（如LLaVA）在视觉-语言推理方面表现出色。这些模型首先将图像嵌入到大量的固定视觉令牌中，然后将它们输入到大型语言模型（LLM）。然而，这种设计在处理高分辨率图像和视频等密集视觉场景时会导致大量令牌，从而导致效率低下。尽管存在令牌剪枝/合并方法，但它们为每个图像生成单个长度的输出，无法在信息密度与效率之间灵活权衡。受到套娃玩偶概念的启发，我们提出了M3：套娃多模态模型，它学习将视觉内容表示为捕捉不同粗细粒度信息的嵌套视觉令牌集合。  ## 任务  我们的方法为LMMs带来了几个独特的优势：(1) 在测试实例中，用户可以明确控制视觉粒度，例如，根据内容的复杂性或简洁性调整用于表示图像的令牌数量；(2) M3提供了一个分析现有数据集所需粒度的框架，我们发现像COCO这样的基准只需要大约~9个视觉令牌就能获得与使用所有576个令牌相当的准确性；(3) 我们的方法为探索性能与视觉令牌长度之间的最佳权衡提供了基础，研究显示当前固定规模表示与理想上限之间存在显著差距。|
|**2024-05-27**|**NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models**|Chankyu Lee et.al.|[2405.17428](http://arxiv.org/abs/2405.17428)|null|本文介绍了一种名为NV-Embed的新型大语言模型，专门设计用于提升基于解码器的大型语言模型在文本嵌入任务中的性能，包括密集向量检索。NV-Embed通过多种架构设计和训练策略显著增强模型的灵活性和表现，同时保持其简洁性和可复现性。  在架构方面，我们引入了隐式注意力层来获取池化嵌入，这在检索和下游任务准确性上均优于平均池化或使用LLMs的最后一个<EOS> token嵌入。为了改进表示学习，我们移除了LLMs的自回归注意力掩码，在对比性训练中允许更全面的信息交互。  在训练策略上，我们采用两阶段的对比性指令调优方法。第一阶段在检索数据集上进行指令训练，利用批次内负样本和精心挑选的难例。第二阶段将各种非检索任务的数据融入指令调优，不仅提高非检索任务的准确性，还提升了检索性能。  凭借这些创新，NV-Embed仅使用公开数据就实现了前所未有的高分，达到69.32，荣登大规模文本嵌入基准（MTEB）（截至2024年5月24日）榜首，涵盖56项任务，包括检索、重排、分类、聚类和语义文本相似度。尤其值得注意的是，我们的模型在BEIR的15项检索任务中取得了最高的59.36分。NV-Embed模型的源代码将在以下网址开源：https://huggingface.co/nvidia/NV-Embed-v1。|
|**2024-05-27**|**Reason3D: Searching and Reasoning 3D Segmentation via Large Language Model**|Kuan-Chih Huang et.al.|[2405.17427](http://arxiv.org/abs/2405.17427)|**[link](https://github.com/kuanchihhuang/reason3d)**|**随着多模态大型语言模型（LLMs）的最新进展，它们在概念推理等领域展现出巨大潜力。然而，在理解三维环境方面的应用仍相对有限。本文提出Reason3D，这是一种专为全面3D理解设计的新颖LLM。Reason3D接受点云数据和文本提示作为输入，生成文本响应和分割掩码，支持高级任务，如3D推理分割、层次搜索、表达式指代和详细掩码输出的问答。特别是，我们设计了一种分层掩码解码器，能够精确定位广阔场景中的小物体。该解码器首先生成一个粗略的位置估计，覆盖物体的大致区域，然后采用逐步细化的策略，显著提高对象识别和分割的精度。实验结果显示，Reason3D在ScanNet和Matterport3D等大规模数据集上，在3D表达式指代、3D问答和3D推理分割任务上表现出卓越性能。代码和模型已在以下链接提供：https://github.com/KuanchihHuang/Reason3D。**|
|**2024-05-27**|**LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence**|Zhuoling Li et.al.|[2405.17424](http://arxiv.org/abs/2405.17424)|null|由于实体代理需要与现实世界互动，它们必须具备全面的先验知识、长远规划能力以及快速响应速度。尽管近期基于大型语言模型（LLM）的代理表现出色，但它们仍存在一些局限性。例如，LLM的输出通常是描述性的句子，在确定具体动作时可能存在歧义。为了克服这些问题，我们提出了大型自回归模型（LARM）。LARM利用文本和多视角图像作为输入，并以自回归方式预测后续动作。为了训练LARM，我们开发了一种新颖的数据格式，称为自回归节点传输结构，并构建了相应的数据集。通过两阶段训练，LARM成功在《我的世界》（Minecraft）中收集魔法装备，这比先前最佳方法所能达到的成就需要更复杂的决策链。此外，LARM的速度是最快的，比以前快6.8倍。|
|**2024-05-27**|**Self-Corrected Multimodal Large Language Model for End-to-End Robot Manipulation**|Jiaming Liu et.al.|[2405.17418](http://arxiv.org/abs/2405.17418)|null|当机器人操作策略面对新任务或物体实例时，其动作性能往往不尽人意。因此，自动检测和自我纠正失败动作的能力对于实际的机器人系统至关重要。近期，多模态大型语言模型（Multimodal Large Language Models，MLLM）在视觉指令跟随方面展现出前景，并在多种任务中展现出强大的推理能力。为了将通用MLLM作为端到端的机器人代理，我们提出了Self-Corrected (SC)-MLLM，不仅使其能够预测末端执行器位置，还赋予其自主识别并纠正错误动作的能力。首先，我们通过参数效率高的微调，使MLLM具备姿态预测功能，将其转化为一个语言建模问题。在遇到执行失败时，模型能识别低层次动作错误的原因（如位置和旋转误差），并主动寻求专家的提示。根据反馈，SC-MLLM会重新思考当前失败场景，生成修正后的动作。此外，我们设计了一种连续策略学习方法，针对成功纠正的样本，提升模型对当前场景配置的适应性，减少专家干预的频率。  为了评估我们的SC-MLLM，我们在模拟和真实世界环境中进行了广泛实验。结果表明，与先前最先进的机器人MLLM（ManipLLM）相比，SC-MLLM显著提高了操作精度：在已知物体类别上从57%提升至79%，在未知新类别上从47%提升至69%。|
|**2024-05-27**|**THREAD: Thinking Deeper with Recursive Spawning**|Philip Schroeder et.al.|[2405.17402](http://arxiv.org/abs/2405.17402)|**[link](https://github.com/philipmit/thread)**|大型语言模型（LLMs）在各种场景中展现出卓越的能力，但随着上下文的长度和复杂度增加，它们仍面临挑战。为此，我们提出了Thinking Recursively and Dynamically（ThReaD）方法。ThReaD将模型生成过程构想为一个执行流程，根据上下文可以完整运行或动态地创建新线程。通过子线程，模型可以分发任务（如思考、获取信息），子线程只返回父线程所需的令牌，从而让模型能够根据需要调整产生令牌时使用的中间工作量。我们在任务解决和问答等场景中应用ThReaD，使其能递归地将给定的任务或问题分解为逐步简化的小子问题，由单独的子线程解决。我们使用少量样本学习的方式实现ThReaD，并在包括ALFWorld、TextCraft、WebShop在内的多个基准测试上评估GPT-4和GPT-3.5的表现，以及两个新基准：DataCommons QA和MIMIC-III ICU QA。实验结果显示，ThReaD在这些基准上实现了最先进的性能，相对于现有框架，即使是小型模型（如Llama-3-8b和CodeLlama-7b）也能提升10%到50%的绝对分数。|
|**2024-05-27**|**MindMerger: Efficient Boosting LLM Reasoning in non-English Languages**|Zixian Huang et.al.|[2405.17386](http://arxiv.org/abs/2405.17386)|**[link](https://github.com/cone-mt/mindmerger)**|## 任务  推理能力对于大型语言模型（LLMs）至关重要，但英语与其他非英语语言之间的差距明显。一些研究通过微调LLMs以重新学习非英语的推理能力，而另一些方法则使用外部模型（如英语翻译文本）的输出来替换非英语输入，以应对LLM理解非英语的挑战。然而，这些方法往往未能充分利用LLMs内在的推理和语言理解能力。为了更好地利用LLMs的思维和语言理解能力，我们提出了一种新方法，称为MindMerger，它将LLMs与多语言模型的外部语言理解能力相结合，以提升多语言推理性能。我们还引入了两步训练策略，首先将外部能力嵌入LLMs，然后训练外部能力和内置能力的协作使用。在三个多语言推理数据集和一个语言理解数据集上的实验表明，MindMerger始终优于所有基线，特别是在低资源语言上。在不更新LLMs参数的情况下，MGSM数据集上所有语言的平均准确率提高了6.7%，低资源语言提高了8.0%。|
|**2024-05-27**|**ReMoDetect: Reward Models Recognize Aligned LLM's Generations**|Hyunseok Lee et.al.|[2405.17382](http://arxiv.org/abs/2405.17382)|null|随着大型语言模型（LLMs）的卓越性能和易用性提升，它们带来的社会风险，如假新闻生成，促使开发出能检测LLM生成文本（LGT）的方法以确保安全使用。然而，由于大量LLM的存在，逐个识别它们的特点变得不切实际。因此，研究关注的是这些强大模型共有的特性，即“对齐训练”，即训练LLMs生成更符合人类偏好的文本。我们的关键发现是，随着这些对齐训练的LLMs致力于最大化人类偏好，它们生成的文本甚至比人类撰写的文本在估计偏好上更高，这使得利用偏好模型（一个训练来模拟人类偏好分布的LLM）轻易就能检测到这些文本。  基于这一发现，我们提出两种进一步增强偏好模型检测能力的训练策略：（1）持续偏好微调，使模型更偏向于识别对齐的LLG；（2）奖励模型对人/LLM混合文本的学习，即使用对齐LLM重述的人类原创文本，这是一种介于LGT和人类文本之间的偏好基准，有助于更好地学习决策边界。我们在六个文本领域和十二种对齐LLM上进行了广泛评估，结果显示我们的方法表现出最先进的性能。相关代码已在https://github.com/hyunseoklee-ai/reward_llm_detect上提供。|
|**2024-05-27**|**RTL-Repo: A Benchmark for Evaluating LLMs on Large-Scale RTL Design Projects**|Ahmed Allam et.al.|[2405.17378](http://arxiv.org/abs/2405.17378)|**[link](https://github.com/AUCOHL/RTL-Repo)**|大型语言模型在辅助进行寄存器传输级（Register Transfer Level, RTL）设计任务上展现出潜力。然而，现有的基准测试在反映真实世界RTL项目复杂性方面存在显著差距。为此，该论文提出了一项新的基准——RTL-Repo，专为评估大型语言模型在大规模RTL设计项目中的性能而设计。RTL-Repo包含了从GitHub公共仓库提取的超过4000个Verilog代码样本，每个样本都提供了对应仓库的完整上下文。我们对包括GPT-4、GPT-3.5、Starcoder2以及像VeriGen和RTLCoder这样的Verilog专用模型在内的多款最先进的模型在RTL-Repo基准上的性能进行了评估，比较它们在生成复杂项目的Verilog代码方面的表现。RTL-Repo为硬件设计社区提供了一个宝贵的资源，用于评估和比较语言模型在实际RTL设计场景中的性能，并针对复杂的多文件RTL项目专门训练Verilog代码生成。RTL-Repo是开源的，已在GitHub上公开可用。|
|**2024-05-28**|**Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models**|ShengYun Peng et.al.|[2405.17374](http://arxiv.org/abs/2405.17374)|null|### 背景  安全校准是确保大型语言模型（LLMs）的行为符合人类偏好并避免有害行为的关键，但近期研究显示，仅使用少量精心设计的训练样本来微调模型可能导致安全性被轻易破坏。我们致力于通过探索LLM的安全景观来评估微调过程中的风险。我们发现了一个普遍存在于流行开源LLM模型参数空间中的新现象，称为“安全盆地”：随机扰动模型权重能使模型在局部区域保持原始校准模型的安全性。  ### 发现与贡献  我们的发现启发我们提出了一种新的安全度量方法——VISAGE，它通过探测模型的安全景观来评估LLM微调过程中的安全性。可视化校准模型的安全景观有助于理解微调如何使模型偏离安全盆地，从而损害安全性。此外，我们观察到系统提示在保护模型方面的重要性，这种保护甚至会传递给处于安全盆地内的扰动版本。这些从安全景观研究中得出的见解为未来LLM安全领域的研究提供了新的洞见。|
|**2024-05-24**|**Scaling Laws for Discriminative Classification in Large Language Models**|Dean Wyatte et.al.|[2405.15765](http://arxiv.org/abs/2405.15765)|null|## 背景  现代大型语言模型（LLMs）标志着机器学习模型能力的一个重大飞跃。这些模型能够对各种查询生成合理的回答，这表明它们在客户服务应用中具有潜力。然而，LLMs已被观察到存在胡言乱语的问题，这在短期内限制了它们在客户服务中的应用。为了解决这个问题，我们提出了一种系统，将语言建模任务重新构想为分类任务，以帮助客户服务代表选择最佳的模板回复。我们的目标是为客服代表提供最合适的前K个候选回复。  ## 任务描述  我们展示了离线和在线实验的结果，证明了实验系统的有效性，离线实验显示出改进，而在线实验则带来了统计显著的效果提升。此外，我们分享了通过模型参数调整进行的验证损失和前K精度的度量曲线。最后，我们讨论了模型大小、延迟和准确性之间的权衡，并展望了未来可能的应用领域。|
|**2024-05-24**|**Large Language Models Reflect Human Citation Patterns with a Heightened Citation Bias**|Andres Algaba et.al.|[2405.15739](http://arxiv.org/abs/2405.15739)|**[link](https://github.com/andresalgaba/llm_citation_patterns)**|论文摘要： 引用实践对于构建科学知识结构至关重要，但往往受到当代规范和偏见的影响。随着大型语言模型（如GPT-4）的出现，这一领域出现了新的动态。研究者首次探索了完全依赖参数知识而非基于搜索或检索增强生成的推荐引用的特性及其潜在偏见。实验使用了一组包含166篇来自AAAI、NeurIPS、ICML和ICLR的论文，这些论文在GPT-4的知识截止日期后发表，涉及3,066个引用。实验让GPT-4为匿名文本中的引用提供学术参考。结果揭示了人类和语言模型（如GPT-4）的引用模式惊人相似，但GPT-4显示出更强的高引用偏见，即使在控制了出版年份、标题长度、作者数量和会议等因素后依然存在。此外，我们发现GPT-4生成的既有和不存在引用的特性高度一致，表明模型内化了引用模式。通过分析引用图谱，显示GPT-4推荐的引用嵌入在相关引用网络中，暗示其对概念的深入理解。尽管语言模型可以辅助引用生成，但它们也可能放大现有偏见并引入新偏见，可能影响科学知识的传播。我们的结果强调了识别模型偏见的必要性，并开发平衡的方法与语言模型互动的重要性。|
|**2024-05-24**|**LM4LV: A Frozen Large Language Model for Low-level Vision Tasks**|Boyang Zheng et.al.|[2405.15734](http://arxiv.org/abs/2405.15734)|**[link](https://github.com/bytetriper/lm4lv)**|大型语言模型（LLMs）的成功催生了多模态大型语言模型（MLLMs）的研究热潮，它们正在改变计算机视觉领域的多个研究范式。尽管MLLMs在诸如视觉问答（VQA）和文本到图像等高级视觉和 Vision-and-Language 任务上表现出色，但尚无研究探讨过低级视觉任务如何从这些模型中受益。我们发现，当前大多数MLLM的设计使其对低级特征视而不见，因此在解决低级视觉任务方面存在固有限制。为此，我们提出 $\textbf{LM4LV}$ ，这是一个框架，它允许一个冻结的LLM无需任何多模态数据或先验知识就能解决一系列低级视觉任务。这突显了LLMs在低级视觉领域的强大潜力，并弥合了MLLMs与低级视觉任务之间的鸿沟。我们期望这项工作能激发对LLMs的新视角，加深对其工作机制的理解。|
|**2024-05-24**|**Optimizing Large Language Models for OpenAPI Code Completion**|Bohdan Petryshyn et.al.|[2405.15729](http://arxiv.org/abs/2405.15729)|**[link](https://github.com/BohdanPetryshyn/openapi-completion-benchmark)**|近期，大型语言模型（LLMs）在代码生成任务中的进步极大地改变了软件开发领域。尽管主流编程语言的代码补全解决方案表现出色，但它们在处理较少见的格式，如OpenAPI定义时性能欠佳。本研究评估了GitHub Copilot，一个流行的商业代码补全工具，在OpenAPI完成任务中的表现，并针对Meta开源的Code Llama模型提出了一系列针对该任务的优化策略。研究中设计了一个语义感知的OpenAPI完成基准，通过实验分析了不同提示工程和微调技术对Code Llama模型性能的影响。经过微调的Code Llama模型在正确性上达到了比GitHub Copilot高出55.2%的峰值，同时其参数数量仅为商业解决方案（基于Codex模型）的1/25。此外，研究还改进了一种广泛使用的代码填充训练方法，解决了模型在接收到小于训练时使用的上下文长度提示时的性能不足问题。|
|**2024-05-24**|**Prompt-Aware Adapter: Towards Learning Adaptive Visual Tokens for Multimodal Large Language Models**|Yue Zhang et.al.|[2405.15684](http://arxiv.org/abs/2405.15684)|null|为了弥合视觉和语言模态之间的鸿沟，多模态大型语言模型（Multimodal Large Language Models，MLLMs）通常会学习一个适配器，将视觉输入转化为大语言模型（LLMs）能理解的令牌。然而，大多数适配器生成的视觉令牌相对固定，不考虑提示中提及的具体对象。由于这些适配器对图像中的每个细节分配同等关注，且倾向于处理整个场景，这可能会增加大语言模型在处理复杂场景时的认知负荷。为此，我们提出了提示感知适配器。这类适配器设计有根据提示特定关注点动态嵌入视觉输入的能力。具体来说，提示感知适配器利用全局和局部文本特征，在粗粒度和细粒度层次上捕捉与提示最相关的视觉线索。这种方法显著提升了大语言模型理解和解释视觉内容的能力。在各种视觉问答任务中，如计数和位置推理实验中，提示感知适配器的效果得到了验证。|
|**2024-05-24**|**What Do You See? Enhancing Zero-Shot Image Classification with Multimodal Large Language Models**|Abdelrahman Abdelhamed et.al.|[2405.15668](http://arxiv.org/abs/2405.15668)|null|这篇论文探讨了如何利用大型语言模型（LLMs）进行零样本图像分类。作者提出了一种简单但有效的方法，通过将多模态LLMs应用于图像输入，生成详尽的文本表示。这些文本表示被转化为跨模态嵌入空间中的固定维特征，并结合使用于零样本分类，无需为每个数据集设计复杂的提示。研究者采用通用提示策略，而非针对每个数据集单独调整。实验结果显示，这种方法在多个数据集上表现出色，比先前方法的准确性有所提升。平均而言，在十个基准测试中，该方法比传统方法提高了4.1个百分点，尤其在ImageNet数据集上的提升达到了6.8个百分点。这表明，多模态LLMs有潜力显著增强如零样本图像分类之类的计算机视觉任务，为现有技术带来了显著的进步。|
|**2024-05-24**|**Class Machine Unlearning for Complex Data via Concepts Inference and Data Poisoning**|Wenhan Chang et.al.|[2405.15662](http://arxiv.org/abs/2405.15662)|null|在人工智能时代，用户可能因隐私顾虑要求AI公司从训练数据集中删除他们的信息。作为模型所有者，重新训练模型会消耗大量计算资源，因此机器遗忘（machine unlearning）技术应运而生，以允许删除请求的训练数据或类别，同时尽量减少对模型性能的影响。然而，对于大规模复杂数据，如图像或文本，从模型中“遗忘”一个类别可能导致性能下降，因为难以确定类别与模型之间的关联。为此，我们提出使用概念（Concept）而非图像特征或文本数据中的令牌来表示要删除类别的语义信息，这有助于切断模型与类别的联系，实现彻底消除影响。  为了分析复杂数据中的概念影响，我们采用了后处理概念瓶颈模型和集成梯度技术，精确识别不同类别中的概念。然后，我们利用随机标签和目标标签的数据污染策略，提出遗忘方法。我们在图像分类模型和大型语言模型（LLMs）上测试了我们的方法，结果一致显示，提出的策略能准确地从模型中抹除目标信息，同时保持模型性能的大部分。|
|**2024-05-24**|**$$\mathbf{L^2\cdot M = C^2}$$ Large Language Models as Covert Channels... a Systematic Analysis**|Simen Gaure et.al.|[2405.15652](http://arxiv.org/abs/2405.15652)|null|近年来，大型语言模型（LLMs）因其在翻译、预测和内容生成等任务中的出色表现而备受瞩目。同时，研究界发现LLMs易受攻击，但也能增强系统的安全性。然而，这些开源的LLMs在作为掩蔽通信媒介，如支持抗审查通信方面的能力如何呢？本论文从实验角度出发，通过实证测量开源LLM模型（Llama-7B）的安全性与容量，以评估其作为掩蔽通信的有效性。尽管结果显示，基于这种模型的通道不太可能实现高实际比特率，这取决于消息长度和模型熵，但我们发现对手发现隐秘通信的可能性较低。为了使结果易于广泛参考，我们采用了一个简单且直观的方案，并假设模型是公开可用的。|
|**2024-05-24**|**LLM-based Robot Task Planning with Exceptional Handling for General Purpose Service Robots**|Ruoyu Wang et.al.|[2405.15646](http://arxiv.org/abs/2405.15646)|null|在日常生活中开发通用服务机器人的需求促使机器人必须能恰当地执行多种基础行为。近期，大规模语言模型（LLMs）的训练进步使得可以直接根据自然语言指令生成任务序列，无需额外的领域知识。然而，尽管LLMs的输出在语义上是正确的，但生成的任务计划可能并不精确地对应于可接受的动作，并且可能存在各种语言模糊性。LLM的幻觉问题对机器人任务规划构成挑战，可能导致生成的内容与现实世界事实或用户输入不符。为此，我们提出了一种基于约束LLM提示的任务规划方法，该方法可以从命令中生成可执行的动作序列。此外，我们还设计了一个异常处理模块来应对LLM幻觉问题，确保生成的结果在当前环境中是可接纳的。我们在RoboCup@Home命令生成器生成的命令上测试了我们的方法，结果显示机器人在理解和执行任务方面表现出色。|
|**2024-05-24**|**GECKO: Generative Language Model for English, Code and Korean**|Sungwoo Oh et.al.|[2405.15640](http://arxiv.org/abs/2405.15640)|null|我们介绍GECKO，一个专为韩语和英语（包括编程语言）设计的双语大语言模型（LLM）。它基于LLaMA架构，使用平衡且高质量的韩英语数据集进行预训练。本报告详述了我们在构建数据管道和训练模型过程中的一些努力。尽管GECKO的词汇量较小，但其在生成韩语和英语令牌时表现出高效性能。我们在代表性的基准测试上评估了其性能，特别是在韩国MMMLU（韩国多模态多语言理解）任务上表现优异，而在英语和代码方面则显示出适度的能力，尽管其训练的令牌数量少于专注于英语的LLMs。GECKO以宽松的许可协议对开源社区开放，我们希望它能为韩语LLM研究提供研究基线和实用见解。您可以在以下链接找到该模型：https://huggingface.co/kifai/GECKO-7B。|
|**2024-05-23**|**A Nurse is Blue and Elephant is Rugby: Cross Domain Alignment in Large Language Models Reveal Human-like Patterns**|Asaf Yehudai et.al.|[2405.14863](http://arxiv.org/abs/2405.14863)|null|跨领域对齐是指将一个概念从一个领域映射到另一个领域的任务。例如，询问“如果\textit{医生}是一种\textit{颜色}，它会是什么颜色？”这个看似奇特的课题旨在研究人们如何通过类别映射和对这些映射的推理来表征具体和抽象的概念。在这篇论文中，我们借鉴认知科学中的这一任务，通过行为研究评估大型语言模型（LLMs）在概念化和推理能力上的表现。我们通过提示LLMs执行跨域映射任务，并在群体和个体层面分析它们的响应。此外，我们还评估了模型对其预测进行推理的能力，通过分析和分类它们对这些映射的解释。结果显示，人类和模型的映射以及解释存在显著相似性，表明模型以与人类类似的方式表征概念。这种相似性不仅体现在模型的表示上，也体现在它们的行为中。而且，模型大多给出有效的解释，并采用与人类类似的推理路径。|
|**2024-05-23**|**Bitune: Bidirectional Instruction-Tuning**|Dawid J. Kopiczko et.al.|[2405.14862](http://arxiv.org/abs/2405.14862)|null|我们提出了一种名为Bitune的方法，该方法提升了预训练的解码器型大语言模型在指令调优方面的性能，从而在多个下游任务上实现了显著的提升。Bitune通过同时应用自回归和双向注意力到提示上，以获取更精确的查询或指令表示。我们为此引入了两组参数，并采用了参数高效微调技术来处理。这两种特征随后被组合成一个加权平均，其中权重由可训练系数决定，用于生成新的令牌。实验结果表明，Bitune在零样本设置下在常识推理、算术和语言理解任务上表现出色。大量的消融研究验证了每个组件的作用，并显示了该方法对不同PEFT技术的鲁棒性。|
|**2024-05-23**|**PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression**|Vladimir Malinovskii et.al.|[2405.14852](http://arxiv.org/abs/2405.14852)|**[link](https://github.com/vahe1994/aqlm)**|## 背景  对于大型语言模型（LLMs）的“极端”压缩，即将其参数压缩至1-2位每参数，以适应资源受限设备上的高效执行，引起了广泛关注。现有研究主要集中在改进一次性量化技术和权重表示上；然而，纯后训练方法在精度与位宽权衡方面的收益正在减少。当前最先进的量化方法，如QuIP#和AQLM，包含对部分压缩参数的小规模校准数据微调；然而，这些针对压缩权重的微调通常仅使用直通估计器（STE），STE在这种场景下的性能尚不明确。  本工作质疑在极端LLM压缩中使用STE的有效性，并系统地研究了量化感知微调策略。我们提出PV-Tuning，一个无特定架构限制的框架，它扩展并改进了现有的微调策略，并在某些受限情况下提供收敛保证。在实际应用中，当用于1-2位矢量量化时，PV-Tuning在高性能模型如Llama和Mistral上优于先前的技术。通过使用PV-Tuning，我们在2位参数的情况下首次实现了Llama 2家族模型的帕累托最优量化。|
|**2024-05-23**|**HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models**|Bernal Jiménez Gutiérrez et.al.|[2405.14831](http://arxiv.org/abs/2405.14831)|**[link](https://github.com/osu-nlp-group/hipporag)**|为了在恶劣多变的自然环境中生存，哺乳动物的大脑发展出存储大量世界知识并不断整合新信息的能力，同时避免灾难性遗忘。尽管大型语言模型（LLMs）如带有检索增强生成（RAG）的方法在处理此类任务上已取得显著成就，但它们在大规模新经验融合方面仍面临挑战。本研究中，我们提出HippoRAG，一个受人类长期记忆海马回索引理论启发的新型检索框架，旨在促进对新经验的更深、更有效集成。HippoRAG巧妙地协同LLMs、知识图谱以及个性化PageRank算法，模拟人脑皮层和海马体在记忆中的不同作用。  我们将HippoRAG与现有RAG方法在多轮问答任务中进行比较，结果显示HippoRAG显著优于当前最先进的方法，性能提升高达20%。单步检索时，HippoRAG表现出与迭代检索方法如IRCoT相当或更好的性能，同时成本节省10-30倍，速度提升6-13倍。当将HippoRAG融入IRCoT后，还能带来额外的显著增益。最后，我们展示HippoRAG能够应对现有方法难以触及的新场景。代码和数据已在<https://github.com/OSU-NLP-Group/HippoRAG>上开源。|
|**2024-05-23**|**Can LLMs Solve longer Math Word Problems Better?**|Xin Xu et.al.|[2405.14804](http://arxiv.org/abs/2405.14804)|null|### 翻译  数学应用题（MWPs）是衡量大型语言模型（LLMs）能力的关键，但现有研究主要集中在简短背景的题目上。然而，现实生活中的数学问题往往涉及复杂情境，因此LLMs解决长篇数学应用题的能力对于其在实际场景的应用至关重要，但这一方面尚未得到充分探索。本研究首次关注Context Length Generalizability（CoLeG），即LLMs处理长篇数学应用题的能力。我们创建了Extended Grade-School Math（E-GSM）数据集，其中包含带有详细叙述的问题。为此，我们提出了两个新指标来评估LLMs在这类任务上的效能和鲁棒性。  通过对现有零样本提示方法以及商业和开源模型的考察，我们发现它们在CoLeG方面普遍存在不足。针对不同类型的LLMs，我们提出针对性的解决方案：对于专有模型，我们设计了一种新的指导性提示以减轻长文本的影响；对于开源模型，我们开发了一种数据增强任务以提升模型的适应性。我们的全面实验结果显示，我们的方法不仅在E-GSM上表现出色，而且在其他多个数学应用题基准上也展现出良好的泛化能力。  本研究的结果为未来利用LLMs处理复杂现实问题的研究提供了方向，为当前限制提出了实用解决方案，并为进一步探索模型泛化性和训练策略开辟了道路。|
|**2024-05-23**|**Lessons from the Trenches on Reproducible Evaluation of Language Models**|Stella Biderman et.al.|[2405.14782](http://arxiv.org/abs/2405.14782)|null|在自然语言处理（NLP）领域，有效评估语言模型仍然是一项未解的挑战。研究人员和工程师面临诸多方法论难题，例如模型对评估设置的敏感性、不同方法之间的比较困难，以及可重复性和透明度的缺失。本文基于三年的大型语言模型评估经验，为研究者提供指导和教训。首先，我们概述了语言模型评估中常见的问题。其次，我们阐述了应对或减轻这些问题的最佳实践。第三，我们介绍了Language Model Evaluation Harness（lm-eval）：一个开源库，旨在独立、可重复和扩展地评估语言模型，以解决这些问题。我们将介绍库的功能，并通过案例研究展示如何使用该库来缓解这些方法论关注点。|
|**2024-05-23**|**WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models**|Peng Wang et.al.|[2405.14768](http://arxiv.org/abs/2405.14768)|**[link](https://github.com/zjunlp/easyedit)**|**在大型语言模型（LLMs）中，随着世界事实的不断增长和纠正错误响应的需求，模型编辑的方法需要不断更新知识。论文的核心问题是：在编辑过程中，知识应存储在模型的哪个记忆层次更为合适。研究发现，直接修改长期记忆（模型参数）或利用工作记忆（通过检索的神经网络激活）都会导致不可逾越的三角困境——可靠性、泛化能力和局部性无法同时实现于终身编辑场景中。直接修改参数会与无关的预训练知识或先前编辑产生冲突（可靠性差、局部性不足）；而基于检索的工作记忆难以使模型理解并泛化编辑（泛化能力弱）。因此，作者提出了一个名为WISE的新方法，旨在弥合记忆之间的鸿沟。  在WISE中，设计了一种双参数内存机制，包括主内存用于存储预训练知识，侧内存用于存放编辑后的知识。仅对侧内存中的知识进行编辑，并训练一个路由器，以便根据查询决定从哪个内存中获取信息。对于持续编辑，采用了知识切片机制，将不同的编辑分布在参数的不同子空间中，然后合并到共享内存中，以避免冲突。实验结果表明，WISE在问答、幻觉生成和跨不同趋势的LLM架构（如GPT、LLaMA和Mistral）的终身模型编辑任务中表现出色，超越了先前的模型编辑方法，成功克服了上述困境。代码将在https://github.com/zjunlp/EasyEdit上发布。**|
|**2024-05-23**|**FinRobot: An Open-Source AI Agent Platform for Financial Applications using Large Language Models**|Hongyang Yang et.al.|[2405.14767](http://arxiv.org/abs/2405.14767)|**[link](https://github.com/ai4finance-foundation/finrobot)**|**随着金融机构和专业人士越来越多地将大型语言模型（LLMs）融入工作流程，金融行业与AI社区之间仍存在显著障碍，如专有数据和专业知识。这些挑战限制了AI在提升金融任务效率方面的潜力。鉴于金融分析的重要性，我们旨在开发专门针对金融的LLM驱动工具链，并通过开源项目推动其普及，促进AI在金融决策中的广泛应用。本文介绍FinRobot，一个创新的开源AI代理平台，支持多个金融专业AI代理，每个都由LLM驱动。平台主要分为四层：1）金融AI代理层，通过构建金融Chain-of-Thought（CoT）将复杂的金融问题分解为逻辑序列；2）金融LLM算法层，根据特定任务动态配置合适的模型应用策略；3）LLMOps和DataOps层，通过训练/微调技术以及使用与任务相关的数据生成精确模型；4）多源LLM基础模型层，整合各种LLM，使上述各层可以直接访问。FinRobot旨在为专业分析师和非专业人士提供实践操作，让他们能够利用强大的AI技术进行高级金融分析。FinRobot的开源代码可在此获取：\url{https://github.com/AI4Finance-Foundation/FinRobot}。**|
|**2024-05-23**|**Evaluating Large Language Models for Public Health Classification and Extraction Tasks**|Joshua Harris et.al.|[2405.14766](http://arxiv.org/abs/2405.14766)|null|随着大型语言模型（LLMs）的快速发展，人们对其在公共卫生领域支持专家工作的潜力产生了浓厚兴趣。本研究通过结合六个外部标注的和七个内部标注的数据集，评估了LLMs在处理与健康负担、流行病学风险因素和公共卫生干预相关的文本分类和提取任务上的性能。我们首先对五个开源大模型（参数量从7亿到70亿不等）进行了零样本的上下文学习测试。结果显示，Llama-3-70B-Instruct表现出色，微-F1得分在17个任务中的15项中最高。各任务间的性能差异显著，例如，有些模型如Contact Classification的得分低于60%，而像GI疾病分类这样的任务，所有模型都能达到80%以上的微-F1。对于12个任务的子集，我们还评估了GPT-4，发现其与Llama-3-70B-Instruct的结果相当，Llama-3-70B-Instruct在其中6个任务上得分更高或持平。总体而言，根据初步结果，我们发现LLMs有可能成为公共卫生专家从各种自由文本源提取信息的有效工具，有助于公共卫生监测、研究和干预措施。|
|**2024-05-23**|**Large language models can be zero-shot anomaly detectors for time series?**|Sarah Alnegheimish et.al.|[2405.14755](http://arxiv.org/abs/2405.14755)|null|近期的研究表明，大型语言模型能够执行多种任务，包括时间序列预测。这些模型的灵活性使其适用于众多应用。本文提出一项新颖的研究，探讨大型语言模型在复杂的时间序列异常检测任务中的性能。对于语言模型而言，这涉及识别输入序列（或多个部分）中的异常点，以及处理时间序列数据而非传统的文本输入。我们介绍了sigllm，一个专为时间序列异常检测设计的大型语言模型框架。该框架包含将时间序列转换为文本的模块，以及端到端的流程，用于引导语言模型进行异常检测。我们试验了两种测试大型语言模型能力的方法：一是直接提示模型指出输入中的异常元素；二是利用语言模型的预测能力来辅助检测过程。  我们在11个来自不同来源的数据集上评估了我们的框架，使用了10种不同的管道。结果显示，预测方法在所有11个数据集中都显著优于提示方法，尤其是在F1分数上。尽管大型语言模型能够发现异常，但目前的深度学习模型在性能上仍占优，其表现比大型语言模型高出30%。|
|**2024-05-21**|**Reducing Transformer Key-Value Cache Size with Cross-Layer Attention**|William Brandon et.al.|[2405.12981](http://arxiv.org/abs/2405.12981)|null|## 翻译  键值缓存对于加速Transformer架构的自回归大型语言模型（LLMs）的解码至关重要。然而，随着序列长度增加和批量大小增大，存储键值缓存所需的内存可能会变得难以承受。自从Transformer诞生以来，两个最有效的内存减小策略是多查询注意力（MQA）及其推广，群组查询注意力（GQA）。MQA和GQA通过让多个查询头共享单个键/值头，显著减少了不同键/值头的数量，同时对准确性影响较小。本文展示了如何进一步发展MQA，即在相邻层之间也共享键和值头，我们将其称为跨层注意力（CLA）。实验表明，使用CLA，可以在保持接近原始MQA精度的同时，将键值缓存的大小再减少2倍。我们在从头训练10亿参数和30亿参数模型的实验中验证了这一点，结果表明，CLA在内存与准确性之间的权衡上提供了优于传统MQA的帕累托改进，使得更长的序列长度和更大的批量大小下的推理成为可能。|
|**2024-05-21**|**Energy Rank Alignment: Using Preference Optimization to Search Chemical Space at Scale**|Shriram Chennakesavalu et.al.|[2405.12961](http://arxiv.org/abs/2405.12961)|**[link](https://github.com/rotskoff-group/llm-era)**|在化学空间中的搜索是一个极具挑战性的问题，因为可能的分子数量随着原子数量呈组合级增长。大型自回归模型通过学习化学化合物数据库已经产生了强大的生成器，但我们仍然缺乏有效策略来生成具有特定性质的分子。这个问题与大型语言模型的“对齐”问题相似，尽管在许多化学任务中，我们有一个明确且易于评估的奖励函数。本文介绍了一种名为能量排名对齐（ERA）的算法，它利用明确的奖励函数构建了一个梯度优化目标，用于调整自回归策略。理论上，我们发现该算法与Proximal Policy Optimization（PPO）和Direct Preference Optimization（DPO）密切相关，但其最小化器收敛于一个理想的吉布斯-玻尔兹曼分布，奖励函数扮演了能量角色。此外，该算法具有高度可扩展性，无需强化学习，并且在每对样本的偏好观察次数较少时，相对于DPO表现出色。  我们将这种方法应用于分子变压器的对齐，以生成具有外部指定属性的分子，并发现它能稳健地进行搜索，探索化学空间的多样化部分。虽然我们的重点在于化学搜索，但我们在一个AI监督的任务上也取得了优秀结果，表明该方法是可扩展且通用的。|
|**2024-05-21**|**Aggregation of Reasoning: A Hierarchical Framework for Enhancing Answer Selection in Large Language Models**|Zhangyue Yin et.al.|[2405.12939](http://arxiv.org/abs/2405.12939)|**[link](https://github.com/yinzhangyue/AoR)**|## 背景 近期，Chain-of-Thought提示的进展极大地推动了大型语言模型（LLMs）在复杂推理任务中的突破。当前研究通过采样多种推理路径并根据答案频率进行ensemble，提高了LLMs的推理性能。然而，这种方法在正确答案处于少数的情况时失效。我们发现这是制约LLMs推理能力的关键因素，仅凭预测答案无法解决这个问题。为此，我们提出了一个层次化的推理聚合框架AoR（推理聚合），它依据推理链条的评估来选择答案。此外，AoR引入了动态采样策略，根据任务复杂度调整推理链条的数量。  ## 任务 一系列复杂推理任务的实验结果显示，AoR相较于主流ensemble方法表现出色。进一步分析表明，AoR不仅适用于各种LLMs，而且在与现有方法的性能天花板比较中，达到了更优秀的水平。|
|**2024-05-21**|**Skin-in-the-Game: Decision Making via Multi-Stakeholder Alignment in LLMs**|Bilgehan Sel et.al.|[2405.12933](http://arxiv.org/abs/2405.12933)|null|大型语言模型在诸如总结、算术推理和问答等任务上表现出色。然而，在道德推理和伦理决策方面，尤其是在涉及多个利益相关者的复杂情景中，它们面临严峻挑战。本文提出了一种名为Skin-in-the-Game（SKIG）的框架，旨在通过从不同利益相关者角度审视决策的后果，提升语言模型在道德推理中的能力。SKIG的核心机制是模拟行动的责任感，结合同理心练习和风险评估，对提高其有效性至关重要。我们使用专有和开源语言模型在各种道德推理基准上验证SKIG的表现，并通过深入的消融分析探究其关键组件。|
|**2024-05-21**|**Code-mixed Sentiment and Hate-speech Prediction**|Anjali Yadav et.al.|[2405.12929](http://arxiv.org/abs/2405.12929)|null|在多语言环境中，混合代码（code-mixed discourse）指的是单文本中融合多种语言的现象，尤其是在官方语言多元的国家的非正式交流中常见。随着大型语言模型在自然语言处理任务中的主导地位提升，我们针对代码混合语境的研究也随之展开。首先，我们特别设计了四款新的英语-印地语和英语-斯洛文尼亚双语预训练遮罩语言模型，以适应非正式语言。接着，我们对各种类型的模型——包括单语、双语、少量语言和大规模多语言模型——在社交媒体文本的情感分析和攻击性语言检测等任务上的性能进行了评估。结果显示，最有效的分类器是针对社交媒体文本的专业化双语和多语言模型，随后是非专业的大规模多语言和单语模型，而大型生成模型的表现并不突出。对于涉及情感的问题，模型在处理代码混合数据时总体上略优于非代码混合数据。|
|**2024-05-21**|**Streamlining Software Reviews: Efficient Predictive Modeling with Minimal Examples**|Tim Menzies et.al.|[2405.12920](http://arxiv.org/abs/2405.12920)|**[link](https://github.com/timm/ez)**|该论文提出了一项新的软件分析挑战任务。在这个被称为“软件审查”的过程中，一组SME（主题专家）会评审软件行为示例，以建议如何改进软件的运行。由于SME的时间通常非常有限，理想的状况是，该团队仅通过查看少量具有高度信息价值的示例就能完成优化任务。为了支持这个审查过程，研究探索了训练预测模型的方法，该模型能够预测某个专家是否会喜欢或不喜欢下一个示例。这种预测模型可以与SME合作，引导他们探索所有示例，同时在专家离开后，模型也可以作为代理，处理新出现的案例，以应对专家们的忙碌。  在31个案例研究中（涵盖了从软件流程的高层决策到视频编码软件配置的低层决策），我们展示了仅使用12到30个标签就能建立这样的预测模型。据我们所知，仅凭少数示例（不依赖大型语言模型）就能取得这样的成果，在当前尚属罕见。遵循开放科学的原则，我们将在<https://github.com/timm/ez/tree/Stable-EMSE-paper>提供所有的代码和数据，以便他人能复制、验证或在此基础上进一步改进这些结果。|
|**2024-05-21**|**G-DIG: Towards Gradient-based DIverse and hiGh-quality Instruction Data Selection for Machine Translation**|Xingyuan Pan et.al.|[2405.12915](http://arxiv.org/abs/2405.12915)|null|大型语言模型（LLMs）在通用场景中展现出显著能力，通过指令微调，它们能够与人类在多种任务上协同。然而，指令数据的多样性和质量是指令微调面临的两大挑战。为此，本论文提出了一种新颖的基于梯度的方法，用于自动选择机器翻译中的高质量和多样化的指令微调数据。我们的核心创新在于分析单个训练样例如何在训练过程中影响模型。通过结合影响力函数和一小部分高质量种子数据，我们选择对模型产生积极影响的样例作为高质量数据。此外，为了增加数据多样性，我们通过聚类其梯度并重采样，最大化它们对模型产生的影响多样性。在WMT22和FLORES翻译任务上的广泛实验验证了我们方法的优越性，深入分析进一步证实了其效果和泛化能力。|
|**2024-05-21**|**An Empirical Study and Analysis of Text-to-Image Generation Using Large Language Model-Powered Textual Representation**|Zhiyu Tan et.al.|[2405.12914](http://arxiv.org/abs/2405.12914)|**[link](https://github.com/llm-conditioned-diffusion/llm-conditioned-diffusion.github.io)**|一个关键的先决条件是准确理解文本输入，这对于忠实的文本到图像生成至关重要。现有的方法利用CLIP模型的文本编码器来表示提示。然而，预训练的CLIP模型仅能处理英文，且其文本编码器的模型容量相对有限。相比之下，大型语言模型（LLMs）支持多语言输入，能够处理更长的上下文，并提供更优秀的文本表示。本文研究了使用LLMs作为文本编码器以提升文本到图像生成中的语言理解能力。然而，从头开始训练包含LLMs的文本到图像生成模型需要大量的计算资源和数据。  为此，我们提出了一种三阶段训练流程，有效地整合现有文本到图像模型与LLMs，同时保持高效的训练。特别地，我们设计了一个轻量级适配器，使得能够快速使用LLMs生成的文本表示来训练文本到图像模型。大量的实验表明，我们的模型不仅支持多语言输入，还能处理更长的上下文，而且在图像生成质量上表现出色。|
|**2024-05-21**|**Topic Modelling Case Law Using a Large Language Model and a New Taxonomy for UK Law: AI Insights into Summary Judgment**|Holli Sargeant et.al.|[2405.12910](http://arxiv.org/abs/2405.12910)|**[link](https://github.com/AhmedIzzidien/TopicLLM)**|**该论文关注法律分析中的一个重要空白，通过构建和应用一种新颖的判例主题分类法，对英国的简易判决案件进行了探索。利用精心挑选的简易判决案例数据集，我们利用大型语言模型Claude 3 Opus研究功能性话题和趋势。结果显示，Claude 3 Opus在主题分类上的准确率为87.10%，揭示了不同法律领域中简易判决的明显模式。由于英国的判例法并未原始标注关键词或提供主题过滤选项，这项研究不仅深化了我们对简易判决主题本质的理解，还展示了传统方法与人工智能驱动分类方法结合的可能性。因此，本文提供了英国法律的新通用分类框架。这项工作的意义为司法行政领域的进一步研究和计算法学研究方法论讨论奠定了基础。**|
|**2024-05-21**|**Adversarial DPO: Harnessing Harmful Data for Reducing Toxicity with Minimal Impact on Coherence and Evasiveness in Dialogue Agents**|San Kim et.al.|[2405.12900](http://arxiv.org/abs/2405.12900)|null|近期，大规模语言模型（LLMs）和各种有效的训练方法的兴起推动了开放领域对话系统的发展。然而，这些模型中的毒性问题对用户体验构成重大挑战。本文提出了一种创新的训练算法——对抗式直接偏好优化（ADPO），它是在直接偏好优化（DPO）的基础上改进的。ADPO旨在训练模型增加对优选回复的概率分布，同时降低对使用有毒控制令牌生成的不安全回复的概率。研究显示，ADPO能够增强模型抵御有害对话的能力，同时尽量减少性能下降。此外，我们证明ADPO提供了比传统DPO更为稳定的训练流程。据我们所知，这是首次将有害数据直接融入生成模型的DPO变体，从而减少了人工创建安全对话数据的需求。|
|**2024-05-20**|**Adapting Large Multimodal Models to Distribution Shifts: The Role of In-Context Learning**|Guanglin Zhou et.al.|[2405.12217](http://arxiv.org/abs/2405.12217)|**[link](https://github.com/jameszhou-gl/icl-distribution-shift)**|**近期的研究表明，大型多模态模型（LMMs）在应对自然分布变化时表现出极高的鲁棒性，常常超越先前的基准。然而，领域特定的适应仍然是必要的，尤其是在医疗等专业领域。鉴于LMMs庞大的参数空间使其微调不切实际，本研究聚焦于探索上下文学习（ICL）作为一种增强LMM适应性的有效方法。我们发现，ICL的成功在很大程度上依赖于示例的选择，这与大型语言模型类似，但对面临分布变化的LMMs提出了独特挑战。为此，我们评估了一种无监督的ICL方法——TopKNearestPR，该方法通过特征相似性进行最近示例搜索来选择示例。研究揭示了这种方法在处理分布转移场景下的视觉编码器缺陷对其效果的限制。  为解决这些问题，我们提出了一种新颖的方法——InvariantSelectPR，它利用类条件对比不变性（CCI）来提升预训练视觉编码器的稳健性。CCI通过增强不同类别间的区分度并确保对领域特定变化的不变性，提高了编码器识别和检索最有信息价值示例的能力。这种方法有助于引导LMM适应新的查询样本，即使在不同的分布下也是如此。实验结果显示，InvariantSelectPR显著提高了LMM的适应性，在Camelyon17和HAM10000基准数据集上的7-shot任务中，分别实现了34.2%和16.9%的准确率提升，相对于零-shot性能，这是显著的进步。**|
|**2024-05-20**|**MathBench: Evaluating the Theory and Application Proficiency of LLMs with a Hierarchical Mathematics Benchmark**|Hongwei Liu et.al.|[2405.12209](http://arxiv.org/abs/2405.12209)|**[link](https://github.com/open-compass/mathbench)**|**随着大型语言模型（LLMs）的最新进展在数学领域取得了显著进步，传统的数学基准如GSM8k在全面评价这些模型的数学能力方面存在局限。为了弥补这一不足，我们提出了MathBench，这是一个全新基准，旨在严格评估大型语言模型的数学能力。MathBench覆盖广泛的数学学科，对理论理解和实际问题解决能力进行详尽评估。它分为五个阶段，从基础算术到大学数学，结构上设计用于考察模型在不同深度知识的理解。每个阶段包括理论问题和应用题，以衡量模型的数学熟练度及其在实际情境中应用概念的能力。MathBench的目标是提升对LLMs数学能力的评价，提供对其知识理解水平和问题解决技能的细致视角，同时支持双语环境。该项目已发布在https://github.com/open-compass/MathBench。**|
|**2024-05-20**|**Developers' Perceptions on the Impact of ChatGPT in Software Development: A Survey**|Thiago S. Vaillant et.al.|[2405.12195](http://arxiv.org/abs/2405.12195)|**[link](https://github.com/gpt-impact/Paper-content)**|随着大型语言模型（如ChatGPT）的不断发展，其强大的自然语言处理能力和广泛应用引起了广泛关注。尽管人工智能（AI）与软件工程（SE）的融合趋势日益明显，但关于这种融合如何影响软件开发实践和认知的研究仍显不足。为了揭示将AI驱动工具，如ChatGPT，融入软件开发过程的影响和挑战，我们进行了一项调查，针对207名软件开发者进行了研究。调查内容包括ChatGPT对软件质量、生产力以及开发者工作满意度的影响，同时还探讨了他们对未来ChatGPT应用的预期、对可能的工作岗位替代的担忧，以及对监管措施的看法。|
|**2024-05-20**|**CT-Eval: Benchmarking Chinese Text-to-Table Performance in Large Language Models**|Haoxiang Shi et.al.|[2405.12174](http://arxiv.org/abs/2405.12174)|null|该论文介绍了一个名为CT-Eval的中文文本转表格数据集，旨在衡量大语言模型在非英语语言环境下的文本转表格任务性能。由于现有英文文本转表格数据集主要面向英语，CT-Eval填补了这一空白，选择了一种流行的多学科中文在线百科作为来源，涵盖了28个领域以保证数据多样性。为了减少数据虚构（hallucination）问题，研究者首先训练了一个语言模型来识别并过滤掉存在虚构问题的样本，然后人工标注验证集和测试集中的错误。最终，CT-Eval包含了大约88,600个任务样本。通过CT-Eval，研究者评估了开源和闭源大语言模型（如GPT-4）的表现，结果显示零-shot模式下这些模型与人类判断仍有显著差距。经过微调后，开源模型在文本转表格能力上有了显著提升，大幅超越了GPT-4。总之，CT-Eval不仅为评估和理解现有大语言模型的中文文本转表格能力提供了有价值的工具，也为提升这类模型在这项任务上的性能提供了宝贵资源。|
|**2024-05-20**|**Fennec: Fine-grained Language Model Evaluation and Correction Extended through Branching and Bridging**|Xiaobo Liang et.al.|[2405.12163](http://arxiv.org/abs/2405.12163)|**[link](https://github.com/dropreg/fennec)**|**随着大型语言模型的迅速发展，它们在众多现实任务中的应用日益广泛，主要目标是符合人类的意图。然而，理解人类意图的复杂性使得依赖于耗时的人工评估成为必要。为了缓解这一问题，我们探讨了利用开源大型语言模型作为评估者的趋势，特别是在GPT-4的流行背景下。我们提出了一种名为\textbf{Fennec}的框架，专注于\textbf{F}ine-grained \textbf{E}valuation（细致评估）和\textbf{N}eeded \textbf{E}xtension（必要扩展）通过分支（Branching）和连接（Bridging）。分支操作将评估任务分解为不同维度和粒度，从而减轻评估挑战。同时，连接操作融合了多样化的训练数据集，增加了评估任务的多样性。实验结果显示，我们的7B模型在各种常用基准上的\textit{一致性}和\textit{一致同意}性能均优于开源的更大规模评估模型，接近GPT-4的表现。我们利用模型的精细校正功能改进多个模型响应，结果显示，这种优化提升了响应质量，在MT-Bench上提高了1-2分。我们的代码已在GitHub上开源\footnote{\url{https://github.com/dropreg/Fennec}}。**|
|**2024-05-20**|**Eliciting Problem Specifications via Large Language Models**|Robert E. Wray et.al.|[2405.12147](http://arxiv.org/abs/2405.12147)|null|这篇论文探讨了如何利用大型语言模型（LLMs）在认知系统中实现问题定义的转化。通常情况下，人类需要将问题描述转化为认知系统能理解的形式。研究者展示了LLMs能够处理自然语言中定义的问题类别，并将其转换为半形式化规格，这样现有推理和学习系统可以解决这类问题的具体实例。他们设计了一种由LLM驱动的认知任务分析师代理，这种系统能够根据自然语言描述的任务生成问题空间的定义。LLM提示源自人工智能文献中的问题空间概念和通用问题解决策略（如波利亚的《如何解决问题》）。随后，认知系统利用这些问题空间规格，结合领域通用的解决问题策略（如搜索），来解决该类问题的不同实例。这一初步结果表明，通过消除问题表述的中介过程，LLMs有可能加速认知系统的研究，同时保持其核心能力，如稳健的推理和在线学习。|
|**2024-05-20**|**MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning**|Ting Jiang et.al.|[2405.12130](http://arxiv.org/abs/2405.12130)|**[link](https://github.com/kongds/mora)**|**低秩适应是大型语言模型中流行的参数高效微调方法。在这篇论文中，我们研究了低秩更新（如LoRA实现）的影响。我们的发现指出，这种机制可能限制了大语言模型学习和记忆新知识的能力。受此启发，我们提出了一种新的方法MoRA，它利用平方矩阵实现高秩更新，同时保持与LoRA相同的可训练参数数量。为此，我们引入了相应的非参数运算器，以降低输入维度并增加输出维度处理平方矩阵。这些运算器确保权重能无缝融入到大语言模型中，使得我们的方法能够像LoRA一样部署。我们在五个任务上进行了全面评估：指令调整、数学推理、连续预训练、记忆以及预训练。在内存密集型任务上，我们的方法优于LoRA，并在其他任务上表现出相当的性能。**|
|**2024-05-20**|**Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation**|Zhankui He et.al.|[2405.12119](http://arxiv.org/abs/2405.12119)|null|大型语言模型（LLMs）正在通过出色地索引项目内容、理解复杂的对话上下文并生成相关项目标题，革新了对话推荐系统。然而，控制推荐项目的分布仍是一个挑战，导致在针对对话推荐平台的快速变化的数据分布，如项目流行度上，性能欠佳。在对话推荐中，LLMs通过自回归方式生成项目标题（作为多个令牌），这使得获取和控制所有项目推荐变得困难。因此，我们提出了一种名为“重索引-然后适应”（Reindex-Then-Adapt，RTA）的框架，它将多令牌项目标题转换为单个令牌于LLMs内，随后调整这些单令牌项目标题的概率分布。RTA框架结合了LLMs理解和复杂查询的优势，以及传统推荐系统（RecSys）在对话推荐中有效控制推荐项目分布的能力。实验结果表明，我们的框架在三个不同的对话推荐数据集和两种适应设置下，展示了改进的准确性指标。|
|**2024-05-20**|**Imp: Highly Capable Large Multimodal Models for Mobile Devices**|Zhenwei Shao et.al.|[2405.12107](http://arxiv.org/abs/2405.12107)|**[link](https://github.com/milvlg/imp)**|**尽管大型语言模型（LLMs）和大型多模态模型（LMMs）在开放世界多模态理解方面展现出惊人的能力，但它们通常参数量大、计算需求高，限制了在资源受限环境中的应用。为了应对这一问题，研究人员已经提出了一系列轻量级LMM，旨在在有限规模（如30亿参数）下最大化性能。然而，这些方法多数仅关注设计空间的单一或两个方面，对影响模型能力的关键设计选择尚未进行全面探讨。  本文系统地研究了轻量级LMM的设计，包括模型架构、训练策略和训练数据。根据我们的研究结果，我们构建了一套名为Imp的高性能LMM家族，覆盖20亿到40亿参数规模。尤其值得注意的是，我们的Imp-30亿模型在与同类规模的现有轻量级模型相比时持续领先，并超越了130亿参数规模的最新LMM状态。通过低精度量化和分辨率降低技术，Imp模型能够在高通骁龙8Gen3移动芯片上实现高速部署，每秒处理大约13个令牌的推理速度。**|
|**2024-05-20**|**DOP: Diagnostic-Oriented Prompting for Large Language Models in Mathematical Correction**|Hao Chen et.al.|[2405.12100](http://arxiv.org/abs/2405.12100)|null|## 背景 数学世界问题修正（MWPC）是一个专门针对解决数学问题过程中错误推理的修正任务。本文利用大语言模型（LLMs）的进步，关注两点：（1）区分数学推理与错误修正；（2）探索策略以提升LLMs在数学领域的错误修正能力，以应对MWPC任务。我们注意到，在实时教育中，帮助学生识别错误比单纯提供正确答案更为关键。然而，当前研究往往侧重于获取精确的解题答案，而非纠正可能的错误。因此，我们调整了研究范式，表明提升数学推理能力并不等同于精通错误修正。同时，我们提出了一种名为诊断导向提示（DOP）的新方法，旨在促进LLMs在错误修正方面表现出色。实验结果显示，DOP表现出卓越性能，彰显其重要性。我们强调，在数学教育中，对出色修正者的需要超过了对熟练推理者的追求。代码和数据可在<https://github.com/ChenhaoEcnuCS/Reason-Correct>获取。|
|**2024-05-17**|**A Survey on Large Language Models with Multilingualism: Recent Advances and New Frontiers**|Kaiyu Huang et.al.|[2405.10936](http://arxiv.org/abs/2405.10936)|**[link](https://github.com/kaiyuhwang/mllm-survey)**|**随着大型语言模型（LLMs）的快速发展，在自然语言处理领域展现出显著的多语言能力，引起了学术界和业界的广泛关注。为了减少潜在的歧视并提升技术的通用性和可访问性，对于多语言技术的发展至关重要。尽管LLMs取得了突破，但对多语言场景的深入研究仍显不足。因此，迫切需要一份全面的综述，总结近期的方法、进展、局限性和可能的解决方案。本文旨在从多个角度审视LLMs在多语言环境中的应用。我们首先回顾了预训练语言模型研究的历史演变。接着，我们探讨了LLMs的多语言特性，包括训练和推理方法、模型安全、跨领域与文化适应以及数据集使用。我们还分析了这些方面面临的挑战，并提出可能的解决策略。此外，我们指出了未来的研究方向，以进一步提升LLMs的多语言性能。本综述旨在帮助研究界应对多语言问题，提供一个关于基于LLMs的多语言自然语言处理核心概念、关键技术及最新进展的全面理解。**|
|**2024-05-17**|**The Local Interaction Basis: Identifying Computationally-Relevant and Sparsely Interacting Features in Neural Networks**|Lucius Bushnaq et.al.|[2405.10928](http://arxiv.org/abs/2405.10928)|**[link](https://github.com/apolloresearch/rib)**|### 概述  机械解释性目标是通过逆向工程理解神经网络的行为。然而，现有方法在解析神经网络激活方面面临挑战，因为缺乏对激活的分解，使得单个神经元或模型组件无法清晰对应于独特的特征或功能。为此，我们提出了一种新颖的可解释性方法——局部交互基（Local Interaction Basis，LIB）。LIB旨在通过消除无关激活和交互，识别计算特征。该方法摒弃无意义的激活方向，并使基础与相邻层间雅可比矩阵的奇异向量对齐。同时，它根据特征对后续计算的重要性进行缩放，生成一个显示模型中所有计算相关特性和交互的图谱。  我们在模块加法和CIFAR-10模型上评估了LIB的有效性，结果表明，相比于主成分分析，LIB能识别出更多计算相关的特征，并呈现出更稀疏的交互。然而，在应用于语言模型时，LIB并未显著提高可解释性或交互稀疏度。因此，我们得出结论，尽管LIB是一种有前景的理论驱动方法，但当前形式并不适用于大型语言模型。|
|**2024-05-17**|**COGNET-MD, an evaluation framework and dataset for Large Language Model benchmarks in the medical domain**|Dimitrios P. Panagoulias et.al.|[2405.10893](http://arxiv.org/abs/2405.10893)|null|这篇技术论文阐述了COGNET-MD，一个专为医疗领域设计的大型语言模型评估的新基准。我们提出了一种评分框架，旨在评估语言模型理解医学文本的能力，并且设计了一系列难度分级的多项选择题（MCQ）数据库。这个数据库由多个医疗领域的专家合作创建，以反映当前医学趋势，确保安全、实用和适用性。初期版本包含了精神科、牙科、肺病学、皮肤科和内分泌学等领域的题目，但会持续扩展，未来还会加入更多医学学科。|
|**2024-05-17**|**Application of Artificial Intelligence in Schizophrenia Rehabilitation Management: Systematic Literature Review**|Hongyi Yang et.al.|[2405.10883](http://arxiv.org/abs/2405.10883)|null|该综述旨在系统地评估人工智能（AI）在精神分裂症患者康复管理中的现状和前景，以及其对康复过程的影响。我们从2012年至现在筛选了70项研究，重点关注机器学习、深度学习、强化学习等技术在心理健康干预和管理中的应用、技术类别、产品和数据类型，如生态瞬时评估、行为和语音数据的分析。结果显示，AI在症状监测、复发风险预测和康复治疗中具有广泛的应用潜力。此外，本研究还探讨了基于AI的新兴产品、技术和分析方法，如社交媒体分析、严肃游戏和大型语言模型在康复中的潜在挑战和未来发展方向。总的来说，这篇论文系统回顾了AI在精神分裂症康复管理中的应用，并为未来的研究路径提供了有价值的见解和建议。|
|**2024-05-17**|**The Future of Large Language Model Pre-training is Federated**|Lorenzo Sani et.al.|[2405.10853](http://arxiv.org/abs/2405.10853)|null|## 背景  生成式预训练大型语言模型（LLMs）因其在众多任务上的出色表现而备受瞩目，这得益于它们所接受的海量训练数据。根据已建立的规模法则，LLMs未来性能的提升在很大程度上依赖于我们能够利用的计算和数据资源。联邦学习（FL）有可能释放全球大部分未充分利用的数据和计算能力，这些是当前以数据中心为中心的LLM训练方法所忽视的。本文提出了一种稳健、灵活且可复现的FL方法，旨在促进机构间的大规模协作，共同训练LLMs，从而动员更多的计算和数据资源，甚至可能达到或超越中心化的性能。  ## 任务  我们的工作展示了一种FL训练方法，它能够在有限资源下扩展到百亿元级的联邦LLM，使得拥有丰富数据的实体能够成为预训练LLMs的主导力量，而不是仅让计算资源丰富的机构独占鳌头。这种方法强调了联邦训练的规模效益，并为实现这一目标提供了一种实用路径。|
|**2024-05-17**|**Large Language Model (LLM) for Telecommunications: A Comprehensive Survey on Principles, Key Techniques, and Opportunities**|Hao Zhou et.al.|[2405.10825](http://arxiv.org/abs/2405.10825)|null|随着大型语言模型（LLMs）因其卓越的理解和推理能力而备受瞩目，它们在各个领域取得了显著进步，尤其在第六代（6G）通信技术的推动下展现出人工智能通用性（AGI）的潜力。本研究旨在全面概述LLM赋能的电信网络。首先，我们概述了LLMs的基础，包括模型架构、预训练、微调、推理与应用、模型评估，以及在电信部署中的运用。接着，我们将探讨LLM支持的关键技术和电信应用，涉及生成、分类、优化和预测问题。生成应用包括电信领域知识、代码和网络配置自动生成。基于LLM的分类任务涵盖网络安全、文本、图像和流量分类。此外，我们介绍了利用LLMs的自动化优化技术，如强化学习的奖励函数设计和口语强化学习。对于预测问题，LLMs可用于时间序列预测和多模态电信预测。最后，我们指出了LLM赋能电信网络所面临的挑战，并展望了未来的研究方向。|
|**2024-05-17**|**ActiveLLM: Large Language Model-based Active Learning for Textual Few-Shot Scenarios**|Markus Bayer et.al.|[2405.10808](http://arxiv.org/abs/2405.10808)|null|主动学习旨在通过优先处理最能提升学习效果的实例来减少标注工作量。然而，许多主动学习策略面临“冷启动”问题，即在初期需要大量数据才能发挥效能，这限制了它们在预训练模型（如BERT）上的应用，这些模型在少量样本情况下已表现良好。为此，我们提出了一种新颖的主动学习方法——ActiveLLM，它利用大型语言模型（如GPT-4、Llama 3和Mistral Large）进行实例选择。实验证明，ActiveLLM显著提高了BERT分类器在少量样本情况下的性能，超越了传统主动学习方法和SetFit等少数样本学习方法。此外，ActiveLLM还能扩展到非少量样本场景，支持迭代选择，从而帮助其他主动学习策略克服冷启动难题。结果表明，ActiveLLM为改善不同学习环境中的模型性能提供了有前景的解决方案。|
|**2024-05-17**|**Empowering Small-Scale Knowledge Graphs: A Strategy of Leveraging General-Purpose Knowledge Graphs for Enriched Embeddings**|Albert Sawczyn et.al.|[2405.10745](http://arxiv.org/abs/2405.10745)|null|### 翻译  知识密集型任务对机器学习（ML）技术提出了严峻挑战。通常采用的方法，如大型语言模型（LLMs），在处理这类任务时往往存在局限性。然而，人们已经努力通过知识图谱（KG）来弥补这些不足，尤其是通过将小规模的领域特定KG与通用KG相结合。尽管KG在知识表示方面具有优势，但构建它们的成本可能阻碍了广泛的研究和应用。为此，我们提出了一种框架，旨在通过链接到大规模通用KG来提升小型领域特定KG嵌入的学习性能。实验结果显示，这种方法带来了显著的提升，例如，Hits@10指标最高提高了44%。这一相对未被充分探索的研究方向有望促进KG在知识密集型任务中的更频繁运用，从而产生更为稳健、可靠的ML解决方案，它们相较于流行但易出错的LLM方法更具可靠性。关键词：知识图谱、知识图谱补全、实体对齐、表示学习、机器学习|
|**2024-05-17**|**Efficient Multimodal Large Language Models: A Survey**|Yizhang Jin et.al.|[2405.10739](http://arxiv.org/abs/2405.10739)|**[link](https://github.com/lijiannuist/efficient-multimodal-llms-survey)**|**在过去一年里，多模态大型语言模型（Multimodal Large Language Models，MLLMs）在诸如视觉问答、视觉理解和推理等任务上展现出卓越性能。然而，这些模型的庞大规模和高昂的训练与推理成本限制了它们在学术界和工业界的广泛应用。因此，研究高效且轻量级的MLLM具有巨大的潜力，特别是在边缘计算环境中。本综述全面系统地回顾了当前高效MLLM的研究现状。我们概述了代表性高效模型的发展历程，总结了有效结构和策略的研究状态，以及其实用应用。最后，我们讨论了当前高效MLLM研究的局限，并展望了有前景的未来发展方向。如需更多信息，请参考我们的GitHub仓库：https://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey。**|
|**2024-05-17**|**INDUS: Effective and Efficient Language Models for Scientific Applications**|Bishwaranjan Bhattacharjee et.al.|[2405.10725](http://arxiv.org/abs/2405.10725)|null|大型通用语言模型在自然语言处理任务上表现出色。然而，先前的研究表明，针对特定领域的训练数据可以使模型在专业任务上表现更佳。为此，我们开发了INDUS，一套专为地球科学、生物学、物理学、太阳物理、行星科学和天文学领域设计的定制化语言模型。这些模型基于精心挑选的科学语料库，包括：（1）一个使用领域专用词汇和数据集训练的编码器，用于提升自然语言理解任务的表现；（2）一个基于对比学习的通用文本嵌入模型，利用多源数据集进行训练，以优化信息检索任务；（3）通过知识蒸馏技术缩小规模的模型，适用于对延迟和资源有限的应用。此外，我们创建了三个新的科学基准数据集：CLIMATE-CHANGE-NER（实体识别）、NASA-QA（抽取式问答）和NASA-IR（信息检索），以推动跨学科领域的研究进展。最后，实验结果显示，我们的模型在新任务和相关领域现有基准任务上均优于通用编码器（如RoBERTa）和现有的领域特定编码器（如SciBERT）。|
|**2024-05-16**|**UniRAG: Universal Retrieval Augmentation for Multi-Modal Large Language Models**|Sahel Sharifymoghaddam et.al.|[2405.10311](http://arxiv.org/abs/2405.10311)|null|## 背景  近期，多模态（MM）大型语言模型（LLMs）已经解锁了许多需要多模态理解（如图像描述或视觉问答）和生成（如文本引导的图像生成或编辑）复杂任务。为了进一步提升MM-LLMs的输出质量，我们提出了一种模型通用的UniRAG技术，它在推理阶段将相关检索信息添加到提示中，作为少量样例。与普遍认为检索增强（RA）主要改进罕见实体的生成或理解不同，我们在MSCOCO数据集上对包括GPT4、Gemini-Pro在内的专有模型以及Llava、LaVIT和Emu2等开源小型模型进行了评估，结果显示，这些模型在输入提示通过MM检索器（如UniIR模型）增强后，显著提高了生成质量。|
|**2024-05-16**|**4D Panoptic Scene Graph Generation**|Jingkang Yang et.al.|[2405.10305](http://arxiv.org/abs/2405.10305)|**[link](https://github.com/jingkang50/psg4d)**|**我们生活在一个三维空间中，同时通过第四维时间向前推进。为了使人工智能能够全面理解这种4D环境，我们提出了一种新的表示形式——4D全景场景图（PSG-4D），它将动态4D世界中的原始视觉数据抽象为节点和边，节点代表具有精确位置和状态信息的实体，边捕捉时间关系。为了促进在这一新领域的研究，我们构建了一个丰富的注释PSG-4D数据集，包含3000个RGB-D视频，总计100万帧，每帧都带有4D全景分割掩码以及详细的动态场景图标签。我们为此任务提出了一种名为PSG4DFormer的Transformer模型，该模型能够预测全景分割掩码，沿时间轴跟踪掩码，并通过关系组件生成相应的场景图。在新数据集上的大量实验表明，我们的方法为未来的PSG-4D研究提供了一个强大的基准。最后，我们展示了如何通过将大型语言模型融入我们的PSG-4D系统来实现动态场景理解的一个实际应用示例。**|
|**2024-05-16**|**HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models**|Rhea Sanjay Sukthanker et.al.|[2405.10299](http://arxiv.org/abs/2405.10299)|**[link](https://github.com/automl/hw-aware-llm-bench)**|**随着语言模型的规模不断扩大，对硬件指标（如延迟、能耗、GPU内存使用和性能）之间的权衡需求日益增长。人们正在寻求为不同语言模型配置建立帕累托前沿，以在指定硬件限制下找到最优模型。然而，对多种架构在多台设备上的全面训练和评估在计算上是不可行的。为此，我们提出了HW-GPT-Bench，这是一个基于硬件感知的语言模型代理基准，利用神经架构搜索（NAS）中的权重共享技术，在一个模型中高效地训练包含不同规模语言模型的超网络。我们在13种设备上对这些模型进行了性能剖析，考虑了5种硬件指标和3种不同的模型规模。最后，我们通过8种不同的多目标NAS算法展示了HW-GPT-Bench的可用性，并评估了由此产生的帕累托前沿的质量。我们的目标是推动和加速大型语言模型的多目标方法，如NAS和结构化剪枝的研究。**|
|**2024-05-16**|**Timeline-based Sentence Decomposition with In-Context Learning for Temporal Fact Extraction**|Jianhao Chen et.al.|[2405.10288](http://arxiv.org/abs/2405.10288)|null|**摘要：**  事实抽取对于构建知识图谱至关重要。随着对时间相关事实在下游任务中的需求增长，出现了时间性事实抽取的任务。本文特别关注从自然语言文本中提取时间性事实。先前的研究未能妥善处理复杂句子中时间与事实对应关系的建立难题。为解决这一挑战，我们提出了一种基于时间线的句子分解策略，利用大语言模型（LLMs）进行上下文学习，以实现对事实相关时间线的精细理解。然而，直接使用LLMs进行时间性事实抽取的性能并不理想。因此，我们引入了TSDRE方法，将LLMs的分解能力融入到小型预训练语言模型（PLMs）的传统微调过程中。  为了支持评估，我们构建了一个复杂的时序事实抽取数据集ComplexTRED。实验结果显示，TSDRE在HyperRED-Temporal和ComplexTRED数据集上实现了最先进的性能。|
|**2024-05-16**|**Revisiting OPRO: The Limitations of Small-Scale LLMs as Optimizers**|Tuo Zhang et.al.|[2405.10276](http://arxiv.org/abs/2405.10276)|null|近年来，许多研究旨在通过策略性提示提升大型语言模型（LLMs）的效能。特别是优化通过prompting（OPRO）方法表现出顶尖性能，它利用LLMs作为优化器，目标是寻找能最大化任务准确性的指令。本论文重新审视了OPRO在小型LLMs（如LaMa-2系列和Mistral 7B）上的自动化提示效果。我们的研究表明，对于小型LLMs，OPRO的效果有限，因为其有限的推理能力限制了优化潜力。因此，我们建议未来的自动提示工程应同时考虑模型能力和计算成本。针对小型LLMs，我们推荐直接提供明确阐述目标和方法的指令，作为稳健的提示基线，以确保在当前研究中实现高效且有效的提示设计。|
|**2024-05-16**|**Keep It Private: Unsupervised Privatization of Online Text**|Calvin Bao et.al.|[2405.10260](http://arxiv.org/abs/2405.10260)|**[link](https://github.com/csbao/kip-privatization)**|**## 背景  作者身份混淆技术有望通过自动重写文本来保护网络通信中的个人隐私。然而，在自然语言处理（NLP）文献中，这些技术的评估大多局限在狭小场景下，主要依赖于表面的编辑操作，可能导致输出不自然。本研究提出了一种自动文本私密化框架，通过强化学习对大型语言模型进行微调，以生成兼顾准确、连贯和隐私的重写。我们在大规模的英语Reddit帖子测试集上进行了详尽的评估，该数据集由68,000名作者撰写，包含短到中等长度的文本。我们探讨了在不同评估条件下，如作者简介长度和作者识别策略，性能的变化。我们的方法在自动化指标和人工评估中保持高文本质量，并成功地规避了几种自动作者识别攻击。**|
|**2024-05-16**|**When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models**|Xianzheng Ma et.al.|[2405.10255](http://arxiv.org/abs/2405.10255)|null|随着大型语言模型（LLMs）的不断发展，它们与三维空间数据（3D-LLMs）的融合取得了显著进步，这极大地增强了理解和互动物理环境的能力。这篇综述详细探讨了使LLMs能够处理、理解并生成三维数据的方法论，强调了LLMs的独特优势，如上下文学习、逐步推理、开放词汇能力和丰富的世界知识，这些将极大地推动人工智能体在空间理解与交互方面的发展。研究覆盖了从点云到神经辐射场（NeRF）等各种三维数据表示，并考察了它们与LLMs在任务中的结合，如三维场景理解、描述、问答和对话，以及基于LLM的代理进行空间推理、规划和导航。此外，我们还简要回顾了其他结合三维和语言的方法。本文的元分析显示了显著的进步，但也指出了挖掘3D-LLMs全部潜力所需的创新方法的必要性。因此，本文旨在为未来的研究方向提供指导，探索和扩展3D-LLMs在理解和互动复杂三维世界的能力。为了支持本调查，我们已在GitHub上建立了一个项目页面，整理并列出了相关论文：https://github.com/ActiveVisionLab/Awesome-LLM-3D。|
|**2024-05-16**|**A Systematic Evaluation of Large Language Models for Natural Language Generation Tasks**|Xuanfan Ni et.al.|[2405.10251](http://arxiv.org/abs/2405.10251)|null|近期的研究已评估了大型语言模型（LLMs）在常识推理、数学推理和代码生成等方面的能力。然而，据我们所知，尚无专门针对自然语言生成（NLG）任务的深入研究，这是衡量模型优秀程度的关键标准。因此，本论文旨在全面评估知名且性能出色的LLMs，包括ChatGPT、ChatGLM、基于T5的模型、基于LLaMA的模型和Pythia模型，在对话生成和文本总结等NLG任务中的表现。我们选择了涵盖英语和中文的数据集，并设计了一种共同的评估框架，包括输入模板和后处理策略。研究结果报告了自动评分，同时进行了详细分析。|
|**2024-05-16**|**IntelliExplain: Enhancing Interactive Code Generation through Natural Language Explanations for Non-Professional Programmers**|Hao Yan et.al.|[2405.10250](http://arxiv.org/abs/2405.10250)|null|大型语言模型（LLMs）在根据自然语言描述自动生成可执行代码方面展现出巨大潜力，特别是通过互动功能，用户可以通过迭代反馈指导模型。然而，当前的互动方式往往假设用户具备调试源代码的专业知识，对非专业程序员不太友好。这使得使互动代码生成对不同编程水平的个体更易于使用成为一个挑战。为解决这个问题，我们提出了IntelliExplain，这是一种创新的人机交互范式，通过让用户通过自然语言解释与源代码互动，提升非专业人士的体验。用户通过提供他们发现错误的自然语言纠正反馈，来指导系统修订代码，直到用户对系统的代码解释感到满意。我们的用户研究显示，使用IntelliExplain的用户在Text-to-SQL和Python代码生成任务中的成功率分别比纯GPT-3.5提高了11.6%和25.3%，同时所需时间分别减少了39.0%和15.6%。|
|**2024-05-16**|**CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations**|Jiahao Zhao et.al.|[2405.10212](http://arxiv.org/abs/2405.10212)|null|在这篇论文中，我们提出了一种创新的心理学基准测试——CPsyExam，它源于中国语言考试的问题。CPsyExam旨在分别强调心理学知识和案例分析的重要性，认识到将心理学知识应用于实际情境的价值。从22,000个问题库中，我们精选了4,000个来构建该基准，确保了主题的均衡覆盖，并包含了各种案例分析方法的多样性。此外，我们对一系列现有的大型语言模型（LLMs）进行了评估，包括开源和API基础的模型。实验和分析结果显示，CPsyExam是一个有效的确立语言模型对心理学理解能力的基准，同时支持在不同粒度上比较这些模型。|

<p align=right>(<a href=#updated-on-20240620>back to top</a>)</p>

