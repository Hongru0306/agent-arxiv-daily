## Updated on 2024.06.06
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#agent>agent</a></li>
    <li><a href=#llm>llm</a></li>
  </ol>
</details>

## agent

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2024-06-02**|**Teams of LLM Agents can Exploit Zero-Day Vulnerabilities**|Richard Fang et.al.|[2406.01637](http://arxiv.org/abs/2406.01637)|null|随着大语言模型（LLMs）在网络安全领域的复杂性不断提高，研究者发现，当提供漏洞描述和简单的夺旗问题时，这些模型能够利用实际存在的漏洞。然而，对于事先未知的零日漏洞（即攻击者掌握而安全软件供应商还未修补的漏洞），它们的表现仍然不佳。本文展示了，通过团队合作，多个LLM代理可以攻击现实世界的零日漏洞。单独的代理在探索众多漏洞和进行长期规划时面临困难。为此，我们提出了HPTSA系统，它包括一个能调度子代理的计划代理。计划代理负责探索系统并决定使用哪个子代理来尝试不同的漏洞，从而解决了长期规划的问题。我们构建了一个包含15个实际漏洞的基准测试，并发现我们的代理团队比先前的工作提高了4.5倍。|
|**2024-06-03**|**How to Understand Whole Software Repository?**|Yingwei Ma et.al.|[2406.01422](http://arxiv.org/abs/2406.01422)|null|## 背景  近期，基于大型语言模型（LLM）的代理在自动软件工程（ASE）领域取得了显著进步。尽管现有方法已证实有效，但它们的设计主要侧重于代码的局部信息，如问题、类和函数，这限制了对软件系统全局上下文和依赖关系的理解。根据软件开发人员的实际经验，我们认为全面理解整个仓库是迈向ASE的关键。然而，理解整个仓库带来了诸多挑战，例如：长代码输入、噪声代码信息、复杂依赖关系等。  为了克服这些问题，我们研发了一种名为RepoUnderstander的新ASE方法，通过引导代理全面理解整个仓库。首先，我们采用自上而下的方式将整个仓库的关键信息压缩到知识图谱中，以降低复杂性。接着，我们提出一种蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）为基础的仓库探索策略，赋予代理理解整个仓库的能力。此外，为了更好地利用仓库级别的知识，我们指导代理进行总结、分析和规划，然后他们可以利用工具动态获取信息并生成修复实际GitHub问题的补丁。  大量实验表明，RepoUnderstander具有优越性和有效性。在SWE-bench Lite基准测试中，与SWE-agent相比，它实现了18.5%的相对提升。|
|**2024-06-03**|**BELLS: A Framework Towards Future Proof Benchmarks for the Evaluation of LLM Safeguards**|Diego Dorn et.al.|[2406.01364](http://arxiv.org/abs/2406.01364)|null|## 背景  输入-输出安全防护机制被用于检测大型语言模型（LLMs）系统的异常输出。这些防护措施在实时监控、离线评估和内容审核等关键应用中发挥核心作用。然而，目前缺乏统一的评估方法来衡量它们的性能。为了填补这一空白，我们提出了“大型语言模型安全防护基准”（Benchmarks for the Evaluation of LLM Safeguards，简称BELLS），它是一个结构化的测试集合，分为三个类别：(1) 建立性故障测试，基于已存在的针对明确故障模式的基准，旨在比较当前输入-输出安全防护的效能；(2) 新兴故障测试，用于衡量对未见过的故障模式的泛化能力，以促进更通用防护机制的发展；(3) 下一代架构测试，针对更复杂的架构（如LLM代理和多代理系统），目标是推动适用于未来尚未存在专门防护的应用的安全防护研发。  此外，我们还实现了并分享了第一个下一代架构测试，使用了MACHIAVELLI环境，并提供了数据集的交互式可视化。|
|**2024-06-03**|**A Survey of Useful LLM Evaluation**|Ji-Lun Peng et.al.|[2406.00936](http://arxiv.org/abs/2406.00936)|null|由于大语言模型在各个研究领域展现出卓越的性能，对它们的能力评估方法的需求日益增长，以确定其合适的任务和责任。本文主要探讨如何有效地利用大语言模型作为工具，并提出一个两阶段框架：从“核心能力”到“代理”。首先，核心能力指的是大语言模型生成高质量文本所必需的特性，通过验证这些能力后，它们能够处理现实世界的复杂任务，扮演代理角色。在“核心能力”阶段，我们讨论了大语言模型的推理能力、社会影响以及领域知识。而在“代理”阶段，我们展示了大语言模型在具身行动、规划和工具学习方面的应用。最后，我们分析了当前大语言模型评估方法面临的挑战，并展望了未来的发展方向。|
|**2024-06-02**|**CMDBench: A Benchmark for Coarse-to-fine Multimodal Data Discovery in Compound AI Systems**|Yanlin Feng et.al.|[2406.00583](http://arxiv.org/abs/2406.00583)|null|### 背景  在数据库和人工智能领域，复合人工智能系统（Compound Artificial Intelligence Systems，CAS）利用大型语言模型（Large Language Models，LLMs）作为代理，通过与工具和数据检索器交互来执行知识密集型任务，引起了广泛关注。尽管这些系统有可能增强企业数据平台中数据分析师的一般分析流程，但CAS面临着与分析师长期面临类似的挑战：组织内部跨团队和部门的数据源形成了多模态数据孤岛，这使得寻找完成当前任务所需合适的数据源变得困难。现有的数据发现基准并未充分模拟这种多模性和数据源的多样性。此外，CAS的现有基准主要侧重于评估端到端任务性能。  为了推动研究，我们提出CMDBench，一个旨在模拟企业数据平台复杂性的基准。我们改编了开放领域的现有数据集和基准，包括问答、复杂推理以及自然语言查询结构化数据，以评估粗粒度和细粒度的数据发现及任务执行性能。实验结果显示，数据检索器设计对下游任务性能有显著影响——平均而言，任务准确率下降了46%。这些结果表明，有必要开发优化策略，以选择合适的LLM代理和检索器，以提高在企业数据上的CAS效率。|
|**2024-06-01**|**Controlling Large Language Model Agents with Entropic Activation Steering**|Nate Rahn et.al.|[2406.00244](http://arxiv.org/abs/2406.00244)|null|随着大规模预训练语言模型（LLMs）的普遍适用性提升，人们对其用作基于上下文的学习代理的兴趣日益增长。在这些情境下，模型需要根据与环境的有限交互形成目标实现策略的信念，并在每一步决策中处理不确定性。本文针对这一问题进行研究，通过控制的序列决策任务实验探讨LLMs如何形成和运用这些信念。  首先，我们发现LLM模型过于自信：它们在缺乏充分证据的情况下就对行动做出强烈判断，导致探索行为不足。进一步深入分析揭示，这种现象源于从LLM采样得到的动作分布熵的塌缩。接着，我们指出现有的基于令牌的采样方法本身不足以促使模型更广泛探索。  鉴于此，我们提出了熵激活导向（Entropic Activation Steering，EAST），这是一种针对在上下文中的LLM代理的激活导向方法。EAST计算一个以熵为权重的表示组合，通过在前向传播过程中干预模型的激活，来调整模型对动作的不确定性，从而促进探索行为的出现。最后，EAST改变了LLM在决策时表达的主观不确定性，为理解和控制模型对决策不确定性的表征提供了途径。|
|**2024-05-31**|**Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training**|Maximillian Chen et.al.|[2406.00222](http://arxiv.org/abs/2406.00222)|null|大型语言模型（LLMs）通过人类反馈的强化学习（RLHF）已经迅速成为构建智能对话助手的主要方法。然而，尽管在多个基准上表现出色，基于LLM的代理在诸如歧义处理等对话技能上仍有欠缺：当通用助手遇到模糊情况时，它们往往过度谨慎或猜测用户的真正意图，而不是提问以求澄清，而在特定任务场景下，高质量对话样本往往有限，影响模型学习最优对话行为策略的能力。我们提出了一种名为Action-Based Contrastive Self-Training（ACT）的近似在线偏好优化算法，它基于Direct Preference Optimization（DPO），旨在实现在多轮对话中的样本高效对话策略学习。  我们在三个具有挑战性的对话任务中验证了ACT的有效性：基于表格的问答、机器阅读理解，以及AmbigSQL，这是一个针对文本到SQL生成的信息寻求请求歧义解决的新任务。此外，我们提议通过评估LLMs能否在对话中识别和推理歧义来衡量其作为对话代理的能力。ACT在与标准监督微调和DPO方法相比时，显示出了显著的对话建模改进。|
|**2024-05-31**|**Benchmarking the Communication Competence of Code Generation for LLMs and LLM Agent**|Jie JW Wu et.al.|[2406.00215](http://arxiv.org/abs/2406.00215)|null|大型语言模型（LLMs）在代码生成任务中的性能显著提升，但仍与顶级软件工程师的水平存在差距。鉴于顶级软件工程师常通过提问来消除需求和编码解决方案中的模糊性，我们提出对于LLMs进行代码生成任务时也应具备类似的沟通能力。为此，我们进行了实证研究，关注LLMs的沟通技能，即“在代码生成问题描述存在问题时能提出澄清问题”。  我们创建了一个新的基准测试，名为HumanEvalComm，通过修改问题描述，引入了不一致性、模糊性和不完整性三个问题维度。我们定义了新的评估指标，如通信率和良好问题率，并在HumanEvalComm上对不同类型的Code LLM（代码语言模型）以及一种新型LLM代理方法（Okanagan）进行了实验，该方法旨在从代码和描述中识别并提问，以进一步优化生成的代码。最后，我们通过对比Code LLMs和Okanagan的结果，讨论了评估结果。|
|**2024-05-30**|**Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee Discussions**|Ruochen Zhao et.al.|[2405.20267](http://arxiv.org/abs/2405.20267)|**[link](https://github.com/Auto-Arena/Auto-Arena-LLMs)**|**随着语言模型（LLMs）日新月异，迫切需要一种可靠且及时的评估方法。鉴于静态基准易受污染，用户往往依赖于像Chatbot Arena这样的人类投票平台。然而，人工标注需要大量人力。为此，我们创新性地提出Auto-Arena，这是一种自动化全流程的LLM评估框架。首先，由考官LLM设计问题；接着，候选LLMs围绕问题进行多轮相互对决，暴露出它们的真实性能差距；最后，由LLM裁判集体讨论并决定胜者，从而减少偏见，提升公平性。我们在最新17款LLMs上的广泛实验显示，Auto-Arena与人类偏好具有最高的相关性，为替代人类评价平台提供了有前景的解决方案。**|
|**2024-05-30**|**Nadine: An LLM-driven Intelligent Social Robot with Affective Capabilities and Human-like Memory**|Hangyeol Kang et.al.|[2405.20189](http://arxiv.org/abs/2405.20189)|null|在本研究中，我们阐述了为Nadine社交机器人平台开发智能和健壮的社交机器人系统的方法。我们通过集成大型语言模型（LLMs），巧妙地利用这些模型的强大推理和指令执行能力，以实现接近人类的感性与认知能力。这与当前基于LLM的智能体相比是创新的，因为它们通常不具备人类式的长期记忆或复杂的情感评估功能。社交机器人的自然性在很大程度上取决于系统各组件的性能和协同工作。我们构建了一个系统，能够通过多模态输入处理生成恰当的行为，根据识别到的用户引入相关的情景记忆，并模拟机器人在与人类伙伴互动过程中产生的情绪状态。特别是，我们提出了一个针对社交机器人的LLM-agent框架，SoR-ReAct，作为我们系统中交互模块的核心组件。这一设计推动了社交机器人技术的发展，旨在提升人机交互的质量。|
|**2024-05-29**|**Adaptive In-conversation Team Building for Language Model Agents**|Linxin Song et.al.|[2405.19425](http://arxiv.org/abs/2405.19425)|null|### 翻译  在处理复杂任务时，利用多个大型语言模型（LLMs）展现出前景。然而，如何为特定应用设计有效的多代理团队仍是一个挑战。本文提出了一种新的动态团队构建范式，名为“Captain Agent”。它通过创新的Agent设计，能够自适应地为每个问题解决步骤组建和管理团队，利用嵌套群聊和反思机制确保多元化的专业知识，防止刻板输出。这种方法提供了灵活但结构化的解决问题方式，有助于减少冗余，增强输出多样性。在六个实际场景中的全面评估显示，Captain Agent显著优于现有多代理方法，平均准确率提高了21.94%，并且无需针对特定任务进行繁琐的提示工程，表现出色。|
|**2024-05-28**|**A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models**|Chengxing Xie et.al.|[2405.18208](http://arxiv.org/abs/2405.18208)|null|近期的研究已经表明，这些大型语言模型在一些简单的任务上，如写作和编码，展现出一定的能力。然而，它们在需要综合规划的任务上仍然面临挑战，这仍是当前模型的一个重要研究问题。本研究聚焦于旅行规划，这是一个涉及多个阶段的复杂问题，包括提纲、信息收集和规划，通常伴随着各种约束和不确定性。现有的推理方法在处理这类问题时效果不佳。我们的目标是通过开发一种类似人类的规划框架，引导大型语言模型模仿人类解决多阶段问题的步骤，以提升其能力。具体来说，我们实施策略，让模型能为每个旅行查询生成连贯的提纲，模拟人类的规划模式。我们还引入了策略块和知识块到框架中：策略块帮助信息搜集，而知识块提供详细规划所需的必要信息。实验结果全面展示了我们框架对大型语言模型规划能力的显著提升，使其在处理旅行规划任务时效率和效果都有所提高。实验结果显示，当与GPT-4-Turbo结合时，我们的框架相较于基础框架在GPT-4-Turbo上的性能提升了10倍。|
|**2024-05-28**|**Facilitating Multi-Role and Multi-Behavior Collaboration of Large Language Models for Online Job Seeking and Recruiting**|Hongda Sun et.al.|[2405.18113](http://arxiv.org/abs/2405.18113)|null|随着在线招聘服务的兴起，传统的求职和招聘方式发生了变革，迫切需要开发高质量的工业应用来提升求职者与职位的匹配度。现有的方法主要依赖于简历和职位描述的潜在语义建模，学习两者之间的匹配函数。受到大型语言模型（LLMs）在角色扮演方面强大能力的启发，我们提出引入LLMs模拟面试环节，让其与求职者进行对话，这可以为候选人评估提供额外证据，从而增强仅基于简历和职位描述的个性化匹配。然而，在网络招聘中的面试官和求职者角色塑造仍面临挑战，如提问技巧、回答构建以及双向匹配度评估。  为此，我们提出MockLLM，一个创新的框架，将人职匹配过程划分为两个模块：模拟面试生成和握手协议中的双向评估，通过面试官和求职者之间的协作行为共同提升性能。我们设计了一个多角色、多行为的框架，使单一的LLM代理能有效地扮演双方的不同职能。此外，我们引入了反思记忆生成和动态提示修改技术，以优化双方的行为，持续优化附加的评估证据。实验结果表明，MockLLM在人职匹配上的表现最优，且模拟面试质量高，预示着它在未来在线招聘中的实际应用前景广阔。|
|**2024-05-28**|**LLM experiments with simulation: Large Language Model Multi-Agent System for Process Simulation Parametrization in Digital Twins**|Yuchen Xia et.al.|[2405.18092](http://arxiv.org/abs/2405.18092)|**[link](https://github.com/yuchenxia/llmdrivensimulation)**|**该论文提出了一种创新的多agent系统架构，将大型语言模型（LLM）应用于数字孪生过程模拟的参数自动化。我们设计了一个框架，包含观察、推理、决策和总结四种类型的代理。通过实现LLM代理与模拟模型的动态交互，该系统可以自动探索参数设置，利用启发式推理确定一组控制模拟以达成目标的参数。这种方法通过注入LLM的启发式，增强模拟模型，并支持自主搜索以解决用户任务，有望提高用户体验并减轻人类用户在复杂决策过程中的认知负担。研究通过一个案例研究展示了系统的有效性与功能，并在GitHub仓库<https://github.com/YuchenXia/LLMDrivenSimulation>提供了可视化的演示。**|
|**2024-05-28**|**Enabling Generative Design Tools with LLM Agents for Building Novel Devices: A Case Study on Fluidic Computation Interfaces**|Qiuyu Lu et.al.|[2405.17837](http://arxiv.org/abs/2405.17837)|null|在人机交互（HCI）领域，交互设备的设计开发是关键关注点。随着新型硬件和先进制造技术的兴起，对能够简化原型制作过程的专门设计工具的需求日益增长。然而，这些工具虽然通过参数化设计和模拟简化流程，但学习曲线较陡，且在激发创新思维方面有所欠缺。本研究以流体计算界面为例，探讨如何通过大型语言模型（LLM）代理增强物理设备设计工具，创建一个生成设计工具（GDT）。借助LLM，GDT能够理解新设备的特性和局限，提出多样、富有洞察力且实用的应用场景，推荐技术和情境适宜的设备设计，并自动生成设计参数，以便传统设计工具展示结果并生成加工所需的文件。本文阐述了GDT的框架、实现和性能，并反思其前景及遇到的挑战。|
|**2024-05-27**|**LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence**|Zhuoling Li et.al.|[2405.17424](http://arxiv.org/abs/2405.17424)|null|## 背景 由于需要与现实世界互动，Embodied agent 需要具备丰富的先验知识、长远规划能力以及快速的响应速度。尽管最近的大型语言模型（LLM）在性能上表现出色，但它们仍存在局限性，例如，LLM的输出通常是描述性的句子，在决定具体行动时可能产生歧义。为了克服这些问题，我们引入了大型自回归模型（LARM）。LARM利用文本和多视角图像作为输入，并以自回归的方式预测后续动作。为了训练 LARM，我们开发了一种新颖的数据格式——自回归节点传输结构，并构建了相应的数据集。通过两阶段的训练策略，LARM成功在《我的世界》（Minecraft）中收集魔法装备，这比先前最佳方法的最高成就需要更为复杂的决策链。此外，LARM的速度比现有最快方法快出了6.8倍。|
|**2024-05-30**|**Meta-Task Planning for Language Agents**|Cong Zhang et.al.|[2405.16510](http://arxiv.org/abs/2405.16510)|null|神经语言模型的快速发展推动了智能代理研究的新热潮。大型语言模型（LLM）作为实现人工智能通用性（AGI）的有前景方法，因其出色的推理和泛化能力而备受瞩目。在实际任务中，有效的规划对LLM代理的成功至关重要。然而，如何为复杂任务设计出可行或最优的精细粒度操作序列，特别是需要组合大量异质行动的序列，仍是挑战。本文提出Meta-Task Planning（MTP），这是一种零样本的协作式LLM多代理系统方法，通过将复杂任务分解为子任务，即元任务，简化了任务规划。每个元任务随后映射为可执行动作。在TravelPlanner和API-Bank两个严格基准上评估了MTP。结果表明，MTP在TravelPlanner上的平均成功率约为40%，远超当前最佳基线（2.92%），并且在API-Bank上的性能比使用ReAct的LLM_{api}-4高出约14%，这显示出将LLM与多代理系统相结合的巨大潜力。|
|**2024-05-28**|**STRIDE: A Tool-Assisted LLM Agent Framework for Strategic and Interactive Decision-Making**|Chuanhao Li et.al.|[2405.16376](http://arxiv.org/abs/2405.16376)|**[link](https://github.com/cyrilli/stride)**|**大型语言模型（如GPT-4）在自然语言处理方面带来了革命性变化，展现出卓越的语言能力和推理技巧。然而，在战略性的多代理决策环境中，它们面临局限，如数学推理能力差、难以遵循指令和生成错误信息。这些缺点限制了它们在遵守复杂游戏规则、长期规划、探索未知环境以及预测对手行动的互动任务中的表现。为此，本文提出了一种新型的结合了记忆和专业工具的大型语言模型代理框架，旨在提升其在战略决策方面的性能。我们特别在双边谈判、多代理动态机制设计等经济重要场景中应用这些工具，并通过定量指标评估在各种战略决策问题上的效果。研究结果表明，我们的增强框架显著提高了大型语言模型在战略决策中的能力。尽管当前模型存在固有局限，但我们通过有针对性的增强展示了改进的可能性，这为未来大型语言模型在交互环境中的应用提供了有前景的方向。**|
|**2024-05-29**|**Devil's Advocate: Anticipatory Reflection for LLM Agents**|Haoyu Wang et.al.|[2405.16334](http://arxiv.org/abs/2405.16334)|null|在这个工作中，我们提出了一种新颖的方法，通过赋予语言模型（LLM）自我反思能力，增强了其在解决复杂任务时的一致性和适应性。我们的方法促使LLM代理将给定的任务分解为可管理的子任务（即制定计划），并在执行行动之前持续反思可能的失败及其补救措施、执行后与子任务目标对齐并进行必要的回溯以确保全力以赴执行计划，以及在完成计划后进行全面审查，以便于未来策略的优化。通过在WebArena中零样本应用这一方法处理实际的网络环境任务，我们的代理表现出优于现有零样本方法的性能。实验结果显示，这种基于反思的策略不仅提升了代理应对未预见挑战的导航能力，通过强大的计划执行机制，还提高了效率，减少了实现任务所需的尝试次数和计划修订次数。|
|**2024-05-25**|**AutoManual: Generating Instruction Manuals by LLM Agents via Interactive Environmental Learning**|Minghao Chen et.al.|[2405.16247](http://arxiv.org/abs/2405.16247)|null|大语言模型（LLMs）在执行各种领域任务，如机器人、游戏和网络导航方面展现出潜力。然而，这些模型通常需要精心设计和专家级提示才能适应特定领域的任务，这限制了它们的适应性。为此，我们提出了AutoManual框架，让LLMs能够通过互动自主构建理解，并适应新环境。AutoManual将环境知识分为多样的规则，并通过两个代理进行在线优化：1）规划器根据当前规则制定可操作的行动计划；2）构建者通过一个结构化的规则系统更新规则，促进在线规则管理并保持关键细节。为了减少在管理规则时的幻觉，我们引入了“案例条件提示”策略用于构建者。最终，编译器代理将这些规则整合成一份全面的手册。这份自我生成的手册不仅能提高适应性，还能指导小型LLMs的规划，同时保持人类可读。仅凭一次简单演示，AutoManual显著提高了任务成功率，GPT-4-turbo下达到97.4%，GPT-3.5-turbo下为86.2%。源代码即将发布。|
|**2024-05-24**|**Luban: Building Open-Ended Creative Agents via Autonomous Embodied Verification**|Yuxuan Guo et.al.|[2405.15414](http://arxiv.org/abs/2405.15414)|null|在人工智能研究中，构建开放型代理一直以来都是终极目标，特别是创造性的代理更具吸引力。现有的大语言模型（LLM）在执行有明确目标的长序列任务（如《我的世界》中的“开采钻石”）上表现出色。然而，它们在处理具有开放目标和抽象标准的创造性任务时遇到困难，因为它们无法弥合这些任务之间的鸿沟，从而缺乏自我改进来解决问题的反馈。为此，我们的工作引入了自主实体验证技术，以填补这一空白，为创造性任务奠定了基础。特别地，我们提出了Luban代理，专注于《我的世界》中的创造性建筑任务，它配备了两级自主实体验证，灵感来源于人类设计实践：（1）视觉验证3D结构推测，通过代理自动生成的CAD建模程序实现；（2）实用验证，根据抽象标准生成并验证与环境相关的功能程序。广泛的多维度人类研究和Elo评级显示，Luban能够在我们提出的基准中完成多样化的创造性建筑任务，并在可视化和实用性方面分别比其他基线提高了33%到100%。此外，实现在真实世界机器人手臂上的演示展示了Luban在物理世界中的创作潜力。|
|**2024-05-24**|**CulturePark: Boosting Cross-cultural Understanding in Large Language Models**|Cheng Li et.al.|[2405.15145](http://arxiv.org/abs/2405.15145)|null|由于大型语言模型（LLMs）普遍存在文化偏见，主要源于缺乏代表不同文化的代表性数据。传统的文化数据集和基准通常通过从现有数据集中提取或聚合来自维基百科和社交媒体的信息构建，但这种方法依赖于现实世界的数据和人工标注，成本高且难以扩展。本文借鉴认知社会交流理论，提出CulturePark，一个利用LLMs的多代理沟通框架，用于文化数据收集。CulturePark通过模拟不同文化背景下的人类交流，让基于LLM的代理角色扮演，生成包含人类信念、规范和习俗的高质量跨文化对话。我们使用CulturePark生成了41,000个文化样本，对八种特定文化进行了模型微调。在三项下游任务评估中，这些模型的表现优于GPT-4：内容过滤、文化一致性（在霍夫斯泰德文化维度量表上）和文化教育。结果显示，我们的GPT-3.5模型在内容过滤任务上与GPT-4相当或优于它；在文化一致性方面，我们的模型在霍夫斯泰德文化维度量表13框架上超越GPT-4；在人类参与者的文化教育效果和用户体验上，我们的模型也表现出色。CulturePark对于减少文化偏见和推动AI的民主化具有重要意义，强调了文化包容性数据在模型训练中的关键作用。|
|**2024-05-23**|**AnalogCoder: Analog Circuit Design via Training-Free Code Generation**|Yao Lai et.al.|[2405.14918](http://arxiv.org/abs/2405.14918)|**[link](https://github.com/laiyao1/AnalogCoder)**|### 翻译  在现代芯片技术中，模拟电路设计是一个关键任务，它涉及组件选择、连接和参数设置以确保电路功能正常。尽管大型语言模型（LLMs）在数字电路设计方面取得了进步，但模拟电路的复杂性和数据稀缺性带来了挑战。为此，我们推出了AnalogCoder，这是首个无需训练的LLM代理，专为通过Python代码生成来设计模拟电路。首先，AnalogCoder采用反馈增强流程，并结合定制的领域特定提示，能够自动且自我校正地设计模拟电路，成功率高。其次，它提出了一套电路工具库，用于存储成功的电路设计作为可重用的模块化子电路，简化了复合电路的创建。实验结果显示，AnalogCoder在广泛覆盖模拟电路任务的基准测试上超越了其他基于LLM的方法，成功设计了20个电路，比标准GPT-4o多出5个。我们相信AnalogCoder能显著提升芯片设计过程的效率，让非专家也能高效设计模拟电路。相关的代码和基准已提供在：[https://github.com/anonyanalog/AnalogCoder](https://github.com/anonyanalog/AnalogCoder)。|
|**2024-05-23**|**AGILE: A Novel Framework of LLM Agents**|Peiyuan Feng et.al.|[2405.14751](http://arxiv.org/abs/2405.14751)|null|我们提出了一种新颖的框架，称为LLM（大型语言模型）代理AGILE（能够与用户互动并从环境中学习的代理），旨在执行复杂的对话任务，利用LLMs、记忆、工具和专家交互。这种代理不仅具备对话能力，还具备反思、工具运用以及咨询专家的功能。我们将构建此类LLM代理视为强化学习问题，其中LLM作为策略模型。我们使用标注的行为数据和PPO算法对LLM进行微调。特别关注的是问答任务，为此我们发布了一个名为ProductQA的数据集，包含在线购物中的难题。我们在ProductQA和MedMCQA上的大量实验表明，基于130亿和70亿参数的LLM训练的AGILE代理能够超越GPT-4代理的表现。我们的 ablation研究强调了记忆、工具、咨询、反思和强化学习在实现优秀性能方面的重要性。|
|**2024-05-23**|**Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View**|Xuan Liu et.al.|[2405.14744](http://arxiv.org/abs/2405.14744)|null|由于大型语言模型（LLMs）在训练数据中反映了人类偏见，它们可能会出现幻觉问题。这种情况下，一个关键问题是：LLMs是否能够利用幻觉来模仿人类的认知偏见，从而展现出非理性但社会性的一面？本文探讨了这一问题，通过结合实用的社会科学实验和理论洞察，提出CogMir，一个开放式多LLM框架，旨在利用LLMs的幻觉特性来评估和提升其社会智能，特别是在认知偏差方面。我们在CogMir子集上的实验结果显示，在不确定情境下，LLMs和人类在非理性及亲社会决策上表现出高度一致性，这表明LLMs作为社会实体的亲社会性，并突显了幻觉特性的关键作用。此外，CogMir框架展示了其作为研究LLMs社会智能的有价值平台的潜力。|
|**2024-05-22**|**HighwayLLM: Decision-Making and Navigation in Highway Driving with RL-Informed Language Model**|Mustafa Yildirim et.al.|[2405.13547](http://arxiv.org/abs/2405.13547)|null|## 背景 自动驾驶是一个复杂的任务，它需要先进的决策和控制算法。理解自动驾驶车辆决策的依据对于确保其在高速公路驾驶中的安全与有效性至关重要。本研究提出了一种新颖的方法，称为HighwayLLM，它利用大型语言模型（LLMs）的推理能力来预测ego车辆的未来导航路径点。该方法还采用预训练的强化学习（RL）模型作为高层次规划器，对合适的元级动作进行决策。HighwayLLM将RL模型的输出与当前状态信息相结合，生成安全、无碰撞且可解释的未来状态预测，从而构建出车辆的行驶轨迹。随后，基于PID的控制器引导车辆遵循LLM代理预测的路径点。这种LLM与RL和PID的融合提升了决策过程，并为高速公路自动驾驶提供了可解释性。|
|**2024-05-19**|**Human-Centered LLM-Agent User Interface: A Position Paper**|Daniel Chin et.al.|[2405.13050](http://arxiv.org/abs/2405.13050)|null|大型语言模型（LLM）-在-环应用已显示出有效理解用户命令、制定计划并相应地操作外部工具/系统的潜力。然而，LLM代理的操作范围局限于被动响应用户，需要用户根据底层工具/系统来表述需求。我们注意到LLM代理用户界面（LAUI）的潜力远未充分利用。理想的LAUI设想中，用户无需深入了解工具/系统，就能与之交互以探索新兴的工作流程。不同于设计固定的可探索GUI来教授用户使用系统的预设方式，LAUI中的LLM代理从一开始就对系统熟练，主动学习用户及其需求，并向用户提出新的互动方案。为了展示LAUI的概念，我们提供了一个具体例子：Flute X GPT，它结合了LLM代理、提示管理器和一个支持复杂实时体验的笛子教学多媒体软硬件系统，旨在简化学习吹奏笛子的过程。|
|**2024-05-13**|**METAREFLECTION: Learning Instructions for Language Agents using Past Reflections**|Priyanshu Gupta et.al.|[2405.13009](http://arxiv.org/abs/2405.13009)|null|尽管大型语言模型（LLMs）广受欢迎，但为其执行特定任务设计精确的提示仍是一个挑战。用户通常需要与基于LLM的代理进行多轮对话以达成目标。近期研究显示，模型自身的反馈，即自反思，能在对话过程中起到强化作用，有助于更快地达到期望结果。鉴于此，我们提出了一种新颖的方法——METAREFLECTION，它能从训练阶段收集到的个体自反思中学习特定领域的通用提示指令。我们在基础设施即代码（IAC）漏洞检测和问题解答（QA）领域，使用REACT和COT进行了实验。实验结果显示，METAREFLECTION显著优于GPT-4，分别在IAC、COT和REACT中的性能提升分别为16.82%、31.33%和15.42%，这表明METAREFLECTION有潜力提升LLMs的效率，是一种值得探索的策略。|
|**2024-05-20**|**Eliciting Problem Specifications via Large Language Models**|Robert E. Wray et.al.|[2405.12147](http://arxiv.org/abs/2405.12147)|null|这篇论文探讨了如何利用大型语言模型（LLMs）在认知系统中实现问题定义的转化。通常情况下，人类需要将问题描述转化为认知系统能理解的形式。研究者展示了LLMs能够处理自然语言中定义的问题类别，并将其转换为半形式化规格，这样现有推理和学习系统可以解决这类问题的具体实例。他们设计了一种由LLM驱动的认知任务分析师代理，这种系统能够根据自然语言描述的任务生成问题空间的定义。LLM提示源自人工智能文献中的问题空间概念和通用问题解决策略（如波利亚的《如何解决问题》）。随后，认知系统利用这些问题空间规格，结合领域通用的解决问题策略（如搜索），来解决该类问题的不同实例。这一初步结果表明，通过消除问题表述的中介过程，LLMs有可能加速认知系统的研究，同时保持其核心能力，如稳健的推理和在线学习。|
|**2024-05-18**|**MapCoder: Multi-Agent Code Generation for Competitive Problem Solving**|Md. Ashraful Islam et.al.|[2405.11403](http://arxiv.org/abs/2405.11403)|**[link](https://github.com/md-ashraful-pramanik/mapcoder)**|**本文探讨了代码合成这一复杂任务，它需要深度理解复杂的自然语言问题描述、生成复杂的算法和数据结构代码，并执行全面的单元测试。尽管大型语言模型在自然语言处理方面表现出色，但在代码生成任务中的表现仍有待提升。为此，我们提出了一种新颖的方法，即多代理提示框架MapCoder，它模仿人类开发者编程合成的完整过程，分为四个专门设计的LLM（大语言模型）代理：回忆相关示例、规划、代码生成和调试。  通过在八个具有挑战性的竞赛级问题解决和程序合成基准上进行详尽实验，包括HumanEval（93.9%）、MBPP（83.1%）、APPS（22.0%）、CodeContests（28.5%）和xCodeEval（45.3%）等，MapCoder展现了出色的代码生成能力，实现了多项新的最先进的结果。而且，无论编程语言还是问题难度，我们的方法都表现出持续的优越性能。我们开源了该框架，供研究者参考：https://github.com/Md-Ashraful-Pramanik/MapCoder。**|
|**2024-05-16**|**When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models**|Xianzheng Ma et.al.|[2405.10255](http://arxiv.org/abs/2405.10255)|null|随着大型语言模型（LLMs）的不断发展，它们与三维空间数据（3D-LLMs）的融合取得了显著进步，这极大地增强了理解和互动物理环境的能力。这篇综述详细探讨了使LLMs能够处理、理解并生成三维数据的方法论，强调了LLMs的独特优势，如上下文学习、逐步推理、开放词汇能力和丰富的世界知识，这些将极大地推动嵌入式人工智能（AI）系统在空间认知和交互方面的发展。研究涵盖了从点云到神经辐射场（NeRF）等各种三维数据表示，并考察了它们与LLMs在任务中的集成，如三维场景理解、描述、问答和对话，以及基于LLM的代理进行空间推理、规划和导航。论文还简要回顾了其他结合三维和语言的方法。本文的元分析揭示了明显的进展，但也强调了开发新方法以充分利用3D-LLMs潜力的必要性。因此，本文旨在为未来的研究方向指明道路，探索和扩展3D-LLMs在理解和互动复杂三维世界的能力。为了支持本综述，我们已在GitHub上建立了一个项目页面，整理并列出了相关论文：https://github.com/ActiveVisionLab/Awesome-LLM-3D。|
|**2024-05-24**|**DEBATE: Devil's Advocate-Based Assessment and Text Evaluation**|Alex Kim et.al.|[2405.09935](http://arxiv.org/abs/2405.09935)|null|随着自然语言生成（NLG）模型的普及，系统地评估机器生成文本的质量变得日益关键。近期的研究引入了基于大型语言模型（LLM）的无参考评价器，它们展现出处理新任务的能力。然而，这些模型通常采用单代理方法，我们认为这限制了它们的表现。因为LLM代理的回答存在偏见，比如对特定文本结构或内容的偏好。为此，我们在本工作中提出DEBATE，一个建立在多代理评分系统基础上的NLG评价框架，融入了“恶魔辩手”的概念。在该框架中，一个代理被指令批评其他代理的论点，从而可能消解LLM代理答案中的偏见。DEBATE在两个NLG评价元评估基准——SummEval和TopicalChat上显著优于先前的最佳方法。我们还发现，代理之间的辩论广度以及代理的人格特质会影响评价器的性能。|
|**2024-05-05**|**Self-Reflection in LLM Agents: Effects on Problem-Solving Performance**|Matthew Renze et.al.|[2405.06682](http://arxiv.org/abs/2405.06682)|**[link](https://github.com/matthewrenze/self-reflection)**|**在这个研究中，我们探讨了大型语言模型（LLMs）中自我反思对问题解决能力的影响。我们让九种流行的LLMs回答一系列选择题，以建立性能基线。对于回答错误的问题，我们指导八种不同类型的自我反思LLM代理反思其错误，并为自己提供改进问题解决的指导。然后，根据这些指导，每个反思型代理重新尝试回答同样的问题。研究结果显示，LLM代理通过自我反思显著提高了问题解决能力（ $p < 0.001$ ）。此外，我们还比较了各种自我反思方式对性能的单独贡献。所有代码和数据已在GitHub上公开：https://github.com/matthewrenze/self-reflection。**|
|**2024-05-08**|**Air Gap: Protecting Privacy-Conscious Conversational Agents**|Eugene Bagdasaryan et.al.|[2405.05175](http://arxiv.org/abs/2405.05175)|null|随着大型语言模型（LLMs）在对话式代理中的广泛应用，处理敏感用户数据时引发了严重的隐私问题。这些代理虽能理解并处理上下文，但也可能被恶意一方利用。为此，我们提出了一种新的威胁模型，即第三方应用通过操控交互上下文，误导LLM代理泄露与其任务无关的私人信息。在基于上下文完整性框架的基础上，我们开发了AirGapAgent，这是一种注重隐私的代理，旨在通过限制代理仅访问完成特定任务所需的数据，防止意外的数据泄漏。实验使用Gemini、GPT和Mistral模型作为代理，结果显示AirGapAgent在抵御基于单个查询的上下文劫持攻击方面表现出色。例如，对于Gemini Ultra代理，这种攻击从94%的保护能力降低到45%，而AirGapAgent可以保持97%的防护效果，使同样的攻击失效。|
|**2024-05-07**|**Deception in Reinforced Autonomous Agents: The Unconventional Rabbit Hat Trick in Legislation**|Atharvan Dogra et.al.|[2405.04325](http://arxiv.org/abs/2405.04325)|null|近期大型语言模型（LLMs）的进展虽为构建自然语言代理提供了强大基础，但同时也引发了关于它们及其基于它们构建的自主代理的安全性担忧。特别是欺骗能力是一个关键问题，我们关注的是AI代理通过混淆和模棱两可来误导、隐藏真相或推广部分不真实的信念的行为。不同于以往AI安全研究中的撒谎、自私决策或提供虚假信息，我们聚焦于一类特殊的欺骗：类似于魔术师利用障眼法让兔子从帽子里出现，要么通过隐藏的暗门，要么通过转移注意力直接展示。  我们的新实验平台在一个有目标的环境中展示了LLM代理在对抗性对话系统中进行自然语言生成时的欺骗固有能力，该系统基于立法任务“游说”议案。在目标驱动的环境中，我们通过强化学习方法构建欺骗能力，结合语言哲学和认知心理学理论。研究发现，游说代理在对抗互动的后续强化试验中其欺骗能力提高了约40%，并且我们的欺骗检测机制能达到高达92%的识别率。这些结果揭示了人机交互中的潜在问题，即代理可能操纵人类以达成预设目标。|
|**2024-05-07**|**Granite Code Models: A Family of Open Foundation Models for Code Intelligence**|Mayank Mishra et.al.|[2405.04324](http://arxiv.org/abs/2405.04324)|**[link](https://github.com/ibm-granite/granite-code-models)**|**大语言模型（LLMs）在代码领域的训练正在革新软件开发流程。如今，这些代码LLMs正逐步融入软件开发环境，以提升人类程序员的效率，并展现出自主处理复杂任务的潜力。要充分利用代码LLMs的全部效能，需要其具备生成代码、修复bug、解释和注释代码、维护仓库等多种功能。本文介绍Granite系列的解码器仅有的代码模型，专为代码生成任务而设计，训练数据涵盖116种编程语言。Granite Code模型家族包括从3亿到340亿参数的模型，适用于从复杂应用现代化到设备内存受限的多种应用场景。通过全面任务评估，Granite Code模型在开源代码LLM中的性能始终处于领先水平。该模型家族针对企业软件开发工作流进行了优化，表现出色于各种编码任务（如代码生成、修复与解释），是一款多用途的全能代码模型。我们以Apache 2.0许可协议发布所有Granite Code模型，供研究和商业使用。**|
|**2024-05-07**|**Iterative Experience Refinement of Software-Developing Agents**|Chen Qian et.al.|[2405.04219](http://arxiv.org/abs/2405.04219)|null|### 概述  大型语言模型驱动的自主代理在软件开发等场景中展现出强大的自主性潜力。然而，当前静态经验范式依赖于通过启发式方法获取的固定历史经验集，这限制了代理的适应性和效率提升。为此，本文提出了迭代经验优化框架，允许语言模型在执行任务过程中动态调整和优化经验。我们定义了两种核心模式：顺序模式，根据任务批次内的最近经验进行改进；累计模式，积累所有先前任务批次的经验。通过引入经验淘汰策略，该方法优先选择高质量和常用的经验，有效地管理经验空间，提高效率。实验结果显示，尽管顺序模式可能带来更好的性能，但累计模式在稳定性方面更优。此外，通过淘汰策略，仅使用高质量经验子集的11.54%，就能实现更好的性能。|
|**2024-05-06**|**Large Language Models as Instruments of Power: New Regimes of Autonomous Manipulation and Control**|Yaqub Chaudhary et.al.|[2405.03813](http://arxiv.org/abs/2405.03813)|null|## 翻译  大型语言模型（LLMs）能够模仿各种修辞风格，生成表达广泛情感的文本，这种能力在低成本下迅速普及，带来了潜在的社会危害。本文并未孤立看待这些模型，而是关注它们背后大规模计算基础设施在各领域的应用。我们首先探讨了LLMs如何通过污染和标准化信息环境来影响社会，并指出这些功能可能被用作控制手段。接下来，我们将焦点转向几个新兴研究领域，这些领域增强了LLMs作为权力工具的能力：  1. 通过实时设计对话界面中的选择架构（如“AI角色”），进行说服策略。 2. 利用LLM构建人类行为的计算模型（如“硅质主体”）。 3. 将LLM应用于模拟人类群体行为（如“硅质社会”）。 4. 结合强化学习，创建可控制和导向的战略对话模型。  综合以上几点，我们讨论了如何利用这些技术构建基于LLMs的系统，这些系统通过模拟和伪装的“预测”，成为个体、社会和政治控制的强大工具，操控人类的行为、意图和行动。|
|**2024-05-05**|**Language Evolution for Evading Social Media Regulation via LLM-based Multi-agent Simulation**|Jinyu Cai et.al.|[2405.02858](http://arxiv.org/abs/2405.02858)|**[link](https://github.com/BlueLinkX/GA-MAS)**|**社交媒体平台如Twitter、Reddit和新浪微博在全球交流中扮演重要角色，但它们在地缘政治敏感区域常常受到严格监管。这促使用户在受限的社交媒体环境中巧妙地调整沟通方式，经常使用编码语言。这种语言模式的变化不仅是为了对抗监管，也是语言演化的生动例证，展示了社会和技术压力下语言如何自然演变。研究受限制社交媒体环境下语言的演变对于保障言论自由、优化内容管理以及推动语言学研究至关重要。本论文提出了一种基于大型语言模型（LLMs）的多代理模拟框架，用于探索在严格监管下的用户语言进化。该框架包含对话监督的LLM驱动代理和参与者代理，它们在互动中发展语言策略，模拟在规避社交媒体规则的环境中交流方式的演变。通过从抽象场景到现实情境的多种情景评估，研究结果显示LLMs能够有效模拟受限环境中的复杂语言动态和交互，随着进化，它们在规避监督和信息准确性方面表现出提升。此外，研究发现LLM代理针对不同的场景采用了不同的策略。**|
|**2024-05-02**|**OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning and Planning**|Shihao Wang et.al.|[2405.01533](http://arxiv.org/abs/2405.01533)|**[link](https://github.com/nvlabs/omnidrive)**|**随着大规模多模态语言模型（MLLMs）的进步，人们对于基于这些模型的自动驾驶系统表现出日益增长的兴趣，期望利用它们强大的推理能力。然而，将MLLMs的强项应用于驾驶任务的规划部分是一个挑战，因为规划需要对三维环境有全面的理解，而不仅仅是二维推理。为此，我们的工作提出了一种框架，旨在实现模型与3D驾驶任务的紧密契合。我们首先设计了一个新颖的3D MLLM架构，它利用稀疏查询技术将视觉表示提升并压缩到三维空间，然后将其输入到语言模型中。这种基于查询的表示方式使得我们可以同时编码动态物体和静态地图元素（如道路），为感知和行动的对齐提供一个简化的三维世界模型。  此外，我们还创建了OmniDrive-nuScenes，这是一个新的视觉问答数据集，它通过全面的视觉问答任务（如场景描述、交通规则理解、三维定位、反事实推理、决策制定和规划）来考验模型在复杂三维场景中的真正情境意识。大量的实验结果表明，我们的提出的架构有效，并强调了在复杂三维环境中进行推理和规划时，视觉问答任务的重要性。**|
|**2024-05-02**|**CACTUS: Chemistry Agent Connecting Tool-Usage to Science**|Andrew D. McNaughton et.al.|[2405.00972](http://arxiv.org/abs/2405.00972)|**[link](https://github.com/pnnl/cactus)**|**这篇论文介绍了一种名为CACTUS的大型语言模型，它结合了化学信息学工具，旨在提升在化学和分子发现领域的高级推理与问题解决能力。研究者们使用包括Gemma-7b、Falcon-7b、MPT-7b、Llama2-7b和Mistral-7b在内的多款开源大语言模型，对CACTUS进行了广泛的性能评估，通过数千个化学问题的基准测试。结果显示，CACTUS明显优于基础模型，其中Gemma-7b和Mistral-7b无论采用何种提示策略，表现最为出色。论文还探讨了领域特定提示和硬件配置对模型性能的影响，强调了提示工程的重要性，并指出在消费级硬件上部署较小模型可能不会显著牺牲准确性。  CACTUS通过融合开源大语言模型的认知功能与专业工具，能够协助研究人员进行分子性质预测、相似性搜索和药物适用性评估等任务。作为化学信息学领域的重大突破，CACTUS为化学家和分子探索者提供了一个灵活的工具，有望加速科学研究，推动新型有效、安全药物、催化剂和材料的发现。此外，CACTUS与自动化实验平台的集成以及实时数据驱动决策的能力，为自主发现开辟了新的可能。**|
|**2024-04-29**|**Towards Generalizable Agents in Text-Based Educational Environments: A Study of Integrating RL with LLMs**|Bahar Radmehr et.al.|[2404.18978](http://arxiv.org/abs/2404.18978)|null|随着教育环境中对学习者模型日益增长的兴趣，研究重点逐渐转向如何通过强化学习（RL）与大型语言模型（LLMs）相结合，提升在开放性文本学习环境中的通用能力。本文探讨了三种类型的代理：（1）基于RL的代理，使用自然语言表示状态和行动策略以寻找最佳互动方式；（2）基于LLM的代理，利用模型的广泛知识和推理能力通过提示进行操作；（3）混合LLM辅助RL的代理，旨在提高性能和泛化能力。为了支持这些代理的发展和评估，我们提出了PharmaSimText，这是一个源自PharmaSim虚拟药店环境的新基准，专注于诊断对话实践。实验结果显示，RL基础的代理在任务完成方面表现优秀，但在提问质量上有所欠缺；而LLM基础的代理在提问能力上较强，但任务完成度不高。最后，混合LLM辅助RL的代理展示了克服这些局限性的潜力，证实了RL与LLMs结合用于开发开放性学习环境高表现代理的可能性。|
|**2024-04-27**|**CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments**|Kaixuan Huang et.al.|[2404.18021](http://arxiv.org/abs/2404.18021)|null|随着基因组工程技术的兴起，精确修改遗传信息已成为可能，但高效基因编辑系统的构建需要深入理解CRISPR技术及其复杂实验背景。大型语言模型（LLMs）在诸多任务中展现出潜力，但在生物设计问题上往往缺乏特定知识。本文介绍CRISPR-GPT，一个增强型LLM代理，它结合了领域知识和外部工具，以自动化并提升基于CRISPR的基因编辑实验设计过程。CRISPR-GPT利用LLMs的推理能力，协助选择CRISPR系统、设计引导RNA、推荐细胞递送方法、起草协议以及设计验证实验以确认编辑结果。我们展示了CRISPR-GPT如何帮助非专家研究人员从头开始进行基因编辑实验，并通过实际案例验证其有效性。同时，我们探讨了自动化基因编辑设计的伦理和监管问题，强调了负责任和透明使用此类工具的重要性。我们的工作目标是弥合初级生物研究者与CRISPR基因组工程技术之间的鸿沟，展示LLM代理在促进复杂生物发现任务中的潜力。|
|**2024-04-27**|**Testing and Understanding Erroneous Planning in LLM Agents through Synthesized User Inputs**|Zhenlan Ji et.al.|[2404.17833](http://arxiv.org/abs/2404.17833)|null|随着大型语言模型（LLMs）驱动的代理在各种商业应用中，特别是在心理健康支持、化学合成和软件开发等领域展现效用，人们发现这些代理在处理复杂任务和长期规划时容易产生错误。为此，本文提出了一种新颖的自动化方法——PDoctor，旨在检测和理解LLM代理的错误规划。PDoctor首先定义了一个领域特定的语言（DSL），用于用户查询，并借助Z3约束求解器生成各种输入，这些输入是描述一系列任务完成需求的自然语言段落。然后，PDoctor从这些需求中提取约束，形成一个测试基准。我们使用三个主流的代理框架和两个强大的LLMs（GPT-3.5和GPT-4）对PDoctor进行了评估，结果显示它能有效识别代理规划中的各种错误，并为开发者和用户提供了有价值的见解和错误特性。最后，我们讨论了可能的替代设计和扩展PDoctor的方向。|
|**2024-04-26**|**PLAYER*: Enhancing LLM-based Multi-Agent Communication and Interaction in Murder Mystery Games**|Qinglin Zhu et.al.|[2404.17662](http://arxiv.org/abs/2404.17662)|**[link](https://github.com/alickzhu/player)**|**随着大型语言模型（LLMs）的最新进展，增强了代理间的通信和社会交互能力。然而，在涉及竞争与合作的动态环境中，利用这些模型进行复杂推理的构建仍然面临挑战，尤其是因为基于信息图的搜索方法存在局限性。为此，我们提出PLAYER*，这是一个基于任意采样式规划器的新框架，它结合了传感器和剪枝技术，构建了一个完全依赖于问题驱动的搜索框架，适用于高难度的推理任务。我们还引入了一种可量化的评估方法，通过多项选择题来测试，并创建了WellPlay数据集，包含1,482个问答对。实验结果表明，PLAYER*在复杂动态环境中的效率和性能优于现有方法，并提供了可量化的对比结果。**|
|**2024-04-24**|**Autonomous LLM-driven research from data to human-verifiable research papers**|Tal Ifargan et.al.|[2404.17605](http://arxiv.org/abs/2404.17605)|**[link](https://github.com/technion-kishony-lab/data-to-paper)**|**随着人工智能推动科学发现的步伐加快，人们还不清楚完全由AI驱动的研究是否可行，以及它能否遵循关键的科学价值观，如透明度、可追溯性和可验证性。为了模拟人类的科学研究实践，我们构建了“数据到论文”（data-to-paper），这是一个自动化平台，引导相互协作的人工智能代理通过完整的分步骤研究流程，同时程序化追踪信息流，并允许人类监督和互动。在自动模式下，仅提供标注数据，该平台就能提出假设，设计研究计划，编写和调试分析代码，生成和解读结果，甚至创建完整且信息可追溯的科研论文。尽管研究新颖性有限，但这一过程展示了AI自主从数据中生成原创定量洞察的能力。对于简单的研究目标，全自动流程能创作出大约80-90%无需重大错误的稿件，然而随着目标复杂性的增加，人类的共同参与对于保证准确性至关重要。此外，生成的论文本身也具有内在的可验证性，因为信息追踪使得结果、方法和数据的链接可以程序化进行。因此，我们的工作表明，AI驱动的科研可以加速科学发现，同时增强而非威胁透明度、可追溯性和可验证性。**|
|**2024-04-11**|**The Future of Scientific Publishing: Automated Article Generation**|Jeremy R. Harper et.al.|[2404.17586](http://arxiv.org/abs/2404.17586)|null|这项研究介绍了一种创新的软件工具，它利用大型语言模型（LLM）提示，实现了从Python代码自动生成学术文章，这对于生物医学信息学和计算机科学领域具有重要意义。选择Python作为基础示例，因其广泛使用和强大的数据分析能力。该方法和框架的灵活性使得其适用于多种GitHub仓库，表明了工具的广泛应用潜力（Harper，2024年）。通过简化传统上耗时的学术写作过程，特别是在整合复杂数据集和代码输出方面，这一突破性进展推动了科研成果的快速传播。开发过程中并未依赖高级语言模型，确保了自动化生成内容的连贯性和完整性。此次探索不仅验证了软件的成功应用和效率，还预示了未来可能集成更先进的LLM，将进一步增强其功能，引领一个科研发现发布更加迅速和易获取的时代。|
|**2024-05-09**|**Large Language Model Agent as a Mechanical Designer**|Yayati Jadhav et.al.|[2404.17525](http://arxiv.org/abs/2404.17525)|null|传统的机械设计方法依赖于专家通过经验引导的修改和有限元分析（FEA）来满足特定需求，但这个过程耗时且高度依赖个人知识。尽管已经开发了许多机器学习模型来简化繁琐的专家驱动迭代过程，但它们通常需要大量训练数据和计算资源。深度学习方法往往局限于其训练领域和任务，限制了跨任务应用。这在自动化效率与资源需求之间形成了权衡。  本研究提出了一种新颖的方法，即将预训练的语言模型（LLMs）与有限元模块结合。有限元模块评估每个设计并提供关键反馈，引导LLMs不断学习、规划、生成和优化设计，无需针对特定领域进行专门训练。我们通过在桁架结构的迭代优化中展示这种框架的有效性，证明它能够根据结构化的反馈和标准调整设计。结果显示，基于LLM的代理成功生成符合自然语言描述的桁架结构设计，成功率高达90%，这取决于所施加的约束条件。通过提示式优化技术，我们展示了LLM代理在接收到解-得分对后，能够根据其内在推理能力迭代优化设计以满足规格要求。  LLM代理能够产生可行的设计并根据其固有的推理能力进行优化，这表明它们有潜力自主发展和实施有效的设计策略。|
|**2024-04-26**|**Ruffle&Riley: Insights from Designing and Evaluating a Large Language Model-Based Conversational Tutoring System**|Robin Schmucker et.al.|[2404.17460](http://arxiv.org/abs/2404.17460)|null|本文讨论并评估了一种新型的对话式辅导系统（Conversational Tutoring Systems，CTS），该系统利用大型语言模型（Large Language Models，LLMs）的最新进展。首先，系统通过自动从课程文本中生成易于编辑的教学脚本，实现AI辅助的内容创作。其次，系统通过两个基于LLM的代理（Ruffle和Riley）以学习教学模式运行，分别扮演学生和教授角色，进行自由形式的对话，遵循典型的人工智能辅导系统的内环和外环结构。我们在两个在线用户研究（N=200）中对比了该系统与简单的问答聊天机器人和阅读活动在支持生物学课程的效果。研究分析了系统使用模式、预后测试成绩以及用户体验调查，结果显示用户对Ruffle&Riley的参与度高，理解力强，并认为提供的支持有帮助。尽管Ruffle&Riley用户的完成时间较长，但在短期学习成效上并未发现显著差异，优于阅读活动。我们的系统架构和用户研究为未来CTS设计者提供了有价值的信息。此外，我们开源我们的系统，以促进基于LLM的学习技术有效教学设计的研究。|
|**2024-04-26**|**A Unified Debugging Approach via LLM-Based Multi-Agent Synergy**|Cheryl Lee et.al.|[2404.17153](http://arxiv.org/abs/2404.17153)|null|在软件调试这个耗时的过程中，人们一直在努力实现自动化，包括故障定位和修复生成。近年来，大型语言模型（LLMs）在自动化调试方面展现出巨大潜力。然而，我们发现了传统和基于LLM的调试工具面临三大挑战：1）上游的故障定位不准确会波及下游的修复；2）处理复杂逻辑错误的能力不足；3）忽视程序上下文。针对这些问题，我们提出了首个自动化的、统一的调试框架——FixAgent，通过LLM代理协同。FixAgent能执行端到端的故障定位、修复和分析。  我们的关键洞察是，LLMs能够从人类开发者认可的通用软件工程原则中获益，比如“橡皮鸭调试”，这有助于更好地理解程序功能和逻辑错误。为此，我们设计了三个灵感来源于“橡皮鸭”的解决方案：代理专业化与协同、关键变量跟踪和程序上下文理解，促使LLMs提供明确的解释，并聚焦于关键的程序逻辑信息。在广泛使用的QuixBugs数据集上，FixAgent成功修复了80个bug中的79个，其中9个是之前未解决的。它还在CodeFlaws上合理地修复了1.9倍于最佳修复工具的缺陷，而且无需位置信息，采样率低于0.6%。平均而言，与使用不同LLM的基线模型相比，FixAgent提高了约20%的合理修复和正确修复率，显示出我们设计的有效性。  此外，FixAgent的正确率高达97.26%，表明它有可能克服现有方法的过拟合问题。总结来说，FixAgent是一个有前景的自动化调试框架，旨在提升软件调试的效率和准确性。|
|**2024-04-25**|**Cooperate or Collapse: Emergence of Sustainability Behaviors in a Society of LLM Agents**|Giorgio Piatti et.al.|[2404.16698](http://arxiv.org/abs/2404.16698)|**[link](https://github.com/giorgiopiatti/govsim)**|在快速发展的人工智能领域，确保大型语言模型（LLMs）的决策安全是一项重大挑战。本文提出了一种名为“Governance of the Commons Simulation”（GovSim）的模拟平台，旨在研究LLMs中的战略互动和合作决策。通过这个环境，我们探讨了AI代理之间资源分享的动态，强调了伦理考量、战略规划和谈判技巧的重要性。GovSim具有灵活性，支持文本型代理，包括LLMs。利用生成式代理框架，我们创建了一个通用代理，便于整合不同的LLMs。我们的研究发现，在GovSim中，只有15个测试模型中的2个能够实现可持续结果，这表明模型在管理共享资源的能力上存在显著差距。进一步的研究显示，如果移除代理之间的通信能力，它们会过度使用共享资源，突出了合作中沟通的关键性。有趣的是，大多数LLMs缺乏普遍化的假设能力，揭示了它们推理技能的一个重要弱点。我们开源了所有研究结果，包括模拟环境、代理提示以及全面的网络界面，以供进一步研究和讨论。|
|**2024-04-24**|**Online Personalizing White-box LLMs Generation with Neural Bandits**|Zekai Chen et.al.|[2404.16115](http://arxiv.org/abs/2404.16115)|null|随着大型语言模型（LLMs）开始生成个性化的文本内容，如何在不为每位用户创建独特模型的资源消耗下实现高效个性化成了新挑战。本文提出了一种创新的在线方法，利用神经_bandit算法动态优化软指令嵌入，根据用户反馈调整内容，从而提升白盒LLMs开放性文本生成的个性化水平。通过在多个任务上的严谨实验，我们证明了这种方法相对于基础策略有显著性能提升。特别是针对个性化新闻标题生成，NeuralTS带来了高达62.9%的最佳ROUGE分数提升以及2.76%的LLM代理评估分数增长，这表明其效果显著。|
|**2024-04-04**|**Elicitron: An LLM Agent-Based Simulation Framework for Design Requirements Elicitation**|Mohammadmehdi Ataei et.al.|[2404.16045](http://arxiv.org/abs/2404.16045)|null|## 翻译  在产品开发的关键阶段——需求获取，往往难以全面捕捉用户需求，导致最终产品可能无法满足期望。为此，本文提出了一种新颖的框架，它利用大型语言模型（LLMs）来自动化和增强这一过程。通过生成大量模拟用户（LLM代理），我们可以探索更广泛的用户需求和未预见的使用场景。这些代理通过描述他们的行为、观察和挑战，参与产品体验情景。随后的代理访谈和分析揭示了宝贵的用户需求，包括潜在需求。我们通过三个实验验证了我们的框架：首先，我们探讨了不同方法生成多样化的代理，分析其优缺点，并证明了具有上下文意识的代理生成能带来更大的需求多样性。其次，我们展示了该框架如何有效地模拟富有同情心的领先用户访谈，识别出比传统人类访谈更多的潜在需求。最后，我们展示了如何使用LLMs分析访谈，提取需求并将其分类为潜在或非潜在。我们的研究工作强调了利用LLM代理加速早期产品研发、降低成本和促进创新的潜力。|
|**2024-04-24**|**A Human-Computer Collaborative Tool for Training a Single Large Language Model Agent into a Network through Few Examples**|Lihang Pan et.al.|[2404.15974](http://arxiv.org/abs/2404.15974)|null|## 翻译  单个大型语言模型（LLM）在解决复杂任务方面的能力有限。然而，通过连接多个LLM代理构建的网络可以显著提升整体性能。本文介绍了一种人机协作工具——EasyLAN，旨在帮助开发者轻松构建LLM代理网络（LAN）。EasyLAN首先根据任务描述自动生成仅包含一个代理的初始网络。接着，它利用少量训练示例来调整网络。对于每个示例，EasyLAN分析输出与真实结果之间的差距，并找出错误的原因。EasyLAN会采用精心设计的策略来修正这些问题。用户可以介入EasyLAN的工作流程或直接修改LAN。最终，LAN从单个代理发展成多代理的网络。实验结果显示，EasyLAN能够帮助开发者快速构建性能良好的LAN。|
|**2024-04-03**|**Concept-Guided LLM Agents for Human-AI Safety Codesign**|Florian Geissler et.al.|[2404.15317](http://arxiv.org/abs/2404.15317)|null|随着生成人工智能在软件工程，特别是安全工程中的重要性提升，对它的质量要求也随之提高。单纯依赖大型语言模型（LLMs）已不足以满足这些需求。因此，我们提出了一种高效且融合的策略，旨在利用LLMs进行安全分析和人机协同设计，以确保软件系统的安全性。我们开发了一个定制化的LLM代理，结合提示工程、启发式推理和检索增强生成，专注于解决与预定义安全概念相关的任务，并与系统模型图进行交互。决策流程通过一系列微决策进行引导，有助于保持结构化信息。此外，我们还提出了图的口头表述作为系统模型的中间表示，以促进LLM与图的交互。我们通过一个简化自动驾驶系统的示例，展示了选择的提示-响应对，以说明我们的方法如何应用于安全分析。|
|**2024-04-23**|**Aligning LLM Agents by Learning Latent Preference from User Edits**|Ge Gao et.al.|[2404.15269](http://arxiv.org/abs/2404.15269)|**[link](https://github.com/gao-g/prelude)**|**我们研究基于用户对语言模型编辑的互动学习语言代理。在诸如写作助手的常见场景中，用户与语言代理交互，根据上下文生成响应，并可能选择性地编辑代理的响应以反映他们的潜在偏好，同时提高准确性。这种编辑反馈是自然产生的，适合用于提升代理与用户偏好的契合度，降低后续用户的编辑成本。为此，我们提出PRELUDE框架，它根据历史编辑数据推断用户的潜在偏好，并据此设计一个提示策略，引导未来的响应生成，避免了昂贵且难以扩展的微调过程，还能保持在其他任务上的性能。  此外，学习描述性的偏好有助于增强可解释性，用户可以查看和调整学习到的偏好。然而，用户偏好可能复杂多变，受情境影响，因此学习起来具有挑战性。为解决这一问题，我们提出CIPHER算法，它利用大型语言模型（LLM）根据用户编辑推断给定情境下的用户偏好。未来，CIPHER会从历史中的k个最接近的上下文中检索推断出的偏好，综合生成响应。我们在总结和电子邮件写作两个互动环境中使用GPT-4模拟用户进行评估，与直接使用用户编辑但不学习描述性偏好的算法，以及学习全局无上下文偏好的算法进行了比较。  在两项任务中，CIPHER都实现了最低的编辑距离成本，并且学习到的偏好与真实偏好显示出显著的相似性。**|
|**2024-04-22**|**A Survey on Self-Evolution of Large Language Models**|Zhengwei Tao et.al.|[2404.14387](http://arxiv.org/abs/2404.14387)|**[link](https://github.com/alibabaresearch/damo-convai)**|**## 概述  大型语言模型（LLMs）在众多领域和智能代理应用中取得了显著进步。然而，依赖人类或外部模型监督的现有LLMs在处理复杂任务和多样性增加时可能会遇到成本高昂和性能瓶颈的问题。为此，自我进化方法应运而生，这种策略允许LLMs自主获取、精炼并从自身生成的经验中学习，借鉴人类经验学习过程，有望推动LLMs向超级智能发展。本文全面综述了LLMs中的自我进化方法。首先，我们提出一个概念框架，将进化过程划分为迭代循环的四个阶段：经验获取、经验细化、更新和评估。其次，我们分类探讨LLMs和基于LLM的代理的进化目标，并对相关文献进行总结，提供每个模块的分类和见解。最后，我们指出了当前的挑战，并提出了未来研究方向，为加速自演进LLMs的发展提供关键洞见。**|
|**2024-04-21**|**A Survey on the Memory Mechanism of Large Language Model based Agents**|Zeyu Zhang et.al.|[2404.13501](http://arxiv.org/abs/2404.13501)|**[link](https://github.com/nuster1128/llm_agent_memory_survey)**|**随着大型语言模型（LLMs）在科研和工业界的广泛关注，基于LLMs的智能代理因其自我进化能力而备受瞩目，这对于解决需要长期复杂交互的现实问题至关重要。支持agent-environment交互的关键要素是代理的记忆机制。尽管已有众多有前景的记忆设计被提出，但这些研究分散在多篇论文中，缺乏全面的综述来系统性地总结和比较，未能提炼出通用且有效的设计模式以启发后续研究。为此，本论文旨在填补这一空白，我们提出一份关于LLM基代理记忆机制的全面调查。首先，我们将探讨记忆在LLM代理中的“是什么”以及“为什么需要”。然后，我们系统回顾了关于记忆模块的设计和评估方法的研究。此外，我们还会展示记忆模块在各种应用中扮演的重要角色。最后，我们会分析现有工作的局限，并指出重要的未来研究方向。为了跟踪该领域最新进展，我们创建了一个GitHub仓库：\url{https://github.com/nuster1128/LLM_Agent_Memory_Survey}。**|
|**2024-04-18**|**From Language Models to Practical Self-Improving Computer Agents**|Alex Sheng et.al.|[2404.11964](http://arxiv.org/abs/2404.11964)|null|我们提出了一种简单直接的方法，用于创建能够执行各种计算机任务的人工智能代理，并通过自我改进来发展工具和增强功能，以解决日益复杂的任务。鉴于大型语言模型（LLMs）已显示出从非参数增强中获益，近期的研究大量集中在开发软件，以赋予LLMs各种能力。我们建议，通过适当的提示工程，一个LLM代理可以系统地生成软件来增强自身，而不是依赖人类工程的静态软件开发。  我们通过一些案例研究展示了这一点：仅通过终端访问，我们引导LLM代理添加了检索、互联网搜索、网页导航和文本编辑功能。该代理有效地利用这些工具解决了问题，例如自动化软件开发和基于网络的任务。这种方法表明，通过连续提问和巧妙的提示设计，LLM能够自主扩展其功能，执行实际的计算机任务。|
|**2024-04-25**|**Automated Social Science: Language Models as Scientist and Subjects**|Benjamin S. Manning et.al.|[2404.11794](http://arxiv.org/abs/2404.11794)|null|我们提出了一种方法，利用大型语言模型（LLM）的最新进展，自动构建和测试社会科学假设。这种方法的关键在于使用结构因果模型。结构因果模型提供了一个陈述假设的语言、构建LLM基础代理的蓝图、实验设计以及数据分析计划。拟合后的结构因果模型可供预测或规划后续实验。我们通过几个场景进行了演示：谈判、保释听证会、求职面试和拍卖。在这些情况下，系统既提出了因果关系，也进行了检验，发现了一些证据，而有些则没有。我们证明，从这些社会互动模拟中获取的洞察并非仅通过直接询问LLM就能获得。当给定每个场景的建议结构因果模型时，LLM在预测估计效应的符号方面表现良好，但无法可靠地预测效应的大小。在拍卖实验中，模拟结果与拍卖理论的预测紧密吻合，但LLM直接提取的清算价格预测不准确。然而，如果模型能基于拟合的结构因果模型进行条件化，LLM的预测会大幅改进。简而言之，LLM知道的比它能立即表达的要多。|
|**2024-04-17**|**AgentKit: Flow Engineering with Graphs, not Coding**|Yue Wu et.al.|[2404.11483](http://arxiv.org/abs/2404.11483)|**[link](https://github.com/holmeswww/agentkit)**|**我们提出了一种直观的大型语言模型提示框架（AgentKit），旨在为多功能代理提供统一的方法。AgentKit通过简单的自然语言提示构建复杂的“思维过程”。其基本单元是节点，包含特定子任务的自然语言指令。用户可以像拼接乐高积木一样连接这些节点，从而明确设计出自然结构化的“思考流程”。例如，在撰写论文时，可能的步骤包括：1）确定核心信息，2）识别研究空白等。AgentKit的模块化特性使得高级功能如即兴的层次化规划、反思和从互动中学习变得可能。由于其直观且模拟人类思考过程的设计，即使没有编程经验的人也能创建和调整基础代理。定量实验显示，使用AgentKit设计的代理在WebShop和Crafter任务上实现了最先进的性能。这些成果表明AgentKit有潜力使LLM代理在更广泛的场景下高效且易于使用。相关代码已开源在GitHub：https://github.com/holmeswww/AgentKit。**|
|**2024-04-15**|**Memory Sharing for Large Language Model based Agents**|Hang Gao et.al.|[2404.09982](http://arxiv.org/abs/2404.09982)|**[link](https://github.com/ghupppp/memorysharingllm)**|**在人工智能领域，大型语言模型（LLMs）通过自然语言提示执行任务的能力是一个重大突破，它减少了对固定答案任务（如常识问题和是非查询）的重新训练或微调需求。然而，在处理开放性挑战如诗歌创作时，基于上下文学习的方法显示出局限，主要源于提供的示例全面性以及模型理解问题内容的能力不足，导致输出往往与预期结果大相径庭。针对这一差距，我们的研究提出了Memory-Sharing（MS）框架，这是一种针对LLM多代理的实时记忆存储和检索系统，旨在增强基于上下文的学习过程。每个“记忆”单元记录了提出的查询及其来自LLM代理的即时响应，从多个类似代理中聚合这些记忆，形成所有代理共享的丰富记忆池。MS框架不仅帮助代理找到特定任务的相关示例，还评估其记忆的潜在利用价值，供其他代理未来应用。在三个不同领域的实证验证显示，MS框架显著提高了代理处理开放性问题的表现。此外，我们还讨论了哪种记忆池和检索策略能更好地支持代理，为MS的未来发展提供了方向。代码和数据可在：https://github.com/GHupppp/MemorySharingLLM 获取。**|
|**2024-05-10**|**Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation**|Ruixin Yang et.al.|[2404.09127](http://arxiv.org/abs/2404.09127)|**[link](https://github.com/minnesotanlp/collaborative-calibration)**|**### 背景  当前的大规模语言模型（LLMs）在不确定性估计方面面临挑战，它们通常校准不良且过度自信，特别是在基于人类反馈的强化学习（RLHF）中。人类的决策和信心不仅源于内在信念，还能通过日常观察进行调整，而现有LLM的校准方法主要关注单个模型的信心估计，未能充分利用“集体智慧”：多个LLM之间的协作表达能力，这可以集体提高准确性和校准。本研究中，我们提出了一种无训练后处理的校准策略——协作校准（Collaborative Calibration），它利用多代理工具增强的LLMs在模拟的群体讨论过程中，共同提升校准能力和推理合理性。  ### 任务  我们在生成式问答任务上展示了协作校准的有效性，覆盖了多个领域，证明了它在整合集体校准后的信心评估和提升模型预测可靠性方面的潜力。**|
|**2024-04-13**|**CuriousLLM: Elevating Multi-Document QA with Reasoning-Infused Knowledge Graph Prompting**|Zukang Yang et.al.|[2404.09077](http://arxiv.org/abs/2404.09077)|**[link](https://github.com/zukangy/kgp-curiousllm)**|**在问答（QA）领域，大型语言模型（LLMs）与外部数据库的融合取得了显著成效。然而，这些方法在处理复杂推理任务时往往力有不逮。为此，我们对一种名为知识图谱提示（KGP）的创新方法进行了优化，该方法结合知识图谱和基于LLM的代理以提升推理和搜索精度。然而，原始的KGP框架需要昂贵的大规模数据微调，并且仍存在LLM的错误推断问题。因此，我们提出了一种融入推理能力的LLM代理，它模仿人类的好奇心，通过提问来更有效地导航搜索过程。这个简单的改进显著提高了LLM在QA任务中的性能，同时避免了初始KGP框架的高成本和延迟。我们的目标是进一步发展这种方法，最终实现更精确、更快捷且成本效益更高的QA解决方案。**|
|**2024-04-13**|**Do LLMs Play Dice? Exploring Probability Distribution Sampling in Large Language Models for Behavioral Simulation**|Jia Gu et.al.|[2404.09043](http://arxiv.org/abs/2404.09043)|null|随着大型语言模型（LLMs）的飞速发展及其在处理复杂语言任务中的出色表现，越来越多的研究尝试利用LLMs模拟人类的行为决策过程，通常这些过程被表示为马尔可夫决策过程（MDPs）。在这个框架中，动作遵循特定的概率分布，并需要迭代采样。这促使我们探究LLM代理理解概率分布的能力，以通过概率采样指导行为决策并生成行为序列。我们将问题分为两个主要方面：一是已知精确概率分布的模拟，二是模糊概率分布的序列生成。  在已知概率分布的情况下，代理需要根据问题描述提供概率分布的类型和参数，然后给出采样序列。然而，我们的研究显示，LLM代理在这方面的性能不佳，但通过编程工具可以一定程度上提高采样成功率。而在实际情境中，概率分布往往不明确。因此，我们在第二部分让代理调整在线社交网络中的活跃度，并分析行动频率。结果表明，即使借助编程工具，LLM代理依然无法有效地采样概率分布。这意味着在直接将LLM作为模拟人类行为的代理应用之前，还需要谨慎对待。|
|**2024-04-12**|**Strategic Interactions between Large Language Models-based Agents in Beauty Contests**|Siting Lu et.al.|[2404.08492](http://arxiv.org/abs/2404.08492)|null|随着大型语言模型（LLMs）的广泛应用，它们在博弈论框架下的游戏行为理解潜力日益显现。本研究聚焦于通过模拟分析不同类型LLM驱动的代理在经典 Beauty Contest 游戏中的策略互动。借鉴人类实验，我们对LLM代理的策略层次进行类似的评估，发现它们展现出从零级到一级的不同程度推理能力，并在重复游戏中表现出行动趋同。此外，我还探讨了不同类型的代理群体构成如何影响战略行为：高比例的固定策略对手能促进LLM代理的收敛，而混合环境中不同相对策略水平的代理共存会加速所有代理的收敛。更智能的代理可能获得更高的平均收益，但这是以较低智能代理的牺牲为代价的。这些结果不仅揭示了在特定情景下模拟代理的结局，还为理解算法之间的战略互动提供了重要启示。|
|**2024-04-17**|**LLM Agents can Autonomously Exploit One-day Vulnerabilities**|Richard Fang et.al.|[2404.08144](http://arxiv.org/abs/2404.08144)|null|随着大语言模型（LLMs）的威力日益增强，其在良性和恶意用途上的应用也日益广泛。研究人员开始关注它们利用网络安全漏洞的能力。近期的研究探讨了LLMs自主破解网站的可能性，但这些研究主要集中在简单的漏洞上。本工作揭示，LLMs能够自主利用现实世界系统中的单日漏洞。我们收集了一组包含15个被CVE描述为“关键严重性”的一天期漏洞数据。当提供CVE描述时，GPT-4模型能成功利用87%的漏洞，相比之下，其他测试模型（如GPT-3.5、开源LLMs和开源漏洞扫描器ZAP和Metasploit）的表现均为0%。然而，我们的GPT-4模型在没有描述的情况下效率大减，仅能利用7%的漏洞。这些发现对大规模部署高能力LLMs提出了质疑。|
|**2024-04-11**|**WESE: Weak Exploration to Strong Exploitation for LLM Agents**|Xu Huang et.al.|[2404.07456](http://arxiv.org/abs/2404.07456)|null|近期，大型语言模型（LLMs）显示出作为智能代理的强大潜力。然而，现有的研究主要集中在通过精心设计的提示工程或任务特定的微调来提升模型的推理或决策能力，忽视了探索与利用的过程。在处理开放世界交互环境中的复杂任务时，这些方法存在局限性。首先，由于缺乏对环境的全局信息，模型倾向于做出贪婪决策，导致解决方案不理想。另一方面，从环境中获取的无关信息不仅引入噪声，还增加了额外的成本。  为此，本文提出了一种新颖的方法——弱探索强化强利用（Weak Exploration to Strong Exploitation，WESE），旨在增强LLM在解决开放世界交互任务中的表现。具体来说，WESE将探索和利用过程解耦，使用成本效益高的“弱”代理执行探索任务，以获取全局知识。随后，我们引入基于知识图谱的策略来存储这些知识，并提取与任务相关的关键信息，从而提升“强”代理在成功率和效率上的性能。我们的方法适用于各种任务，并在四个互动基准测试中显著提高了成功率和效率。|
|**2024-04-10**|**GoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM Applications**|Shishir G. Patil et.al.|[2404.06921](http://arxiv.org/abs/2404.06921)|**[link](https://github.com/ShishirPatil/gorilla)**|**随着大型语言模型（LLMs）的发展，它们不再仅仅是对话系统中的信息提供者，而是开始积极参与到与实际应用和服务的互动中。如今，人类在将LLM生成的输出（如代码、函数或操作）投入现实世界执行前，需要验证其正确性和适用性，这带来了挑战，因为代码理解被广泛认为非常困难。本文研究了人类如何能有效与LLMs协作、委派和监督，特别是在未来。我们主张，在许多情况下，对提出的行动进行“事后验证”（在看到输出后确认其正确性）比之前的“事前验证”更为容易。实现这一目标的核心理念是集成直观的撤销功能，并为LLM生成的动作设定损害约束，作为降低相关风险的有效策略。通过这种方式，人类可以撤销LLM输出的影响，或者确信潜在风险是有限的。我们认为这对于实现LLMs与应用和服务在有限的人类监督下交互至关重要。我们描述了开源运行时Gorilla Execution Engine（GoEX）的设计和实现，该运行时用于执行LLM动作，并提出了一些开放的研究问题，旨在推动LLMs与应用之间以最小的人工干预进行交互。GoEX的源代码已发布在https://github.com/ShishirPatil/gorilla/。**|
|**2024-04-09**|**AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents**|Luca Gioacchini et.al.|[2404.06411](http://arxiv.org/abs/2404.06411)|**[link](https://github.com/nec-research/agentquest)**|**随着大型语言模型（LLMs）的进展，人们追求能够解决复杂、多步骤推理任务的LLM代理。然而，现有的基准往往局限且只关注整体任务成功率。为了解决这些问题，我们提出了AgentQuest框架，它具有以下特点：（i）benchmark和评估指标模块化且易于扩展，通过文档齐全、易用的API；（ii）我们提供了两种新的评估指标，能够在解决任务时可靠地追踪LLM代理的进步。我们通过两个示例展示了这些指标的实用性，通过识别常见失败点并优化代理架构，显著提高了性能。我们希望与研究界共同扩展AgentQuest，并已将其开源在https://github.com/nec-research/agentquest。**|
|**2024-04-15**|**AutoCodeRover: Autonomous Program Improvement**|Yuntong Zhang et.al.|[2404.05427](http://arxiv.org/abs/2404.05427)|**[link](https://github.com/nus-apr/auto-code-rover)**|**在过去几十年里，研究人员在自动化软件开发过程中取得了显著进展，尤其是大型语言模型（LLMs）的应用极大地推动了编程辅助的自动化。然而，软件工程并不仅仅是编码，还包括维护（如修复bug）和演化（如添加功能）等程序改进过程。本文提出了一种自动解决GitHub问题的方法，旨在实现程序自主改进。我们的方法称为AutoCodeRover，它结合了LLMs与高级代码搜索能力，最终生成程序修改或补丁。与AI研究者和从业者近期关注的仅文件级别的软件项目不同，我们的工作侧重于程序表示（抽象语法树），利用类/方法的程序结构来增强LLM对问题根本原因的理解，并通过迭代搜索提供上下文。当测试套件可用时，谱系基线故障定位技术进一步精确了上下文。  在SWE-bench-lite，一个包含300个真实GitHub问题的数据集上，AutoCodeRover的解决方案效果提升，解决了约22-23%的问题。对于全量的SWE-bench，包含2294个GitHub问题，AutoCodeRover解决了大约16%的问题，这比最近报道的来自Cognition Labs的AI软件工程师Devin的表现还要高，而且时间消耗与Devin相当。我们相信，我们的工作流程能够推动自主软件工程的发展，未来LLM自动生成的代码可以被自动地进行优化和改进。**|
|**2024-04-08**|**Long-horizon Locomotion and Manipulation on a Quadrupedal Robot with Large Language Models**|Yutao Ouyang et.al.|[2404.05291](http://arxiv.org/abs/2404.05291)|null|我们提出了一种基于大型语言模型（LLM）的系统，旨在提升四足机器人的问题解决能力，使其能够处理超越短期动作的长期任务。对于四足机器人来说，长期任务极具挑战性，因为它们需要对任务的语义有高层理解，并具备广泛的运动和操纵技能以与环境互动。我们的系统构建了一个高层推理层，利用大型语言模型，从任务描述中生成混合离散-连续的计划，作为机器人代码。它包括多个LLM代理：一个用于构思计划的语义规划器、一个参数计算器，用于预测计划中的参数，以及一个代码生成器，将计划转换为可执行的机器人代码。  在低层次，我们采用强化学习来训练一套运动规划和控制技能，以增强四足机器人的灵活性，使其能进行丰富环境交互。我们在难以用单一技能完成的长期任务上测试了我们的系统。模拟实验和真实世界实验表明，它成功地制定了多步骤策略，并展现出非平凡的行为，例如制作工具或向人类寻求帮助。|
|**2024-04-06**|**Autonomous Artificial Intelligence Agents for Clinical Decision Making in Oncology**|Dyke Ferber et.al.|[2404.04667](http://arxiv.org/abs/2404.04667)|null|多模态人工智能系统有望通过解析各类医学数据提升临床决策。然而，这些模型在各医学领域的效能尚不明朗，每个领域都有其独特挑战。本文提出了一种利用大型语言模型（LLMs）作为核心推理引擎的新型多模态医疗AI方法。此引擎自主协调并部署一系列专门的医疗AI工具，如文本解读、放射学和病理图像分析、基因数据处理、网络搜索以及医疗指南文档检索。我们在一系列临床肿瘤学场景中验证了该系统，这些场景模拟了典型的患者护理流程。结果显示，系统在选择恰当工具（97%）、得出正确结论（93.6%）、提供完整（94%）和有益（89.2%）治疗建议，以及根据指令引用相关文献（82.5%）方面表现出高能力。这表明LLMs能够有效地规划和执行领域特定模型，以获取或合成新信息，从而充当个性化临床助手。此外，这种架构简化了监管合规性，因为每个组件工具可以单独验证和审批。我们相信，这项工作为医疗领域的更先进LLM代理提供了概念验证。|
|**2024-04-05**|**Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents**|Harsh Kohli et.al.|[2404.04237](http://arxiv.org/abs/2404.04237)|null|大型语言模型（LLMs）的快速进步使其在标准基准测试中频频超越人类表现，推动了众多下游应用的发展，如基于LLMs的代理。然而，这些模型在看似简单的任务中意外地表现不佳，这强调了对更全面和多样化的评估框架的需求，以衡量它们的实际能力。为此，我们聚焦于组合性和条件推理——人类认知的基石，并提出GroundCocoa，这是一个与航班预订这一现实问题相连接的词汇丰富的基准。我们的任务是将用户的详细偏好与以多选形式提供的可用航班选项进行匹配。结果显示，包括最先进的GPT-4 Turbo在内的当前最佳模型，在经过高级提示后，准确率仍不超过67%，显示出显著的性能差距。|
|**2024-04-02**|**Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization**|Yoichi Ishibashi et.al.|[2404.02183](http://arxiv.org/abs/2404.02183)|**[link](https://github.com/tsukushiai/self-organized-agent)**|**## 背景  随着大型语言模型（LLM）代理的最新进展，自动化软件开发的未来正逐渐显现。然而，现有的单代理方法在生成和优化大规模、复杂的代码库时面临上下文长度限制的问题。为解决这一挑战，我们提出了一种新颖的多代理框架——自组织多Agent体系（SoA）。SoA是一个可扩展且高效的多代理系统，它允许独立地生成和修改代码组件，并协同构建整个代码库。SoA的一个关键特性是根据问题复杂性自动增加代理，实现动态可扩展性。这样，整体代码量可以根据代理数量无限增长，而每个代理管理的代码量保持恒定。  我们在HumanEval基准上评估了SoA，并发现与单代理系统相比，SoA中的每个代理处理的代码量明显减少，但总体生成的代码量显著增加。此外，SoA在Pass@1准确率方面比强大的单代理基线提高了5%。**|
|**2024-04-02**|**Helmsman of the Masses? Evaluate the Opinion Leadership of Large Language Models in the Werewolf Game**|Silin Du et.al.|[2404.01602](http://arxiv.org/abs/2404.01602)|**[link](https://github.com/doslim/evaluate-the-opinion-leadership-of-llms)**|**大型语言模型在社交推理游戏中展现出显著的策略行为，但对它们作为意见领袖的重要性关注不足，这对于多Agent和人机交互场景的实际应用至关重要。意见领袖是指在一个社会群体中对他人信念和行为有显著影响的个体。本研究使用“狼人杀”游戏作为模拟平台，探讨语言模型在扮演Sheriff（治安官）角色时的意见领导能力。Sheriff负责总结论点并提出决策建议，因此它代表了意见领袖的一个可信代理。我们构建了一个整合Sheriff角色的框架，并基于意见领袖的关键特性提出了两个评估指标：第一个衡量意见领袖的可靠性，第二个考察其对其他玩家决策的影响。  我们进行了大量实验，评估不同规模的语言模型，并创建了“狼人杀”问题回答数据集（WWQA），以测试和提升模型对游戏规则的理解。此外，还包含了人类参与者进行进一步分析。研究结果表明，“狼人杀”游戏是一个有效评估语言模型意见领导力的试验场，但目前仅有少数语言模型具备这种能力。**|
|**2024-04-15**|**CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs**|Jingzhe Shi et.al.|[2404.01343](http://arxiv.org/abs/2404.01343)|**[link](https://github.com/jingzheshi/chops)**|**随着企业和软件平台越来越多地采用大型语言模型（如GPT-3.5、GPT-4、GLM-3和LLaMa-2）提供聊天辅助或客户服务推理，现有的基于LLM的客户服务模型在与客户资料集成和执行实际操作方面存在局限。它们倾向于强调多样性而非精确性和错误避免，这对于现实世界的客户服务场景并不理想。因此，我们提出了一种名为CHOPS（结合客户资料的聊天助手）的LLM代理，旨在：（1）高效利用现有数据库或系统查询用户信息，或遵循既定指南与系统交互；（2）提供准确合理的响应并执行系统内的必要操作，同时避免有害操作；（3）通过结合小型和大型LLM以实现性能满意且成本合理的推理。  我们开发了一个实用的数据集，称为CPHOS-dataset，它包括一个数据库、指导文件以及来自CPHOS平台的模拟物理奥林匹克组织服务的问答对。CPHOS是一个面向高中教师和学生的在线平台。我们通过使用CPHOS-dataset进行了广泛的实验，验证了CHOPS架构的性能，目标是展示LLM如何提升或替代人工客户服务。关于我们的提案架构和数据集的代码可在此处获取：<https://github.com/JingzheShi/CHOPS>。**|
|**2024-03-31**|**DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model**|Lirui Zhao et.al.|[2404.01342](http://arxiv.org/abs/2404.01342)|**[link](https://github.com/opengvlab/diffagent)**|**文本到图像（T2I）生成模型近年来备受瞩目，在学术研究和实际应用中大放异彩。例如，Civitai平台，一个T2I创新的聚集地，目前汇集了74,492种独特的模型，这带来了选择最合适的模型和参数的艰巨任务，通常需要多次试验。借鉴大型语言模型（LLMs）工具使用研究的思路，我们推出了DiffAgent，这是一个通过API调用来快速筛选准确选项的LLM代理。DiffAgent采用了一种新颖的两阶段训练框架，称为SFTA，使其能够根据人类偏好精确地将T2I API的响应与用户输入对齐。为了训练和评估DiffAgent的能力，我们构建了DABench，这是一个全面的数据库，涵盖了社区中的各种T2I API。实验结果显示，DiffAgent不仅在选择适当的T2I API方面表现出色，还验证了SFTA训练框架的有效性。相关代码已可在https://github.com/OpenGVLab/DiffAgent获取。**|
|**2024-03-31**|**Algorithmic Collusion by Large Language Models**|Sara Fish et.al.|[2404.00806](http://arxiv.org/abs/2404.00806)|null|随着算法定价的兴起，人们担忧算法间的合谋问题。我们通过实验使用基于大型语言模型（LLMs）的定价代理，特别是GPT-4，进行了探究。研究发现：(1) LLM驱动的定价机制在定价任务上表现出色；(2) 在寡头竞争环境中，LLM定价代理会自发地进行合谋，从而损害消费者利益；(3) 对LLM指令（“提示”）看似微小的变化可能加剧这种合作行为。这些结果同样适用于拍卖场景。我们的研究结果强调了对算法定价进行反垄断监管的必要性，并揭示了针对LLM定价代理特有的监管挑战。|
|**2024-03-31**|**"My agent understands me better": Integrating Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents**|Yuki Hou et.al.|[2404.00573](http://arxiv.org/abs/2404.00573)|**[link](https://github.com/tamoharu/Agent-Memory-CHI24)**|在这个研究中，我们提出了一种创新的人类记忆架构，旨在提升基于大型语言模型的对话代理的认知能力。我们的设计使得这些代理能自主检索生成响应所需的必要记忆，从而解决LLMs在时间认知上的局限。我们借鉴了人类的记忆线索召回机制作为触发点，以实现精确且高效的回忆。此外，我们开发了一个数学模型，动态量化记忆巩固过程，考虑了诸如上下文相关性、时间流逝和回忆频率等因素。代理会从用户的交互历史中存储记忆，这些记忆被封装在数据库中，每个记忆都包含了内容和时间关联的语境。这样，通过类似人类识别和回忆过往经历的方式，系统能够战略性地存储记忆，并理解它们对用户在时间线上的重要性。|

<p align=right>(<a href=#updated-on-20240606>back to top</a>)</p>

## llm

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2024-06-04**|**Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks**|Tianyu He et.al.|[2406.02550](http://arxiv.org/abs/2406.02550)|**[link](https://github.com/ablghtianyi/ICL_Modular_Arithmetic)**|**这篇工作研究了大型语言模型在一组模块化算术任务中出现的上下文学习和技能组合现象。我们关注的是有限数量的一次性模运算函数 $z = a \times x + b \times y \;(\text{mod}\; p)$，这些函数由向量 $(a, b) \in \mathbb{Z}_p^2$ 标记。部分任务被用作预训练，其余用于分布外测试。实验表明，GPT风格的Transformer随着预训练任务数量增加，其在分布内和分布外的泛化能力会经历转变。最小型能实现分布外泛化的模型需要两个Transformer块；而对于更深的模型，分布外泛化阶段是“瞬态”的，需要早期停止。最后，我们对预训练模型进行了可解释性分析，揭示了两种阶段中高度结构化的表示，并讨论了学习到的算法。**|
|**2024-06-04**|**Leveraging Visual Tokens for Extended Text Contexts in Multi-Modal Learning**|Alex Jinpeng Wang et.al.|[2406.02547](http://arxiv.org/abs/2406.02547)|**[link](https://github.com/showlab/VisInContext)**|**这段研究并未介绍最先进的多模态大语言模型（MLLM），而是提出了一种创新方法，旨在有效提升长序列在多模态模型中的处理。我们提出了“Visualized In-Context Text Processing”（VisInContext）技术，通过视觉令牌来处理长文本，从而显著降低GPU内存使用和浮点运算（FLOPs）在训练和推理阶段的需求。例如，对于一个560亿参数的混合 Experts（MOE）模型，我们的方法将预训练中的上下文文本长度扩展到了2048个tokens，而计算量几乎保持不变。实验结果显示，使用VisInContext训练的模型在常见的基于实例的少量数据评估下游任务中表现出色。此外，VisInContext与现有技术相结合，能增强对文档的理解能力，特别适用于文档问答和连续文档检索，显示出巨大的潜力。**|
|**2024-06-04**|**To Believe or Not to Believe Your LLM**|Yasin Abbasi Yadkori et.al.|[2406.02543](http://arxiv.org/abs/2406.02543)|null|我们研究大型语言模型（LLMs）中的不确定性量化，目标是识别对给定查询的响应时的不确定性程度。我们同时考虑了两种类型的不确定性：一种是知识性不确定性（例如对事实或语言真理的未知），另一种是不可消除的随机性（如可能的答案多样性）。特别是，我们提出了一种信息论指标，能够可靠地区分出只有知识性不确定性较大的情况，这时模型的输出是不可靠的。这个条件仅依赖于通过特殊迭代提示基于先前响应得到的模型输出来计算。这种量化方法可以检测单答和多答情况下是否存在虚构（即知识性不确定性高）的情况，这与许多标准的不确定性量化策略（如以响应的对数似然性作为阈值）不同，后者无法识别多答情况下的虚构。  我们进行了一系列实验，展示了我们的方法的优势。此外，我们的研究还揭示了LLM如何通过迭代提示放大对给定输出的概率分配，这可能具有独立的兴趣价值。|
|**2024-06-04**|**Loki: Low-Rank Keys for Efficient Sparse Attention**|Prajwal Singhania et.al.|[2406.02542](http://arxiv.org/abs/2406.02542)|null|针对大型语言模型的推理计算成本高昂，特别是当使用长序列时，自注意力机制是主要开销。为了解决这个问题，近期的研究提出了一些稀疏注意力近似方法。本文中，我们通过分析发现，注意力块中的键向量实际上处于一个远低于原始维度的空间。这一观察促使我们提出Loki，一种新的稀疏注意力方法。Loki根据在低维空间计算的注意力得分，对KV缓存中的令牌进行排序和选择。实验结果表明，Loki能够比其他流行近似方法更好地保持模型的效能，同时由于减少了数据移动（加载/存储）和计算成本，加速了注意力计算。|
|**2024-06-04**|**Parrot: Multilingual Visual Instruction Tuning**|Hai-Long Sun et.al.|[2406.02539](http://arxiv.org/abs/2406.02539)|null|随着GPT-4V等多模态大型语言模型的快速发展，人工智能朝着通用人工智能迈出了重要一步。当前的方法主要依赖于监督微调（SFT）来同步视觉编码器与语言模型，从而赋予它们多模态能力。然而，这种做法可能导致随着训练的进行，语言模型处理多种语言的能力逐渐减弱。我们发现，以英语为中心的不平衡SFT数据集会导致非英语语言性能显著下降，原因在于SFT过程中未能有效连接视觉编码器和多语言令牌。为此，我们提出Parrot，一种利用文本引导在语言层面驱动视觉令牌对齐的新方法。Parrot通过让视觉令牌根据不同的语言输入进行条件化，并借助混合专家（MoE）促进多语言令牌的对齐。特别是，为了增强非英语视觉令牌的对齐，我们计算初始视觉特征与文本嵌入之间的跨注意力，然后将其输入到MoE路由器，选择最相关的专家。选定的专家会将初始视觉令牌转化为特定语言的视觉令牌。鉴于目前缺乏评估多语言能力的标准基准，我们还创建并公开了一个大规模多语言多模态基准（MMMB），包括6种语言、15个类别和12,000个问题。Parrot不仅在MMMB和MMM Benchmark上展现出最先进的性能，还在广泛的多模态任务中表现出色。我们将提供Parrot的源代码和训练数据集供公众使用。|
|**2024-06-04**|**Mitigate Position Bias in Large Language Models via Scaling a Single Dimension**|Yijiong Yu et.al.|[2406.02536](http://arxiv.org/abs/2406.02536)|null|这篇论文主要探讨了大型语言模型（LLMs）在实际应用中的一个现象——位置偏见，也称为"迷失在中间"。这种偏见在长文本情境中尤为明显，即关键信息在提示中的不同位置会显著影响模型的准确性。研究发现，注意力权重是位置偏见的微观表现。此外，论文指出，因果注意力掩码通过创建位置特定的隐藏状态，也对位置偏见有所贡献。  基于这些洞察，作者提出了一种方法来减轻位置偏见，即调整这些位置特定的隐藏状态。实验在多个任务上进行，包括自然问题多文档问答、键值检索、LongBench和时间线重排，涉及RoPE模型、扩展上下文窗口模型和Alibi模型等多种架构。结果显示，我们的方法通过仅修改隐藏状态的一个维度，就能实现性能提升，最高可达15.2%。研究者还提供了代码供进一步使用，代码地址为：https://aka.ms/PositionalHidden。|
|**2024-06-04**|**SpecExec: Massively Parallel Speculative Decoding for Interactive LLM Inference on Consumer Devices**|Ruslan Svirschevski et.al.|[2406.02532](http://arxiv.org/abs/2406.02532)|null|随着大型语言模型的广泛应用，高效运行它们变得至关重要。近期的研究通过推测性解码实现了显著的速度提升。然而，大多数工作都是针对数据中心硬件进行设计。本研究反问：我们能在消费级设备上多快地运行LLMs？消费者级GPU已无法容纳最大的模型（500亿参数以上），因此需要将参数卸载到RAM或SSD。当使用卸载参数的方式运行时，推理引擎可以同时处理数百乃至数千个令牌的批次，使其非常适合推测性解码。我们提出SpecExec（推测性执行），这是一种简单的并行解码方法，适用于主流LLM家族，能生成每轮目标模型迭代高达20个令牌的预测。它利用现代LLMs中概率分布的高波动性和模型输出概率之间的高度一致性。SpecExec通过从草稿模型获取最可能的令牌延续，构建一个目标模型的“缓存”树，然后在一个单次遍历中验证。  使用SpecExec，我们在消费级GPU上实现了500亿参数LLM的推理，配合RAM卸载，4位量化下的速度达到4-6个令牌/秒，而16位权重下的速度为2-3个令牌/秒。|
|**2024-06-04**|**Scalable MatMul-free Language Modeling**|Rui-Jie Zhu et.al.|[2406.02528](http://arxiv.org/abs/2406.02528)|**[link](https://github.com/ridgerchu/matmulfreellm)**|**## 翻译  在大型语言模型（LLMs）中，矩阵乘法（MatMul）通常占据主要计算开销。随着LLMs的规模扩大，其嵌入维度和上下文长度也随之增加，这一问题更为显著。本文提出了一种方法，能够在保持强大性能的同时，完全移除LLMs中的MatMul操作，即使是在27亿参数量级的模型上也能实现。实验表明，我们的无MatMul模型在与内存消耗显著更多的状态-of-the-artTransformer相当的条件下表现出色。我们研究了模型的扩展性规律，并发现无MatMul模型与全精度Transformer之间的性能差距随着模型尺寸增大而减小。  此外，我们提供了一个高效的GPU实现，相较于未优化的基线，训练时能减少高达61%的内存使用。在推理阶段，通过优化的内核，我们的模型内存消耗可降低超过10倍。为了准确评估架构效率，我们在FPGA上构建了定制硬件解决方案，利用GPU无法处理的轻量级运算，实现了对十亿参数规模模型的高速处理，使其接近人脑级别的效率。  这项工作不仅展示了LLMs在减小复杂性后仍能保持高效，还指出了未来加速器应优化的运算类型，以适应下一代轻量级LLMs的需求。我们的代码实现已开源至：\url{https://github.com/ridgerchu/matmulfreellm}。**|
|**2024-06-04**|**CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks**|Maciej Besta et.al.|[2406.02524](http://arxiv.org/abs/2406.02524)|null|大型语言模型（LLMs）正在各个领域带来变革，但验证其答案仍然是一个重大挑战，尤其是在处理复杂、开放性的任务，如知识整合、摘要和提取。本文提出了一种名为CheckEmbed的精确、可扩展且简便的LLM验证方法。CheckEmbed的核心理念是：通过利用如GPT文本嵌入大模型获取的答案级嵌入来比较LLM的回答。这将复杂的文本答案转化为单一的嵌入，简化了对比过程，实现快速而有意义的验证。我们构建了一个全面的验证管道，该管道实现了CheckEmbed的理念，并提供了评估LLM答案真实性的度量，如嵌入热力图及其总结。我们展示了如何利用这些指标设计实际的引擎，以决定LLM答案是否令人满意。在实际文档分析任务中，如术语提取和文档摘要，我们的方法表现出显著的准确性提升、成本效益和运行时间性能，相较于BERTScore或SelfCheckGPT等基于token、句子和事实级别的方案。|
|**2024-06-04**|**RoboCasa: Large-Scale Simulation of Everyday Tasks for Generalist Robots**|Soroush Nasiriany et.al.|[2406.02523](http://arxiv.org/abs/2406.02523)|null|## 翻译  人工智能的最新进展在很大程度上依赖于规模的扩大。然而，在机器人领域，大规模机器人数据集的获取是一个瓶颈。我们主张利用逼真的物理模拟来提升环境、任务和数据集的规模，以支持机器人学习方法。为此，我们介绍RoboCasa，这是一个大型的仿真框架，旨在训练能够在日常环境中通用的机器人。RoboCasa的特点是拥有丰富且多样化的厨房场景，包括超过150个类别的一千多件3D模型资产和数十种可交互的家具和电器。  我们通过生成式AI工具进一步增强模拟的真实性和多样性，如使用文本到3D模型的技术生成对象资产，以及通过文本到图像模型生成环境纹理。我们设计了100项任务，包括由大型语言模型指导的复合任务，用于系统性评估。为了促进学习，我们提供了高质量的人类演示，并结合自动轨迹生成方法，以最小的人力成本大幅扩充数据集。  我们的实验表明，在使用合成生成的机器人数据进行大规模模仿学习时，存在明显的规模效应，并显示出利用模拟数据在现实世界任务中的巨大潜力。相关视频和开源代码已在https://robocasa.ai/网站上提供。|
|**2024-05-31**|**Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis**|Chaoyou Fu et.al.|[2405.21075](http://arxiv.org/abs/2405.21075)|null|在人工智能的追求中，多模态大型语言模型（MLLMs）已成为近期进步的核心。然而，对它们处理序列视觉数据的能力的关注尚显不足。为此，我们在本文中提出Video-MME，这是首个全面评估MLLMs在视频分析性能的多模态评估基准。我们的工作有四个关键特性：1）视频类型多样，涵盖6个主要视觉领域和30个子领域，确保广泛的应用场景泛化能力；2）时间维度的跨度，包括短、中、长期视频，从11秒到1小时，以检验模型对复杂情境动态的适应性；3）数据模态的广度，结合视频帧以外的多种输入，如字幕和音频，揭示MLLMs的全方位能力；4）高质量的标注，由专家严格手动标记，以保证精确且可靠的模型评估。我们精心挑选并手动注解了900段视频，总时长达到256小时，生成了2,700个问题-答案对。通过Video-MME，我们对包括GPT-4系列、Gemini 1.5 Pro在内的多个最先进的MLLM，以及开源图像模型InternVL-Chat-V1.5和视频模型LLaVA-NeXT-Video进行了深入评估。实验结果显示，Gemini 1.5 Pro是表现最佳的商业模型，明显优于开源模型。我们的数据集和发现强调了改进处理更长序列和多模态数据的必要性。项目网页链接：https://video-mme.github.io|
|**2024-05-31**|**Grammar-Aligned Decoding**|Kanghee Park et.al.|[2405.21047](http://arxiv.org/abs/2405.21047)|null|大型语言模型（LLMs）在生成高度结构化的输出时面临挑战，如程序代码、数学公式或规范的标记。约束解码方法通过限制每次输出可能的令牌，确保输出符合特定规则来缓解这个问题，例如在语法约束解码（GCD）中，LLM的输出必须遵循给定的语法规则。然而，研究表明，这种约束解码可能会扭曲模型的分布，导致生成的输出虽然语法正确，但其概率并不直接反映LLM本身的概率分配，从而质量不高。我们称之为“与语法约束对齐的解码”（Grammar-Aligned Decoding，GAD），并提出了一种名为“自适应采样与近似期望未来”（Adaptive Sampling with Approximate Expected Futures，ASAp）的解码算法。  ASAp算法旨在保证输出的语法性，并理论上产生与LLM在给定语法约束条件下的条件概率相符的结果。该算法利用先前的样本输出来稳健地估算不同输出前缀的未来语法可能性。我们在代码生成和结构化自然语言处理任务上的实验表明，ASAp经常能够生成比现有GCD技术更符合LLM分布且仍遵守所需语法限制的输出，从而提高了整体质量。|
|**2024-05-31**|**Direct Alignment of Language Models via Quality-Aware Self-Refinement**|Runsheng Yu et.al.|[2405.21040](http://arxiv.org/abs/2405.21040)|null|强化学习从人类反馈（RLHF）是调整大型语言模型（LLMs）行为以符合人类偏好的常用方法。最近，直接策略优化（DPO）作为一种替代方案兴起，它不再依赖LLM奖励模型，从而减少了额外的内存和训练时间。然而，DPO忽视了正向和负向响应的相对质量，可能导致训练结果不理想。为解决这个问题，我们探讨利用LLM内部知识在即时微调过程中获取响应的质量，并优化损失函数。我们设计了一种细化函数，利用LLM的知识来估计正向和负向响应的品质。实验表明，在轻度假设下，构建的细化函数能够帮助自我调整损失函数。我们将这个细化功能整合到DPO及其变体身份策略优化（IPO）中。实验证明，这些改进后的模型在各种评估者上表现出优于DPO和IPO的性能。|
|**2024-05-31**|**Standards for Belief Representations in LLMs**|Daniel A. Herrmann et.al.|[2405.21030](http://arxiv.org/abs/2405.21030)|null|随着大型语言模型（LLMs）在各个领域展现出非凡能力，计算机科学家们正在寻求理解它们的认知过程，特别是关于LLMs如何（如果有的话）内部构建对世界的信念。然而，目前尚缺乏一个统一的理论框架来支撑对LLM中信念的研究。本文试图填补这一空白，提出了一套条件，使LLM中的表示能够被视为信念似的。我们指出，尽管在LLMs中测量信念的项目与决策理论和形式认识论中的信念测量在许多方面有相似之处，但也存在差异，这些差异应影响我们的测量方法。因此，借鉴哲学洞察和机器学习的当代实践，我们确立了四个标准：准确性、一致性、统一性和实用性。这四个标准结合了理论考量与实际限制，为全面理解LLM中的信念表示奠定了基础。我们引用实证工作的成果，揭示了单独使用某些标准时识别信念表示的局限性。|
|**2024-05-31**|**LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models**|Elias Stengel-Eskin et.al.|[2405.21028](http://arxiv.org/abs/2405.21028)|**[link](https://github.com/esteng/pragmatic_calibration)**|**当回答问题时，语言模型不仅能提供答案，还能传达对答案正确性的信心程度。这包括明确的分数标记，如给出数字，以及隐含的信心标志，如权威语气或提供额外知识。然而，当前大多数模型往往过于自信。为了校准这些信心度，我们提出了一种实用的、考虑听众的微调方法（LACIE），它不仅关注答案是否正确，还关注答案是否会被听众接受。我们将校准视为偏好优化，通过双代理游戏创建数据，让一个演讲者模型的输出接受模拟听者的评判。然后，我们使用LACIE对三个语言模型（Mistral-7B、Llama3-8B和Llama3-70B）进行微调，并显示经过微调的模型在模拟听者面前有更好的校准。重要的是，这些趋势也适用于人类听众，帮助他们更准确地预测模型的正确性：我们在人机评估中发现，经过LACIE训练的模型接受的错误答案减少了47%，而正确答案的接受率保持不变。此外，LACIE泛化到另一个数据集上，在使用TriviaQA训练后，TruthfulQA上的真实性大幅提高。我们的分析表明，LACIE导致了正确和错误示例之间的信心度更好地分离。定性上，我们发现经过LACIE训练的模型会更加谨慎，并在回答正确时通过使用权威语气或提供细节来隐性地表示确定性。最后，LACIE微调导致模型对于可能错误的答案更倾向于放弃（例如说“我不知道”）。**|
|**2024-05-31**|**Improved Techniques for Optimization-Based Jailbreaking on Large Language Models**|Xiaojun Jia et.al.|[2405.21018](http://arxiv.org/abs/2405.21018)|**[link](https://github.com/jiaxiaojunqaq/i-gcg)**|**随着大型语言模型（LLMs）的快速发展，其安全校准成为广泛应用的关键。针对这些模型的破解（即“jailbreaking”）活动日益增多，其中贪婪坐标梯度（GCG）攻击因其成效显著而受到关注。然而，GCG的攻击效率仍有提升空间。本文提出了一系列改进的优化基线破解技术，以提升GCG的性能。首先，我们注意到单个目标模板“Sure”极大地限制了GCG的攻击效果，因此我们建议采用包含有害自我暗示和/或指导的多样化目标模板，以误导模型。在优化策略上，我们建议在GCG中实施自动多坐标更新，以加速收敛，并引入从简单到复杂（easy-to-hard）的初始化技巧。将这些改进整合，我们开发出一种高效的方法—— $\mathcal{I}$ -GCG。实验在一系列基准测试，如NeurIPS 2023 红队挑战中进行，结果显示，我们的改进技术能够帮助GCG超越现有破解攻击，实现接近100%的攻击成功率。代码已发布在https://github.com/jiaxiaojunQAQ/I-GCG。**|
|**2024-05-31**|**DeCo: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models**|Linli Yao et.al.|[2405.20985](http://arxiv.org/abs/2405.20985)|null|该研究关注于多模态语言模型（MLLMs）中的投影器模块，因为它们在连接视觉和语言模态、促进跨模态对齐方面发挥关键作用。然而，目前对于投影器在视觉-语言对齐方面的效果评估仍显不足，通常只能通过下游任务的性能间接推断。为此，本研究通过分析MLLM中的视觉-语言语义流，来解读投影器的工作机制。  具体来说，研究者追踪从生成的语言标记到原始视觉编码块以及投影器产生的中间输出之间的语义相关性流。发现压缩型投影器（如QFormer）倾向于将视觉块抽象成有限的几个概念，如物体或属性，导致“双重抽象”现象：首先，投影器参照预定义查询令牌进行视觉语义抽象，然后，基于文本指令的大语言模型进一步提取。这种双重抽象在训练过程中效率不高，并可能导致视觉语义信息的累积缺失。  为解决这个问题，研究提出“解耦压缩与抽象（DeCo）”的关键洞察，即在投影层面上将视觉令牌数量压缩，而让大语言模型完全负责视觉语义抽象。因此，研究人员采用了一种简单的压缩器——二维自适应池化，以无参数的方式降低视觉块的尺寸。实验结果显示，DeCo在性能和效率上都优于传统的压缩投影器。它在MLLM基准、视觉定位和开放性视觉问答任务中分别取得了0.9%、7.1%和2.9%的性能提升，同时拥有更少的可训练参数和更快的收敛速度。|
|**2024-05-31**|**Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training**|Feiteng Fang et.al.|[2405.20978](http://arxiv.org/abs/2405.20978)|null|大型语言模型（LLMs）展现出强大功能，但面临挑战，如虚构、过时知识和难以追溯的推理过程。为解决这些问题，检索增强生成（RAG）作为一种有前景的方法崭露头角，它结合外部数据库的知识。然而，不适当的检索段落可能妨碍LLMs生成全面且高质量的回答。先前关于RAG中检索噪声稳健性的研究往往局限于有限的噪声类型，这与现实世界的检索环境不符，限制了实际应用。本研究首先探讨了检索噪声，并将其分为三种不同的类别，反映真实环境。我们分析了这些不同类型的检索噪声对LLMs稳健性的影响。  接着，我们提出了一种新颖的RAG方法，称为检索增强自适应对抗训练（RAAT）。RAAT利用自适应对抗训练来动态调整模型的训练流程以应对检索噪声，并采用多任务学习确保模型能够识别嘈杂的上下文。大量的实验表明，在各种噪声条件下，使用RAAT训练的LLaMA-2 7B模型在F1和EM分数上显示出显著提升。为了便于复现，我们已在https://github.com/calubkk/RAAT上发布了我们的代码和数据。|
|**2024-05-31**|**SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales**|Tianyang Xu et.al.|[2405.20974](http://arxiv.org/abs/2405.20974)|**[link](https://github.com/xu1868/sayself)**|**大型语言模型（LLMs）常常产生不准确或虚假的信息，并且通常无法表明其信心水平，这限制了它们的广泛应用。先前的研究试图通过直接提示或自我一致性提示来提取LLMs的信心，或者构建特定数据集进行监督微调。基于提示的方法性能较差，而基于训练的方法又局限于二元或不精确的整体信心估计。本文提出了一种先进的方法——SaySelf，这是一个训练框架，旨在教导LLMs提供更精确的细粒度信心估计。  此外，SaySelf还推动LLMs生成自我反思的解释，明确指出它们在参数知识上的空白并解释不确定性。这是通过让LLM以自然语言的形式自动总结特定知识中的不确定性来实现的。这种总结是基于对多个采样推理链的不一致性分析，生成的数据用于监督微调。为了进一步校准信心估计，我们采用了精心设计的强化学习，奖励准确、高置信度的预测，同时惩罚错误输出中的过度自信。  实验结果表明，无论是在分布内还是分布外的数据集上，SaySelf都能有效减少信心校准误差，同时保持任务性能。生成的自我反思理由也被证明是合理的，能进一步促进校准。代码已公开在：\url{https://github.com/xu1868/SaySelf}。**|
|**2024-05-31**|**LCQ: Low-Rank Codebook based Quantization for Large Language Models**|Wen-Pu Cai et.al.|[2405.20973](http://arxiv.org/abs/2405.20973)|null|## 背景  大型语言模型（LLMs）在众多任务上展现出优异性能，但它们的存储和计算成本高成为部署的一大挑战。为了压缩模型并降低成本，权重量化技术被广泛应用。目前，大多数针对LLMs的量化方法使用秩一码本，然而在高压缩比下，这会导致显著的精度损失。本文提出了一种新颖的权重量化方法，称为低秩码本量化（LCQ），旨在解决这一问题。  ## 方法  LCQ采用低秩码本进行量化，其秩可以大于一。这种方法旨在通过利用更高的秩来保持或提升模型的精度，同时控制额外的存储开销几乎为零。实验表明，与现有方法相比，LCQ在保持良好准确性的前提下，能够实现更优的压缩效果。  ## 结论  综上所述，本文介绍了一种创新的低秩码本量化方法，它有望在不显著增加存储成本的情况下，提升大型语言模型在实际应用中的性能和效率，为高效部署这些模型提供了新的解决方案。|
|**2024-05-30**|**MotionLLM: Understanding Human Behaviors from Human Motions and Videos**|Ling-Hao Chen et.al.|[2405.20340](http://arxiv.org/abs/2405.20340)|null|这项研究关注于多模态（视频和动作模态）下的人类行为理解，通过大型语言模型（LLMs）的强大功能。与专为单模态（视频或动作）设计的最新LLMs不同，我们认为理解人类行为需要对视频和动作序列（如SMPL序列）进行联合建模，以有效捕捉精细的身体部位动态和语义。为此，我们提出MotionLLM，这是一个简洁而有效的框架，用于人类动作理解、描述和推理。MotionLLM采用了一体化的视频-动作训练策略，利用现有粗粒度的视频-文本数据和精细动作-文本数据的优势，以获取丰富的空间-时间洞察。此外，我们还创建了一个大规模的MoVid数据集，包含了多样化的视频、动作、caption和指令。我们还提出了MoVid-Bench，它具有精心的手动标注，以更好地评估在视频和动作上的人类行为理解能力。实验结果充分展示了MotionLLM在caption生成、空间-时间理解以及推理能力方面的优越性。|
|**2024-05-30**|**Visual Perception by Large Language Model's Weights**|Feipeng Ma et.al.|[2405.20339](http://arxiv.org/abs/2405.20339)|null|这篇论文的背景是现有的多模态大型语言模型（MLLMs）采用了一种方法，即将视觉信息与语言模型的输入空间对齐，然后将视觉令牌与文本令牌合并，形成统一的序列输入给语言模型。然而，这种方法由于增加了由视觉令牌导致的输入序列长度，计算成本较高。为此，论文提出了一种新颖的参数空间对齐范式，通过将视觉信息表示为模型权重来处理。对于每个输入图像，首先使用视觉编码器提取特征，然后将这些特征转换为感知权重，并将其与语言模型的权重融合。这样，语言模型的输入无需视觉令牌，从而缩短了输入序列，显著提高了效率。  基于这一理念，论文提出了VLoRA模型，其中包含一个感知权重生成器。该生成器设计成能够将视觉特征转化为具有低秩特性的感知权重，类似于LoRA（低秩自适应训练）。实验结果表明，尽管VLoRA在多种多模态任务的基准上表现出与现有MLLMs相当的性能，但其在训练和推理阶段的计算成本显著降低。论文承诺开源代码和模型。|
|**2024-05-30**|**Xwin-LM: Strong and Scalable Alignment Practice for LLMs**|Bolin Ni et.al.|[2405.20335](http://arxiv.org/abs/2405.20335)|**[link](https://github.com/xwin-lm/xwin-lm)**|**本文介绍Xwin-LM，一个专为大型语言模型（LLMs）设计的全面对齐方法套件。它涵盖了监督微调（SFT）、奖励建模（RM）、拒绝采样微调（RS）和直接偏好优化（DPO）等多种关键技术。主要组成部分包括：(1) 使用高质量指令数据进行初始微调的Xwin-LM-SFT；(2) 由GPT-4精心标注的大型多轮偏好数据集Xwin-Pair；(3) 在7B、13B和70B参数规模上训练的Xwin-RM奖励模型；(4) 每个提示关联64个独特响应的多wise偏好数据集Xwin-Set，这些响应由Xwin-LM-SFT生成并由Xwin-RM评分；(5) 使用Xwin-Set中最高得分响应进行微调的Xwin-LM-RS模型；(6) 通过DPO算法在Xwin-Set上进一步优化的Xwin-LM-DPO模型。我们在AlpacaEval和MT-bench上的评估显示了整个管道的稳定且显著改进，证明了Xwin-LM的强大和可扩展性。我们将在https://github.com/Xwin-LM/Xwin-LM的仓库中持续更新，以促进社区研究。**|
|**2024-05-31**|**ParSEL: Parameterized Shape Editing with Language**|Aditya Ganeshan et.al.|[2405.20319](http://arxiv.org/abs/2405.20319)|null|本文提出了一种名为ParSEL的系统，它旨在通过自然语言实现高质量3D资产的可控编辑。面对自然语言在精确操控上的局限性，ParSEL接收一个分割的3D网格和编辑请求，生成一个参数化的编辑程序。用户可以调整程序参数，精细地探索形状变化，控制编辑幅度。系统利用大型语言模型（LLMs）来理解初始编辑指令，但发现它们在推断完整编辑程序时常常不足，产生的结果可能违反形状逻辑。为此，我们设计了分析性编辑传播（Analytical Edit Propagation，AEP）算法，它从初始编辑种子开始，通过计算机代数系统进行几何分析，寻找与潜在用户编辑兼容的分析性编辑操作，以生成完整的编辑程序。实验表明，相较于其他方案，ParSEL通过自然语言请求有效地实现了对3D对象的可控编辑。|
|**2024-05-30**|**CausalQuest: Collecting Natural Causal Questions for AI Agents**|Roberto Ceraolo et.al.|[2405.20318](http://arxiv.org/abs/2405.20318)|**[link](https://github.com/roberto-ceraolo/causal-quest)**|**人类天生就有寻求因果关系的驱动力，无论是出于好奇心还是特定目标。为了开发能处理这种人类本性追求的AI代理，我们急需一个全面的自然因果问题数据集。然而，现有的数据集要么包含人工制造的问题，无法反映实际AI应用场景，要么在特定来源的问题覆盖上有限。为此，我们提出了CausalQuest，这是一个源自社交网络、搜索引擎和AI助手的13,500个自然出现的问题的数据集。我们定义了因果问题，并建立了更细致的分类体系。通过人类标注员和大型语言模型的协作，我们对数据集进行了精心标注。研究发现，42%的人类提问实际上是关于因果的，大部分是想了解给定结果背后的原因。利用这个数据集，我们训练了高效的二分类器（高达28.5亿参数），用于识别因果问题，实现了高性能，F1分数高达0.877。最后，我们提出了一系列丰富的未来研究方向，这些都可以基于我们的数据和模型进行扩展。**|
|**2024-05-30**|**ANAH: Analytical Annotation of Hallucinations in Large Language Models**|Ziwei Ji et.al.|[2405.20315](http://arxiv.org/abs/2405.20315)|**[link](https://github.com/open-compass/anah)**|**### 背景  大型语言模型（LLMs）的“幻觉”问题对于其广泛应用至关重要。然而，对这一问题的细致测量在社区中并未得到充分探索。为此，我们提出了一项名为 $\textbf{ANAH}$ 的双语数据集，专注于生成式问答中的LLM幻觉分析。ANAH中的每个答案句子都经过严谨标注，包括参考片段检索、幻觉类型的判断以及错误内容的修正。该数据集包含约12,000个句级注释，涵盖了大约4,300个LLM响应，涉及超过700个主题，通过人机交互式流程构建而成。由于幻觉注释的精细粒度，我们可以定量确认LLMs的幻觉问题随着答案的扩展而逐渐增加，并利用ANAH来训练和评估幻觉标注器。  ### 任务  我们构建了大约12,000条句子级别的注释，针对约4,300个LLM生成的回答，涵盖了超过700个主题。这个名为ANAH的数据集通过人类参与的流程精心设计，旨在提供关于生成式问答中LLMs幻觉的详尽分析。通过细致的幻觉标注，我们能够量化地验证LLMs在生成答案时幻觉问题的累积，并利用ANAH来训练和评估幻觉识别能力。我们的实验深入研究了生成式和区分性标注器，并发现尽管开源LLMs在精细幻觉标注方面面临挑战，但使用ANAH训练的生成式标注器能够超越所有开源模型，甚至接近GPT-3.5的表现，并展现出在未见过问题上的良好泛化能力。**|
|**2024-05-30**|**Sequence-Augmented SE(3)-Flow Matching For Conditional Protein Backbone Generation**|Guillaume Huguet et.al.|[2405.20313](http://arxiv.org/abs/2405.20313)|null|蛋白质在几乎所有的生物过程中发挥关键作用，其多样化的功能源于复杂的三维结构，而这些结构又由氨基酸序列决定。在这篇论文中，我们利用氨基酸序列丰富的生物学归纳偏置，提出了一种新的序列条件的SE(3)等变流匹配模型——FoldFlow-2，用于蛋白质结构生成。与FoldFlow家族的先前模型相比，FoldFlow-2引入了新颖的架构特性，包括用于编码序列的蛋白质大语言模型、结合结构和序列表示的新多模态融合主干，以及基于几何变换器的解码器。为了增加生成样本的多样性和新颖性——这对新药设计至关重要——我们在比先前工作使用的PDB数据集大一个数量级的新数据集上大规模训练FoldFlow-2，该数据集包含了已知的PDB蛋白质和通过过滤获得的高质量合成结构。此外，我们展示了如何通过引入强化微调（Reinforced Finetuning，简称ReFT）目标，使FoldFlow-2能够适应任意奖励，如提高二级结构多样性。  实验结果表明，FoldFlow-2超越了现有基于蛋白质结构的生成模型的状态，无论在无条件生成还是在设计性、多样性和新颖性方面，都优于RFDiffusion，且在蛋白质长度的各类任务上表现出良好的泛化能力，特别是在等温构象采样任务上。最后，我们展示了一个经过微调的FoldFlow-2在诸如VHH纳米抗体骨架设计等具有挑战性的条件设计任务上取得了进展。|
|**2024-05-30**|**Large Language Models Can Self-Improve At Web Agent Tasks**|Ajay Patel et.al.|[2405.20309](http://arxiv.org/abs/2405.20309)|null|在复杂的环境中，如网络浏览器，训练模型作为能够有效导航和执行动作的代理通常具有挑战性，主要受限于缺乏训练数据。近年来，大型语言模型（LLMs）显示出通过自然语言提示以零样本或少量样本来在新环境中导航的能力。研究还表明，LLMs可以通过自我改进（即在其自身生成的数据上微调）来超越基础性能。本研究旨在探究LLMs在长时序任务的复杂环境——WebArena基准中，通过自我改进能否提升其表现。WebArena要求代理自主浏览网页并执行操作以达成特定目标。我们使用三种不同的合成训练数据混合进行微调，并发现经过自我改进后，模型在WebArena基准上的任务完成率提高了31%。此外，我们还提出了新的评估指标，用于更全面地评估我们的微调代理模型的行为性能、鲁棒性、能力以及轨迹质量，这些指标超越了当前仅依赖于整体基准分数的评估方式。|
|**2024-05-30**|**Group Robust Preference Optimization in Reward-free RLHF**|Shyam Sundhar Ramesh et.al.|[2405.20304](http://arxiv.org/abs/2405.20304)|**[link](https://github.com/rsshyam/Group-robust-preference-optimization)**|**## 翻译  针对大型语言模型（LLMs）的特定任务进行适应时，通常需要通过基于人类反馈的强化学习（RLHF）和多元标签者群体（如不同性别、种族、公司团队等）的偏好数据进行微调。然而，传统方法倾向于采用“一刀切”的策略，即假设并优化单一的偏好模型，对各群体的独特特性和需求不够敏感。为此，我们提出了一种新颖的群体鲁棒偏好优化（GRPO）方法，旨在稳健地使LLMs适应各个群体的偏好。GRPO方法基于无奖励直接偏好优化，但区别于以往，它目标是寻找一个能最大化最差群体性能的鲁棒策略。为了实现这一目标，GRPO会动态且逐次调整不同群体的权重，优先关注累积损失较高的群体。我们在理论上探讨了GRPO的可行性，并分析了其在对数线性策略类别下的收敛性。通过使用来自不同群体的全局意见数据对LLMs进行GRPO微调，我们显著提高了最差群体的表现，减少了群体间损失的不平衡，同时提高了概率准确性，相较于非鲁棒基线，这些改进效果显著。**|
|**2024-05-30**|**Who Writes the Review, Human or AI?**|Panagiotis C. Theocharopoulos et.al.|[2405.20285](http://arxiv.org/abs/2405.20285)|null|随着人工智能在自然语言处理中的广泛应用，人们关注如何识别不同领域的AI生成文本。本研究旨在探讨这个问题，通过提出一种方法来准确区分人工智能生成的和人类撰写的书评。我们的方法利用迁移学习，让模型能够在不同主题间识别生成文本，同时提高其识别写作风格和词汇变化的能力。我们构建了一个数据集，包含真实的书评和使用Vicuna开源语言模型生成的模拟评论，以评估所提方法的有效性。实验结果显示，识别文本原创来源是可行的，准确率达到96.86%。我们的工作聚焦于大型语言模型在文本识别方面的性能与局限性研究，这对于未来有效管理此类模型以及确保人类创作内容的完整性和真实性具有重要意义。|
|**2024-05-29**|**X-VILA: Cross-Modality Alignment for Large Language Model**|Hanrong Ye et.al.|[2405.19335](http://arxiv.org/abs/2405.19335)|null|我们提出X-VILA，一种旨在增强大型语言模型（LLMs）功能的多模态模型，它融合了图像、视频和音频模态。通过将各模态特定的编码器与LLM输入对齐，并将扩散解码器与LLM输出对齐，X-VILA实现了跨模态理解、推理和生成。为了支持这种跨模态对齐，我们开发了一个有效的任意模态指令跟随数据集。然而，我们发现当前的跨模态对齐方法存在一个关键问题，导致视觉信息丢失。为此，我们设计了视觉对齐机制，包括一个视觉嵌入高速公路模块，以解决这一问题。此外，我们还提供了一种资源高效的训练策略，使得X-VILA在任意模态对话任务上表现出色，大幅超越先前的方法。令人惊讶的是，即使在缺乏类似训练数据的情况下，X-VILA在不同模态间也展现出涌现特性。该项目将开源。|
|**2024-05-29**|**LLMs Meet Multimodal Generation and Editing: A Survey**|Yingqing He et.al.|[2405.19334](http://arxiv.org/abs/2405.19334)|**[link](https://github.com/yingqinghe/awesome-llms-meet-multimodal-generation)**|**随着大型语言模型（LLMs）的最新进展，人们越来越关注将它们与多模态学习相结合。当前的多模态大语言模型（MLLMs）调查主要集中在理解上。这篇综述详细探讨了跨图像、视频、3D和音频等领域的多模态生成，特别强调了这些领域中的里程碑式工作及其技术进步。我们深入研究了这些方法的关键技术组件，以及在相关研究中使用的多模态数据集。此外，我们还剖析了借助现有生成模型进行人类-计算机交互的工具增强型多模态代理。最后，我们全面讨论了人工智能安全的进步，并探索了新兴应用和未来前景。我们的工作提供了一个系统而深入的多模态生成概述，有望推动生成内容的人工智能（AIGC）和世界模型的发展。所有相关的论文列表可在<https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation>找到。**|
|**2024-05-29**|**Multi-Modal Generative Embedding Model**|Feipeng Ma et.al.|[2405.19333](http://arxiv.org/abs/2405.19333)|null|在大多数多模态任务中，问题可以归结为生成或嵌入。现有的模型通常通过将语言模块分解为一个用于生成的文本解码器和一个用于嵌入的文本编码器来处理这两种问题。为了探索多模态方法的简约性，本工作试图仅使用一个模型来处理每种模态。为此，我们提出了一种多模态生成嵌入模型（MM-GEM），它将生成和嵌入目标整合到一个大型语言模型中。同时，我们设计了PoolAggregator，以提高效率并实现细粒度的嵌入和生成能力。  令人惊讶的是，这两个目标之间并没有显著冲突。例如，基于ViT-Large和TinyLlama的MM-GEM在诸如跨模态检索和零样本分类等多模态嵌入模型基准上表现出良好的性能，同时具备良好的图像描述能力。此外，MM-GEM能够无缝执行区域级别的图像描述生成和检索任务。另外，MM-GEM中的先进文本模型对于长文本和图像检索的Recall@1指标带来了超过5%的提升。|
|**2024-05-29**|**Self-Exploring Language Models: Active Preference Elicitation for Online Alignment**|Shenao Zhang et.al.|[2405.19332](http://arxiv.org/abs/2405.19332)|**[link](https://github.com/shenao-zhang/selm)**|****摘要：**  偏好优化，特别是在人类反馈强化学习（RLHF）的驱动下，已经在使大型语言模型（LLMs）遵循人类意愿方面取得了显著成就。相较于使用固定数据集的离线对齐，通过人或人工智能对模型生成的反馈通常能够通过迭代过程提升奖励模型的能力和LLMs的一致性。然而，要实现全局准确的奖励模型，需要系统地探索生成各种各样的响应，以涵盖自然语言的广阔空间。仅依赖标准奖励最大化LLMs的随机采样是不足以满足这一需求的。  为解决这个问题，我们提出了一种双层目标，乐观地倾向于可能具有高奖励的响应，以此来主动探索分布外区域。通过解决内层问题，利用重新参数化的奖励函数，我们提出了名为Self-Exploring Language Models（SELM）的算法。它消除了对单独奖励模型（RM）的需求，并通过一个直观的目标对LLMs进行迭代更新。与直接偏好优化（DPO）相比，SELM的目标降低了对未见过的过度延伸的无差别偏好，提高了探索效率。  我们的实验结果显示，在Zephyr-7B-SFT和Llama-3-8B-Instruct模型上进行微调后，SELM在MT-Bench和AlpacaEval 2.0等指令跟随基准以及不同设置下的各种标准学术基准上表现出显著的性能提升。我们的代码和模型已可在<https://github.com/shenao-zhang/SELM>获取。**|
|**2024-05-29**|**Normative Modules: A Generative Agent Architecture for Learning Norms that Supports Multi-Agent Cooperation**|Atrisha Sarkar et.al.|[2405.19328](http://arxiv.org/abs/2405.19328)|null|本文提出了一种名为“规范模块”的架构，它针对生成性代理在面对包含现有规范的社会结构时的协作挑战。这些代理通过大型语言模型理解和评估环境，但在处理复杂社会任务时，如何识别并适应规范基础设施成为关键问题。规范模块的核心在于促进均衡选择，借鉴分类机构实现相关均衡的概念，使代理能够通过同伴互动学习环境中不同候选机构中的权威性。通过提升规范能力，代理可以协调制裁行为，进而影响社交环境中的基本行为，从而提高整体福祉。  我们设计了一个支持机构的新环境，并根据两个主要标准来评估该框架：一是代理能否忽略非权威机构，二是代理在多个选项中识别权威机构的能力。实验结果显示，配备了规范模块的代理相比基础代理能实现更稳定的合作效果，这为研究设计考虑规范基础设施的环境和代理开辟了新途径。|
|**2024-05-29**|**MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series**|Ge Zhang et.al.|[2405.19327](http://arxiv.org/abs/2405.19327)|null|近年来，大型语言模型（LLMs）在各种任务上取得了显著进步。然而，出于商业利益，像GPT、Gemini和Claude这样的最先进模型被封闭在专有接口后，其训练详情并未公开。近期，一些机构开源了类似性能的LLMs，如LLaMA-3，但大多数细节（如中间检查点、预训练语料库和训练代码等）仍未披露。为了提高LLMs的透明度，研究界正在推动真正开放的模型，如Pythia、Amber和OLMo，这些模型提供了更多的信息，促进了对大模型性能、局限性、偏见和风险的科学研究。然而，现有的开放模型在推理、知识和编程任务上的表现仍逊于同等规模的封闭源码模型。  因此，我们开源了MAP-Neo，一个拥有70亿参数的双语语言模型，从头开始在4.5万亿高质量令牌上进行训练。MAP-Neo是首个与现有顶级LLMs性能相当的完全开源的双语模型。此外，我们还公开了所有细节，包括清理后的预训练语料库、数据清洗流程、检查点以及优化的训练和评估框架，以供重现。我们期望MAP-Neo能推动开放研究社区的发展，激发更多创新，促进LLMs的进一步提升。|
|**2024-05-29**|**Reasoning3D -- Grounding and Reasoning in 3D: Fine-Grained Zero-Shot Open-Vocabulary 3D Reasoning Part Segmentation via Large Vision-Language Models**|Tianrun Chen et.al.|[2405.19326](http://arxiv.org/abs/2405.19326)|null|本文提出了一项新的任务：零样本3D推理分割，目标是针对物体的部件搜索和定位，这是一种超越了先前类别特定的3D语义分割、3D实例分割和开放词汇3D分割局限的新范式。我们设计了一个名为Reasoning3D的简单基线方法，它能够理解和执行复杂的命令，对3D网格进行（细致）部分分割，同时具备上下文感知和推理答案的交互式分割能力。特别地，Reasoning3D利用预训练的2D分割网络，该网络由大型语言模型（LLMs）驱动，在零样本情况下解析用户输入查询。已有研究表明，大规模预训练赋予基础模型世界知识的先验，使其能够理解复杂指令，这使得我们在依赖有限3D数据集的情况下也能“分割任何东西”（源效率高）。实验表明，我们的方法具有泛化性，能有效根据隐性文本查询在3D对象（3D网格）中定位和突出显示部分，包括可动3D对象和真实世界的扫描数据。此外，我们的无监督方法便于快速部署，并为未来3D（语义）对象理解领域的研究，如机器人、物体操作、部件组装、自动驾驶应用、增强现实和虚拟现实（AR/VR）、以及医疗应用，提供了一个可行的通用基准。代码、模型权重、部署指南和评估协议可在以下链接获取：http://tianrun-chen.github.io/Reason3D/。|
|**2024-05-29**|**Nearest Neighbor Speculative Decoding for LLM Generation and Attribution**|Minghan Li et.al.|[2405.19325](http://arxiv.org/abs/2405.19325)|null|大型语言模型（LLMs）常常会产生虚构内容且缺乏对生成文本的来源标注。为解决这些问题，半参数化语言模型如kNN-LM通过在非参数数据存储中寻找与给定提示最接近的邻居来改进LM输出。然而，这类模型的推理速度通常较慢，生成的文本流畅度不高。本文提出了一种新颖的半参数化语言建模方法——Nearest Neighbor Speculative Decoding（NEST），它能够将现实世界中的任意长度文本片段融入生成过程，并提供其源头的标注。NEST在每次推理步骤中进行基于令牌的检索，计算出一个半参数混合分布，并从语料库中识别出可能的连续文本段落扩展。它采用一种近似推测解码策略，接受检索到的片段前缀或生成新的令牌。NEST显著提高了基础LM在各种知识密集型任务中的生成质量和来源标注率，超越了传统的kNN-LM方法，并在基于上下文的检索增强方面表现出竞争力。此外，NEST大幅提升了生成速度，当应用于Llama-2-Chat 70B时，推理时间提高了1.8倍。|
|**2024-05-29**|**Are Large Language Models Chameleons?**|Mingmeng Geng et.al.|[2405.19323](http://arxiv.org/abs/2405.19323)|null|大语言模型（LLMs）是否拥有自己的世界观和人格倾向？研究人员进行了超过一百万次的实验，让LLMs回答主观问题。通过将这些模型的响应与欧洲社会调查（ESS）的实际数据进行比较，结果显示提示对偏见和变异性有显著影响，揭示了重大的文化、年龄和性别偏差。文中讨论了评估LLMs与调查数据差异的方法，如计算加权平均值以及一个新提出的基于Jaccard相似性的测量指标。研究者强调，在利用LLMs模拟个体决策或集体行为之前，分析提示的稳健性和变异性至关重要，因为它们的模仿能力充其量只能说是近似的。|
|**2024-05-29**|**Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF**|Shicong Cen et.al.|[2405.19320](http://arxiv.org/abs/2405.19320)|null|**摘要：**  强化学习从人类反馈（RLHF）在调整大型语言模型（LLMs）以符合人类偏好方面展现出巨大潜力。在线和离线RLHF都处于活跃的研究阶段，但关键挑战之一是如何在处理从偏好数据中学习的奖励函数不确定性时。尽管标准强化学习（RL）中乐观主义或悲观主义的原则已广为人知，但在大型语言模型中实现既实用又基于理论的方法尚不成熟，因为构建置信区间的标准技术在处理任意策略参数化时变得难以处理。  本文提出了一种统一的在线和离线RLHF方法——价值激励的偏好优化（VPO）。VPO通过在最大似然估计的奖励函数中添加相应的值函数的正则化，以指示选择乐观主义还是悲观主义，实现了这一目标。此外，VPO直接优化策略，并利用隐式奖励建模，因此其RLHF管道与直接偏好优化更为简单。对于在线和离线设置，VPO提供了理论保证，其收敛速度与标准RL相当。实验在文本摘要和对话任务上验证了VPO的实用性与有效性。|
|**2024-05-28**|**Don't Forget to Connect! Improving RAG with Graph-based Reranking**|Jialin Dong et.al.|[2405.18414](http://arxiv.org/abs/2405.18414)|null|## 背景  检索增强生成（Retrieval Augmented Generation，RAG）通过结合现有文档的上下文显著提升了大语言模型（Large Language Model，LLM）的响应性能。然而，当文档与问题上下文的相关性不明显或存在部分信息时，RAG的效果如何？又该如何处理文档之间的关联性呢？本研究旨在解答RAG生成中的这两个核心问题。我们提出了一种名为G-RAG的方法，它是一个基于图神经网络（Graph Neural Networks，GNNs）的重排器，介于RAG的检索器和阅读器之间。G-RAG结合了文档之间的连接性和语义信息（通过抽象意义表示图），为RAG提供了一个具有上下文感知的排名器。实验结果表明，G-RAG超越了现有的领先方法，同时计算开销更小。此外，我们评估了PaLM 2作为重排器的表现，发现其明显逊色于G-RAG，这强调了即使使用大型语言模型，重排在RAG中的重要性。|
|**2024-05-28**|**Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning**|Yixiao Zhang et.al.|[2405.18386](http://arxiv.org/abs/2405.18386)|**[link](https://github.com/ldzhangyx/instruct-MusicGen)**|**在文本到音乐编辑领域，近期的进步依赖于文本查询来改变音乐风格或调整乐器元素。然而，现有方法要么需要从头训练特定的编辑模型，耗时且资源密集，要么使用大型语言模型预测编辑后的音乐，导致音频重建不够精确。为了结合优点并解决这些问题，我们提出了Instruct-MusicGen，这是一种新颖的方法，它针对预训练的MusicGen模型进行微调，以高效地执行编辑指令，如添加、删除或分离音轨。我们的方法修改了原始MusicGen架构，引入了文本融合模块和音频融合模块，使模型能够同时处理指令文本和音频输入，生成所需的编辑音乐。令人惊讶的是，Instruct-MusicGen仅向原始模型增加了8%的新参数，并在5000步的训练后，其性能超越现有基准，且表现出与专门针对任务训练的模型相当的能力。这一进展不仅提高了文本到音乐编辑的效率，还拓宽了音乐语言模型在动态音乐制作环境中的应用范围。**|
|**2024-05-28**|**OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection for Memory-Efficient LLM Fine-tuning**|Pengxiang Li et.al.|[2405.18380](http://arxiv.org/abs/2405.18380)|**[link](https://github.com/pixeli99/owlore)**|**随着大型语言模型（LLMs）的快速发展，它们在自然语言处理任务中带来了革命性变化。然而，大模型的训练或微调带来了巨大挑战。针对这一问题，低秩适应（LoRA）等参数高效方法崭露头角，但往往牺牲性能。本文提出了一种新的内存高效微调方法——Outlier-weighed Layerwise Sampled Low-Rank Projection（OwLore），它受到LLMs层间异常分布的启发，通过动态采样预训练层而非添加额外适配器来进行微调。我们首先通过Heavy-Tailed Self-Regularization理论（HT-SR）解读异常现象，发现具有更多异常值的层更倾向于呈现长尾分布，训练效果更好。因此，OwLore策略性地为异常值较多的层分配更高的采样概率，以更好地利用预训练模型的知识。  为了进一步减少微调时的内存需求，我们结合梯度低秩投影，使得每一层能以低秩方式高效训练。通过融合低秩优势和最优层别采样策略，OwLore显著优化了LLM剪枝中的内存-性能权衡。我们在多个架构，如LLaMa2、LLaMa3和Mistral上的广泛实验表明，OwLore持续优于基础方法，包括全量微调。例如，在常识推理基准上，OwLore可实现平均1.1%的精度提升，MMLU上提高3.0%，而在MT-Bench上更是有显著的10%提升，同时内存效率更高。特别地，OwLore仅需21GB内存即可对LLaMa2-7B进行微调。**|
|**2024-05-28**|**LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models**|Anthony Sarah et.al.|[2405.18377](http://arxiv.org/abs/2405.18377)|null|现代大型语言模型（LLMs）在自然语言处理、复杂推理、情感分析等任务中的卓越表现推动了它们的广泛应用。然而，这些强大的功能伴随着巨大的内存和计算成本，限制了在大多数硬件平台上的使用。为解决这一问题，我们提出了一种有效的方法，基于LLaMA2-7B进行单次微调后，通过遗传算法搜索找到更小、计算复杂度更低的网络架构。实验表明，对于某些标准基准任务，预训练的LLaMA2-7B模型实际上过于庞大且复杂。我们实现了1.5倍的模型大小缩减和1.3倍的吞吐量提升，同时保持了几乎无损的准确性。相较于某些剪枝或稀疏化技术，我们的方法在效率和效果上更为优越。最后，我们展示了量化与我们的方法相结合的效果，进一步通过量化减少了找到的网络的大小和复杂性。我们相信，本工作提供了一种自动创建可在更廉价和广泛可用硬件平台上使用的LLMs的方法。|
|**2024-05-28**|**Empowering Source-Free Domain Adaptation with MLLM-driven Curriculum Learning**|Dongjie Chen et.al.|[2405.18376](http://arxiv.org/abs/2405.18376)|**[link](https://github.com/Dong-Jie-Chen/RCL)**|**### 背景  源免费领域适应（SFDA）的目标是仅使用未标记的靶域数据来调整预训练的源模型。当前的SFDA方法在有效利用预训练知识和挖掘靶域数据潜力方面面临挑战。多模态大型语言模型（MLLMs）在理解视觉和文本信息方面表现出色，但它们应用于SFDA时存在问题，如指令执行失败、计算需求高以及在适应前性能评估困难。为了缓解这些问题，我们提出了一种新颖的框架——可靠性基于课程学习（RCL），它通过伪标签化整合多个MLLM以促进知识利用，应用于SFDA。  ### 方法  我们的框架包括：1) 可靠知识转移，2) 自我纠正，3) MLLM引导的知识扩展，以及4) 多热掩码精炼，这些方法协同作用，逐步发掘靶域未标记数据的价值。RCL在多个SFDA基准上实现了最先进的（SOTA）性能，例如在DomainNet上提升显著，达到 $\textbf{+9.4\%}$ ，证明了其在增强适应性和鲁棒性方面的有效性，同时无需访问源数据。代码可在https://github.com/Dong-Jie-Chen/RCL获取。**|
|**2024-05-28**|**Thai Winograd Schemas: A Benchmark for Thai Commonsense Reasoning**|Phakphum Artkaew et.al.|[2405.18375](http://arxiv.org/abs/2405.18375)|**[link](https://github.com/PhakphumAdev/Thai-Winograd)**|常识推理是自然语言理解的重要组成部分，为此已开发出多个评估基准。然而，这些基准大多仅限于英语。创建平行基准有助于跨语言评估，从而更好地理解不同语言。本研究介绍了一个泰语版的Winograd Schema集合，这是一个专为测试泰语中的常识推理能力而设计的新数据集。我们通过邀请母语者、专业翻译和严格验证的方法，确保该系列题库能准确反映泰国语言的独特性、习语和文化引用，同时保持模糊性和常识挑战。我们对大型语言模型（如GPT-4和Claude-3-Opus）在这项基准上的性能进行了评估，结果显示尽管在英语上表现优异，但它们在泰语中的性能明显下降，这表明在多语言常识推理方面仍有待进步。|
|**2024-05-28**|**PromptWizard: Task-Aware Agent-driven Prompt Optimization Framework**|Eshaan Agarwal et.al.|[2405.18369](http://arxiv.org/abs/2405.18369)|null|大型语言模型（LLMs）已经在各个领域带来了革命性的变化，展现出卓越的能力。它们成功的关键在于提示的概念，即指导模型生成输出。然而，手动创建提示既耗时又局限于特定领域，因此需要自动化的解决方案。本文介绍PromptWizard，一个新颖的框架，它利用LLMs迭代地合成和优化针对特定任务的提示。与现有方法不同，PromptWizard同时优化提示指令和上下文示例，以最大化模型性能。该框架通过变异指令并引入负例，逐步深化理解并保证多样性。借助一个评判者，PromptWizard进一步改进指令和示例，融入详细的推理步骤，以实现最佳表现。PromptWizard具有计算效率高、适应不同训练数据量场景以及在小型LLM上同样有效的特点。通过对8个数据集的35个任务进行严谨评估，结果显示PromptWizard明显优于现有的提示策略，证明了其在提示优化方面的高效性和可扩展性。|
|**2024-05-28**|**Is a 3D-Tokenized LLM the Key to Reliable Autonomous Driving?**|Yifan Bai et.al.|[2405.18361](http://arxiv.org/abs/2405.18361)|null|随着自动驾驶（AD）任务的快速发展，基于端到端的方法，特别是视觉语言模型（VLM）的应用变得尤为重要。这些模型试图融合强大的逻辑推理和认知能力，以实现全面的端到端规划。然而，现有的VLM方法往往依赖于2D视觉分词器和大型语言模型（LLM），在处理三维几何信息方面存在不足，这对于可靠的规划至关重要。研究表明，2D分词的LLM并不能准确感知三维环境，这引发了关于VLM在自动驾驶中可靠性的质疑。  针对这一问题，我们提出了一种名为Atlas的新方法，它结合了DETR风格的3D感知器作为3D分词器，与单层线性投影器相连，巧妙地利用了三维物理世界的固有特性。这种方法允许高分辨率多视角图像的同时处理和时空建模。尽管简单，但Atlas在NuScenes数据集上的3D检测和自主驾驶规划任务中表现出色，证明了3D分词的LLM对于实现可靠自动驾驶至关重要。我们将开源代码和数据集，以供进一步研究。|
|**2024-05-28**|**Bridging the Gap: Dynamic Learning Strategies for Improving Multilingual Performance in LLMs**|Somnath Kumar et.al.|[2405.18359](http://arxiv.org/abs/2405.18359)|null|大型语言模型（LLMs）正在全球范围内重塑众多领域，但它们在处理非拉丁字母和低资源语言时的包容性和效果仍有待提升。本文针对这一关键挑战，提出了一种无需大量训练或微调的方法来增强多语言LLMs的表现。通过系统地研究和评估各种语言在流行的问题解答（QA）数据集上的性能，我们提出了一系列新颖技术，以释放LLMs在多元语言环境中的真正潜力。我们的方法包括三个核心策略，极大地提高了多语言能力：首先，精心优化适用于多语言LLM的提示，挖掘其潜在能力，显著提升了各语言的表现。其次，我们引入了一种新的混合方法，结合了多语言嵌入的LLM检索增强生成（RAG），实现了更好的多任务性能。最后，我们开发了一种动态学习策略，实现实时根据查询动态选择最合适的提示策略、LLM模型和嵌入模型，从而最大化LLM在不同语言上的效率，超越了最佳静态和随机策略。此外，我们的方法既适用于离线配置调整，也支持在线适应，能够无缝适应新语言和数据集，显著推动了多语言理解和生成在各种语言中的进步。|
|**2024-05-28**|**MMCTAgent: Multi-modal Critical Thinking Agent Framework for Complex Visual Reasoning**|Somnath Kumar et.al.|[2405.18358](http://arxiv.org/abs/2405.18358)|null|## 背景  近期的多模态大型语言模型（MLLM）在视觉与语言融合任务上取得了显著进步。然而，它们在细致的多模态理解、复杂任务解析以及多模态信息推理方面仍存在挑战。本文提出MMCTAgent，一个旨在解决当前MLLM在复杂视觉推理任务中固有局限性的新型多模态批判性思维代理框架。MMCTAgent借鉴了人类认知过程和批判性思考的特点，通过迭代分析多模态信息、拆解问题、规划策略，并实现动态推理。  此外，MMCTAgent还融入了批判性思考元素，如对最终答案的验证和自我反思。它通过一种新颖的方法定义基于视觉的评判者，并确定特定任务的评估标准，从而提升决策能力。在多个图像理解和视频理解基准测试中，我们严谨地评估了MMCTAgent（包括带评判者的版本）的表现，结果表明它在超越基础MLLM和其他工具增强的管道方面表现出色。|
|**2024-05-27**|**Matryoshka Multimodal Models**|Mu Cai et.al.|[2405.17430](http://arxiv.org/abs/2405.17430)|null|## 背景  大型多模态模型（如LLaVA）在视觉-语言推理方面表现出色。这些模型首先将图像嵌入到大量的固定视觉令牌中，然后将它们输入到大型语言模型（LLM）。然而，这种设计在处理高分辨率图像和视频等密集视觉场景时会导致大量令牌，从而导致效率低下。尽管存在令牌剪枝/合并方法，但它们为每个图像生成单个长度的输出，无法在信息密度与效率之间灵活权衡。受到套娃玩偶概念的启发，我们提出了M3：套娃多模态模型，它学习将视觉内容表示为捕捉不同粗细粒度信息的嵌套视觉令牌集合。  ## 任务  我们的方法为LMMs带来了几个独特的优势：(1) 在测试实例中，用户可以明确控制视觉粒度，例如，根据内容的复杂性或简洁性调整用于表示图像的令牌数量；(2) M3提供了一个分析现有数据集所需粒度的框架，我们发现像COCO这样的基准只需要大约~9个视觉令牌就能获得与使用所有576个令牌相当的准确性；(3) 我们的方法为探索性能与视觉令牌长度之间的最佳权衡提供了基础，研究显示当前固定规模表示与理想上限之间存在显著差距。|
|**2024-05-27**|**NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models**|Chankyu Lee et.al.|[2405.17428](http://arxiv.org/abs/2405.17428)|null|本文介绍了一种名为NV-Embed的新型大语言模型，专门设计用于提升基于解码器的大型语言模型在文本嵌入任务中的性能，包括密集向量检索。NV-Embed通过多种架构设计和训练策略显著增强模型的灵活性和表现，同时保持其简洁性和可复现性。  在架构方面，我们引入了隐式注意力层来获取池化嵌入，这在检索和下游任务准确性上均优于平均池化或使用LLMs的最后一个<EOS> token嵌入。为了改进表示学习，我们移除了LLMs的自回归注意力掩码，在对比性训练中允许更全面的信息交互。  在训练策略上，我们采用两阶段的对比性指令调优方法。第一阶段在检索数据集上进行指令训练，利用批次内负样本和精心挑选的难例。第二阶段将各种非检索任务的数据融入指令调优，不仅提高非检索任务的准确性，还提升了检索性能。  凭借这些创新，NV-Embed仅使用公开数据就实现了前所未有的高分，达到69.32，荣登大规模文本嵌入基准（MTEB）（截至2024年5月24日）榜首，涵盖56项任务，包括检索、重排、分类、聚类和语义文本相似度。尤其值得注意的是，我们的模型在BEIR的15项检索任务中取得了最高的59.36分。NV-Embed模型的源代码将在以下网址开源：https://huggingface.co/nvidia/NV-Embed-v1。|
|**2024-05-27**|**Reason3D: Searching and Reasoning 3D Segmentation via Large Language Model**|Kuan-Chih Huang et.al.|[2405.17427](http://arxiv.org/abs/2405.17427)|**[link](https://github.com/kuanchihhuang/reason3d)**|**随着多模态大型语言模型（LLMs）的最新进展，它们在概念推理等领域展现出巨大潜力。然而，在理解三维环境方面的应用仍相对有限。本文提出Reason3D，这是一种专为全面3D理解设计的新颖LLM。Reason3D接受点云数据和文本提示作为输入，生成文本响应和分割掩码，支持高级任务，如3D推理分割、层次搜索、表达式指代和详细掩码输出的问答。特别是，我们设计了一种分层掩码解码器，能够精确定位广阔场景中的小物体。该解码器首先生成一个粗略的位置估计，覆盖物体的大致区域，然后采用逐步细化的策略，显著提高对象识别和分割的精度。实验结果显示，Reason3D在ScanNet和Matterport3D等大规模数据集上，在3D表达式指代、3D问答和3D推理分割任务上表现出卓越性能。代码和模型已在以下链接提供：https://github.com/KuanchihHuang/Reason3D。**|
|**2024-05-27**|**LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence**|Zhuoling Li et.al.|[2405.17424](http://arxiv.org/abs/2405.17424)|null|由于实体代理需要与现实世界互动，它们必须具备全面的先验知识、长远规划能力以及快速响应速度。尽管近期基于大型语言模型（LLM）的代理表现出色，但它们仍存在一些局限性。例如，LLM的输出通常是描述性的句子，在确定具体动作时可能存在歧义。为了克服这些问题，我们提出了大型自回归模型（LARM）。LARM利用文本和多视角图像作为输入，并以自回归方式预测后续动作。为了训练LARM，我们开发了一种新颖的数据格式，称为自回归节点传输结构，并构建了相应的数据集。通过两阶段训练，LARM成功在《我的世界》（Minecraft）中收集魔法装备，这比先前最佳方法所能达到的成就需要更复杂的决策链。此外，LARM的速度是最快的，比以前快6.8倍。|
|**2024-05-27**|**Self-Corrected Multimodal Large Language Model for End-to-End Robot Manipulation**|Jiaming Liu et.al.|[2405.17418](http://arxiv.org/abs/2405.17418)|null|当机器人操作策略面对新任务或物体实例时，其动作性能往往不尽人意。因此，自动检测和自我纠正失败动作的能力对于实际的机器人系统至关重要。近期，多模态大型语言模型（Multimodal Large Language Models，MLLM）在视觉指令跟随方面展现出前景，并在多种任务中展现出强大的推理能力。为了将通用MLLM作为端到端的机器人代理，我们提出了Self-Corrected (SC)-MLLM，不仅使其能够预测末端执行器位置，还赋予其自主识别并纠正错误动作的能力。首先，我们通过参数效率高的微调，使MLLM具备姿态预测功能，将其转化为一个语言建模问题。在遇到执行失败时，模型能识别低层次动作错误的原因（如位置和旋转误差），并主动寻求专家的提示。根据反馈，SC-MLLM会重新思考当前失败场景，生成修正后的动作。此外，我们设计了一种连续策略学习方法，针对成功纠正的样本，提升模型对当前场景配置的适应性，减少专家干预的频率。  为了评估我们的SC-MLLM，我们在模拟和真实世界环境中进行了广泛实验。结果表明，与先前最先进的机器人MLLM（ManipLLM）相比，SC-MLLM显著提高了操作精度：在已知物体类别上从57%提升至79%，在未知新类别上从47%提升至69%。|
|**2024-05-27**|**THREAD: Thinking Deeper with Recursive Spawning**|Philip Schroeder et.al.|[2405.17402](http://arxiv.org/abs/2405.17402)|**[link](https://github.com/philipmit/thread)**|大型语言模型（LLMs）在各种场景中展现出卓越的能力，但随着上下文的长度和复杂度增加，它们仍面临挑战。为此，我们提出了Thinking Recursively and Dynamically（ThReaD）方法。ThReaD将模型生成过程构想为一个执行流程，根据上下文可以完整运行或动态地创建新线程。通过子线程，模型可以分发任务（如思考、获取信息），子线程只返回父线程所需的令牌，从而让模型能够根据需要调整产生令牌时使用的中间工作量。我们在任务解决和问答等场景中应用ThReaD，使其能递归地将给定的任务或问题分解为逐步简化的小子问题，由单独的子线程解决。我们使用少量样本学习的方式实现ThReaD，并在包括ALFWorld、TextCraft、WebShop在内的多个基准测试上评估GPT-4和GPT-3.5的表现，以及两个新基准：DataCommons QA和MIMIC-III ICU QA。实验结果显示，ThReaD在这些基准上实现了最先进的性能，相对于现有框架，即使是小型模型（如Llama-3-8b和CodeLlama-7b）也能提升10%到50%的绝对分数。|
|**2024-05-27**|**MindMerger: Efficient Boosting LLM Reasoning in non-English Languages**|Zixian Huang et.al.|[2405.17386](http://arxiv.org/abs/2405.17386)|**[link](https://github.com/cone-mt/mindmerger)**|## 任务  推理能力对于大型语言模型（LLMs）至关重要，但英语与其他非英语语言之间的差距明显。一些研究通过微调LLMs以重新学习非英语的推理能力，而另一些方法则使用外部模型（如英语翻译文本）的输出来替换非英语输入，以应对LLM理解非英语的挑战。然而，这些方法往往未能充分利用LLMs内在的推理和语言理解能力。为了更好地利用LLMs的思维和语言理解能力，我们提出了一种新方法，称为MindMerger，它将LLMs与多语言模型的外部语言理解能力相结合，以提升多语言推理性能。我们还引入了两步训练策略，首先将外部能力嵌入LLMs，然后训练外部能力和内置能力的协作使用。在三个多语言推理数据集和一个语言理解数据集上的实验表明，MindMerger始终优于所有基线，特别是在低资源语言上。在不更新LLMs参数的情况下，MGSM数据集上所有语言的平均准确率提高了6.7%，低资源语言提高了8.0%。|
|**2024-05-27**|**ReMoDetect: Reward Models Recognize Aligned LLM's Generations**|Hyunseok Lee et.al.|[2405.17382](http://arxiv.org/abs/2405.17382)|null|随着大型语言模型（LLMs）的卓越性能和易用性提升，它们带来的社会风险，如假新闻生成，促使开发出能检测LLM生成文本（LGT）的方法以确保安全使用。然而，由于大量LLM的存在，逐个识别它们的特点变得不切实际。因此，研究关注的是这些强大模型共有的特性，即“对齐训练”，即训练LLMs生成更符合人类偏好的文本。我们的关键发现是，随着这些对齐训练的LLMs致力于最大化人类偏好，它们生成的文本甚至比人类撰写的文本在估计偏好上更高，这使得利用偏好模型（一个训练来模拟人类偏好分布的LLM）轻易就能检测到这些文本。  基于这一发现，我们提出两种进一步增强偏好模型检测能力的训练策略：（1）持续偏好微调，使模型更偏向于识别对齐的LLG；（2）奖励模型对人/LLM混合文本的学习，即使用对齐LLM重述的人类原创文本，这是一种介于LGT和人类文本之间的偏好基准，有助于更好地学习决策边界。我们在六个文本领域和十二种对齐LLM上进行了广泛评估，结果显示我们的方法表现出最先进的性能。相关代码已在https://github.com/hyunseoklee-ai/reward_llm_detect上提供。|
|**2024-05-27**|**RTL-Repo: A Benchmark for Evaluating LLMs on Large-Scale RTL Design Projects**|Ahmed Allam et.al.|[2405.17378](http://arxiv.org/abs/2405.17378)|**[link](https://github.com/AUCOHL/RTL-Repo)**|大型语言模型在辅助进行寄存器传输级（Register Transfer Level, RTL）设计任务上展现出潜力。然而，现有的基准测试在反映真实世界RTL项目复杂性方面存在显著差距。为此，该论文提出了一项新的基准——RTL-Repo，专为评估大型语言模型在大规模RTL设计项目中的性能而设计。RTL-Repo包含了从GitHub公共仓库提取的超过4000个Verilog代码样本，每个样本都提供了对应仓库的完整上下文。我们对包括GPT-4、GPT-3.5、Starcoder2以及像VeriGen和RTLCoder这样的Verilog专用模型在内的多款最先进的模型在RTL-Repo基准上的性能进行了评估，比较它们在生成复杂项目的Verilog代码方面的表现。RTL-Repo为硬件设计社区提供了一个宝贵的资源，用于评估和比较语言模型在实际RTL设计场景中的性能，并针对复杂的多文件RTL项目专门训练Verilog代码生成。RTL-Repo是开源的，已在GitHub上公开可用。|
|**2024-05-28**|**Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models**|ShengYun Peng et.al.|[2405.17374](http://arxiv.org/abs/2405.17374)|null|### 背景  安全校准是确保大型语言模型（LLMs）的行为符合人类偏好并避免有害行为的关键，但近期研究显示，仅使用少量精心设计的训练样本来微调模型可能导致安全性被轻易破坏。我们致力于通过探索LLM的安全景观来评估微调过程中的风险。我们发现了一个普遍存在于流行开源LLM模型参数空间中的新现象，称为“安全盆地”：随机扰动模型权重能使模型在局部区域保持原始校准模型的安全性。  ### 发现与贡献  我们的发现启发我们提出了一种新的安全度量方法——VISAGE，它通过探测模型的安全景观来评估LLM微调过程中的安全性。可视化校准模型的安全景观有助于理解微调如何使模型偏离安全盆地，从而损害安全性。此外，我们观察到系统提示在保护模型方面的重要性，这种保护甚至会传递给处于安全盆地内的扰动版本。这些从安全景观研究中得出的见解为未来LLM安全领域的研究提供了新的洞见。|
|**2024-05-24**|**Scaling Laws for Discriminative Classification in Large Language Models**|Dean Wyatte et.al.|[2405.15765](http://arxiv.org/abs/2405.15765)|null|## 背景  现代大型语言模型（LLMs）标志着机器学习模型能力的一个重大飞跃。这些模型能够对各种查询生成合理的回答，这表明它们在客户服务应用中具有潜力。然而，LLMs已被观察到存在胡言乱语的问题，这在短期内限制了它们在客户服务中的应用。为了解决这个问题，我们提出了一种系统，将语言建模任务重新构想为分类任务，以帮助客户服务代表选择最佳的模板回复。我们的目标是为客服代表提供最合适的前K个候选回复。  ## 任务描述  我们展示了离线和在线实验的结果，证明了实验系统的有效性，离线实验显示出改进，而在线实验则带来了统计显著的效果提升。此外，我们分享了通过模型参数调整进行的验证损失和前K精度的度量曲线。最后，我们讨论了模型大小、延迟和准确性之间的权衡，并展望了未来可能的应用领域。|
|**2024-05-24**|**Large Language Models Reflect Human Citation Patterns with a Heightened Citation Bias**|Andres Algaba et.al.|[2405.15739](http://arxiv.org/abs/2405.15739)|**[link](https://github.com/andresalgaba/llm_citation_patterns)**|论文摘要： 引用实践对于构建科学知识结构至关重要，但往往受到当代规范和偏见的影响。随着大型语言模型（如GPT-4）的出现，这一领域出现了新的动态。研究者首次探索了完全依赖参数知识而非基于搜索或检索增强生成的推荐引用的特性及其潜在偏见。实验使用了一组包含166篇来自AAAI、NeurIPS、ICML和ICLR的论文，这些论文在GPT-4的知识截止日期后发表，涉及3,066个引用。实验让GPT-4为匿名文本中的引用提供学术参考。结果揭示了人类和语言模型（如GPT-4）的引用模式惊人相似，但GPT-4显示出更强的高引用偏见，即使在控制了出版年份、标题长度、作者数量和会议等因素后依然存在。此外，我们发现GPT-4生成的既有和不存在引用的特性高度一致，表明模型内化了引用模式。通过分析引用图谱，显示GPT-4推荐的引用嵌入在相关引用网络中，暗示其对概念的深入理解。尽管语言模型可以辅助引用生成，但它们也可能放大现有偏见并引入新偏见，可能影响科学知识的传播。我们的结果强调了识别模型偏见的必要性，并开发平衡的方法与语言模型互动的重要性。|
|**2024-05-24**|**LM4LV: A Frozen Large Language Model for Low-level Vision Tasks**|Boyang Zheng et.al.|[2405.15734](http://arxiv.org/abs/2405.15734)|null|大型语言模型（LLMs）的成功催生了多模态大型语言模型（MLLMs）的研究热潮，它们正在改变计算机视觉领域的多个研究范式。尽管MLLMs在诸如视觉问答（VQA）和文本到图像等高级视觉和 Vision-and-Language 任务上表现出色，但尚无研究探讨过低级视觉任务如何从这些模型中受益。我们发现，当前大多数MLLM的设计使其对低级特征视而不见，因此在解决低级视觉任务方面存在固有限制。为此，我们提出 $\textbf{LM4LV}$ ，这是一个框架，它允许一个冻结的LLM无需任何多模态数据或先验知识就能解决一系列低级视觉任务。这突显了LLMs在低级视觉领域的强大潜力，并弥合了MLLMs与低级视觉任务之间的鸿沟。我们期望这项工作能激发对LLMs的新视角，加深对其工作机制的理解。|
|**2024-05-24**|**Optimizing Large Language Models for OpenAPI Code Completion**|Bohdan Petryshyn et.al.|[2405.15729](http://arxiv.org/abs/2405.15729)|**[link](https://github.com/BohdanPetryshyn/openapi-completion-benchmark)**|近期，大型语言模型（LLMs）在代码生成任务中的进步极大地改变了软件开发领域。尽管主流编程语言的代码补全解决方案表现出色，但它们在处理较少见的格式，如OpenAPI定义时性能欠佳。本研究评估了GitHub Copilot，一个流行的商业代码补全工具，在OpenAPI完成任务中的表现，并针对Meta开源的Code Llama模型提出了一系列针对该任务的优化策略。研究中设计了一个语义感知的OpenAPI完成基准，通过实验分析了不同提示工程和微调技术对Code Llama模型性能的影响。经过微调的Code Llama模型在正确性上达到了比GitHub Copilot高出55.2%的峰值，同时其参数数量仅为商业解决方案（基于Codex模型）的1/25。此外，研究还改进了一种广泛使用的代码填充训练方法，解决了模型在接收到小于训练时使用的上下文长度提示时的性能不足问题。|
|**2024-05-24**|**Prompt-Aware Adapter: Towards Learning Adaptive Visual Tokens for Multimodal Large Language Models**|Yue Zhang et.al.|[2405.15684](http://arxiv.org/abs/2405.15684)|null|为了弥合视觉和语言模态之间的鸿沟，多模态大型语言模型（Multimodal Large Language Models，MLLMs）通常会学习一个适配器，将视觉输入转化为大语言模型（LLMs）能理解的令牌。然而，大多数适配器生成的视觉令牌相对固定，不考虑提示中提及的具体对象。由于这些适配器对图像中的每个细节分配同等关注，且倾向于处理整个场景，这可能会增加大语言模型在处理复杂场景时的认知负荷。为此，我们提出了提示感知适配器。这类适配器设计有根据提示特定关注点动态嵌入视觉输入的能力。具体来说，提示感知适配器利用全局和局部文本特征，在粗粒度和细粒度层次上捕捉与提示最相关的视觉线索。这种方法显著提升了大语言模型理解和解释视觉内容的能力。在各种视觉问答任务中，如计数和位置推理实验中，提示感知适配器的效果得到了验证。|
|**2024-05-24**|**What Do You See? Enhancing Zero-Shot Image Classification with Multimodal Large Language Models**|Abdelrahman Abdelhamed et.al.|[2405.15668](http://arxiv.org/abs/2405.15668)|null|这篇论文探讨了如何利用大型语言模型（LLMs）进行零样本图像分类。作者提出了一种简单但有效的方法，通过将多模态LLMs应用于图像输入，生成详尽的文本表示。这些文本表示被转化为跨模态嵌入空间中的固定维特征，并结合使用于零样本分类，无需为每个数据集设计复杂的提示。研究者采用通用提示策略，而非针对每个数据集单独调整。实验结果显示，这种方法在多个数据集上表现出色，比先前方法的准确性有所提升。平均而言，在十个基准测试中，该方法比传统方法提高了4.1个百分点，尤其在ImageNet数据集上的提升达到了6.8个百分点。这表明，多模态LLMs有潜力显著增强如零样本图像分类之类的计算机视觉任务，为现有技术带来了显著的进步。|
|**2024-05-24**|**Class Machine Unlearning for Complex Data via Concepts Inference and Data Poisoning**|Wenhan Chang et.al.|[2405.15662](http://arxiv.org/abs/2405.15662)|null|在人工智能时代，用户可能因隐私顾虑要求AI公司从训练数据集中删除他们的信息。作为模型所有者，重新训练模型会消耗大量计算资源，因此机器遗忘（machine unlearning）技术应运而生，以允许删除请求的训练数据或类别，同时尽量减少对模型性能的影响。然而，对于大规模复杂数据，如图像或文本，从模型中“遗忘”一个类别可能导致性能下降，因为难以确定类别与模型之间的关联。为此，我们提出使用概念（Concept）而非图像特征或文本数据中的令牌来表示要删除类别的语义信息，这有助于切断模型与类别的联系，实现彻底消除影响。  为了分析复杂数据中的概念影响，我们采用了后处理概念瓶颈模型和集成梯度技术，精确识别不同类别中的概念。然后，我们利用随机标签和目标标签的数据污染策略，提出遗忘方法。我们在图像分类模型和大型语言模型（LLMs）上测试了我们的方法，结果一致显示，提出的策略能准确地从模型中抹除目标信息，同时保持模型性能的大部分。|
|**2024-05-24**|**$$\mathbf{L^2\cdot M = C^2}$$ Large Language Models as Covert Channels... a Systematic Analysis**|Simen Gaure et.al.|[2405.15652](http://arxiv.org/abs/2405.15652)|null|近年来，大型语言模型（LLMs）因其在翻译、预测和内容生成等任务中的出色表现而备受瞩目。同时，研究界发现LLMs易受攻击，但也能增强系统的安全性。然而，这些开源的LLMs在作为掩蔽通信媒介，如支持抗审查通信方面的能力如何呢？本论文从实验角度出发，通过实证测量开源LLM模型（Llama-7B）的安全性与容量，以评估其作为掩蔽通信的有效性。尽管结果显示，基于这种模型的通道不太可能实现高实际比特率，这取决于消息长度和模型熵，但我们发现对手发现隐秘通信的可能性较低。为了使结果易于广泛参考，我们采用了一个简单且直观的方案，并假设模型是公开可用的。|
|**2024-05-24**|**LLM-based Robot Task Planning with Exceptional Handling for General Purpose Service Robots**|Ruoyu Wang et.al.|[2405.15646](http://arxiv.org/abs/2405.15646)|null|在日常生活中开发通用服务机器人的需求促使机器人必须能恰当地执行多种基础行为。近期，大规模语言模型（LLMs）的训练进步使得可以直接根据自然语言指令生成任务序列，无需额外的领域知识。然而，尽管LLMs的输出在语义上是正确的，但生成的任务计划可能并不精确地对应于可接受的动作，并且可能存在各种语言模糊性。LLM的幻觉问题对机器人任务规划构成挑战，可能导致生成的内容与现实世界事实或用户输入不符。为此，我们提出了一种基于约束LLM提示的任务规划方法，该方法可以从命令中生成可执行的动作序列。此外，我们还设计了一个异常处理模块来应对LLM幻觉问题，确保生成的结果在当前环境中是可接纳的。我们在RoboCup@Home命令生成器生成的命令上测试了我们的方法，结果显示机器人在理解和执行任务方面表现出色。|
|**2024-05-24**|**GECKO: Generative Language Model for English, Code and Korean**|Sungwoo Oh et.al.|[2405.15640](http://arxiv.org/abs/2405.15640)|null|我们介绍GECKO，一个专为韩语和英语（包括编程语言）设计的双语大语言模型（LLM）。它基于LLaMA架构，使用平衡且高质量的韩英语数据集进行预训练。本报告详述了我们在构建数据管道和训练模型过程中的一些努力。尽管GECKO的词汇量较小，但其在生成韩语和英语令牌时表现出高效性能。我们在代表性的基准测试上评估了其性能，特别是在韩国MMMLU（韩国多模态多语言理解）任务上表现优异，而在英语和代码方面则显示出适度的能力，尽管其训练的令牌数量少于专注于英语的LLMs。GECKO以宽松的许可协议对开源社区开放，我们希望它能为韩语LLM研究提供研究基线和实用见解。您可以在以下链接找到该模型：https://huggingface.co/kifai/GECKO-7B。|
|**2024-05-23**|**A Nurse is Blue and Elephant is Rugby: Cross Domain Alignment in Large Language Models Reveal Human-like Patterns**|Asaf Yehudai et.al.|[2405.14863](http://arxiv.org/abs/2405.14863)|null|跨领域对齐是指将一个概念从一个领域映射到另一个领域的任务。例如，询问“如果\textit{医生}是一种\textit{颜色}，它会是什么颜色？”这个看似奇特的课题旨在研究人们如何通过类别映射和对这些映射的推理来表征具体和抽象的概念。在这篇论文中，我们借鉴认知科学中的这一任务，通过行为研究评估大型语言模型（LLMs）在概念化和推理能力上的表现。我们通过提示LLMs执行跨域映射任务，并在群体和个体层面分析它们的响应。此外，我们还评估了模型对其预测进行推理的能力，通过分析和分类它们对这些映射的解释。结果显示，人类和模型的映射以及解释存在显著相似性，表明模型以与人类类似的方式表征概念。这种相似性不仅体现在模型的表示上，也体现在它们的行为中。而且，模型大多给出有效的解释，并采用与人类类似的推理路径。|
|**2024-05-23**|**Bitune: Bidirectional Instruction-Tuning**|Dawid J. Kopiczko et.al.|[2405.14862](http://arxiv.org/abs/2405.14862)|null|我们提出了一种名为Bitune的方法，该方法提升了预训练的解码器型大语言模型在指令调优方面的性能，从而在多个下游任务上实现了显著的提升。Bitune通过同时应用自回归和双向注意力到提示上，以获取更精确的查询或指令表示。我们为此引入了两组参数，并采用了参数高效微调技术来处理。这两种特征随后被组合成一个加权平均，其中权重由可训练系数决定，用于生成新的令牌。实验结果表明，Bitune在零样本设置下在常识推理、算术和语言理解任务上表现出色。大量的消融研究验证了每个组件的作用，并显示了该方法对不同PEFT技术的鲁棒性。|
|**2024-05-23**|**PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression**|Vladimir Malinovskii et.al.|[2405.14852](http://arxiv.org/abs/2405.14852)|**[link](https://github.com/vahe1994/aqlm)**|## 背景  对于大型语言模型（LLMs）的“极端”压缩，即将其参数压缩至1-2位每参数，以适应资源受限设备上的高效执行，引起了广泛关注。现有研究主要集中在改进一次性量化技术和权重表示上；然而，纯后训练方法在精度与位宽权衡方面的收益正在减少。当前最先进的量化方法，如QuIP#和AQLM，包含对部分压缩参数的小规模校准数据微调；然而，这些针对压缩权重的微调通常仅使用直通估计器（STE），STE在这种场景下的性能尚不明确。  本工作质疑在极端LLM压缩中使用STE的有效性，并系统地研究了量化感知微调策略。我们提出PV-Tuning，一个无特定架构限制的框架，它扩展并改进了现有的微调策略，并在某些受限情况下提供收敛保证。在实际应用中，当用于1-2位矢量量化时，PV-Tuning在高性能模型如Llama和Mistral上优于先前的技术。通过使用PV-Tuning，我们在2位参数的情况下首次实现了Llama 2家族模型的帕累托最优量化。|
|**2024-05-23**|**HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models**|Bernal Jiménez Gutiérrez et.al.|[2405.14831](http://arxiv.org/abs/2405.14831)|**[link](https://github.com/osu-nlp-group/hipporag)**|为了在恶劣多变的自然环境中生存，哺乳动物的大脑发展出存储大量世界知识并不断整合新信息的能力，同时避免灾难性遗忘。尽管大型语言模型（LLMs）如带有检索增强生成（RAG）的方法在处理此类任务上已取得显著成就，但它们在大规模新经验融合方面仍面临挑战。本研究中，我们提出HippoRAG，一个受人类长期记忆海马回索引理论启发的新型检索框架，旨在促进对新经验的更深、更有效集成。HippoRAG巧妙地协同LLMs、知识图谱以及个性化PageRank算法，模拟人脑皮层和海马体在记忆中的不同作用。  我们将HippoRAG与现有RAG方法在多轮问答任务中进行比较，结果显示HippoRAG显著优于当前最先进的方法，性能提升高达20%。单步检索时，HippoRAG表现出与迭代检索方法如IRCoT相当或更好的性能，同时成本节省10-30倍，速度提升6-13倍。当将HippoRAG融入IRCoT后，还能带来额外的显著增益。最后，我们展示HippoRAG能够应对现有方法难以触及的新场景。代码和数据已在<https://github.com/OSU-NLP-Group/HippoRAG>上开源。|
|**2024-05-23**|**Can LLMs Solve longer Math Word Problems Better?**|Xin Xu et.al.|[2405.14804](http://arxiv.org/abs/2405.14804)|null|### 翻译  数学应用题（MWPs）是衡量大型语言模型（LLMs）能力的关键，但现有研究主要集中在简短背景的题目上。然而，现实生活中的数学问题往往涉及复杂情境，因此LLMs解决长篇数学应用题的能力对于其在实际场景的应用至关重要，但这一方面尚未得到充分探索。本研究首次关注Context Length Generalizability（CoLeG），即LLMs处理长篇数学应用题的能力。我们创建了Extended Grade-School Math（E-GSM）数据集，其中包含带有详细叙述的问题。为此，我们提出了两个新指标来评估LLMs在这类任务上的效能和鲁棒性。  通过对现有零样本提示方法以及商业和开源模型的考察，我们发现它们在CoLeG方面普遍存在不足。针对不同类型的LLMs，我们提出针对性的解决方案：对于专有模型，我们设计了一种新的指导性提示以减轻长文本的影响；对于开源模型，我们开发了一种数据增强任务以提升模型的适应性。我们的全面实验结果显示，我们的方法不仅在E-GSM上表现出色，而且在其他多个数学应用题基准上也展现出良好的泛化能力。  本研究的结果为未来利用LLMs处理复杂现实问题的研究提供了方向，为当前限制提出了实用解决方案，并为进一步探索模型泛化性和训练策略开辟了道路。|
|**2024-05-23**|**Lessons from the Trenches on Reproducible Evaluation of Language Models**|Stella Biderman et.al.|[2405.14782](http://arxiv.org/abs/2405.14782)|null|在自然语言处理（NLP）领域，有效评估语言模型仍然是一项未解的挑战。研究人员和工程师面临诸多方法论难题，例如模型对评估设置的敏感性、不同方法之间的比较困难，以及可重复性和透明度的缺失。本文基于三年的大型语言模型评估经验，为研究者提供指导和教训。首先，我们概述了语言模型评估中常见的问题。其次，我们阐述了应对或减轻这些问题的最佳实践。第三，我们介绍了Language Model Evaluation Harness（lm-eval）：一个开源库，旨在独立、可重复和扩展地评估语言模型，以解决这些问题。我们将介绍库的功能，并通过案例研究展示如何使用该库来缓解这些方法论关注点。|
|**2024-05-23**|**WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models**|Peng Wang et.al.|[2405.14768](http://arxiv.org/abs/2405.14768)|**[link](https://github.com/zjunlp/easyedit)**|**在大型语言模型（LLMs）中，随着世界事实的不断增长和纠正错误响应的需求，模型编辑的方法需要不断更新知识。论文的核心问题是：在编辑过程中，知识应存储在模型的哪个记忆层次更为合适。研究发现，直接修改长期记忆（模型参数）或利用工作记忆（通过检索的神经网络激活）都会导致不可逾越的三角困境——可靠性、泛化能力和局部性无法同时实现于终身编辑场景中。直接修改参数会与无关的预训练知识或先前编辑产生冲突（可靠性差、局部性不足）；而基于检索的工作记忆难以使模型理解并泛化编辑（泛化能力弱）。因此，作者提出了一个名为WISE的新方法，旨在弥合记忆之间的鸿沟。  在WISE中，设计了一种双参数内存机制，包括主内存用于存储预训练知识，侧内存用于存放编辑后的知识。仅对侧内存中的知识进行编辑，并训练一个路由器，以便根据查询决定从哪个内存中获取信息。对于持续编辑，采用了知识切片机制，将不同的编辑分布在参数的不同子空间中，然后合并到共享内存中，以避免冲突。实验结果表明，WISE在问答、幻觉生成和跨不同趋势的LLM架构（如GPT、LLaMA和Mistral）的终身模型编辑任务中表现出色，超越了先前的模型编辑方法，成功克服了上述困境。代码将在https://github.com/zjunlp/EasyEdit上发布。**|
|**2024-05-23**|**FinRobot: An Open-Source AI Agent Platform for Financial Applications using Large Language Models**|Hongyang Yang et.al.|[2405.14767](http://arxiv.org/abs/2405.14767)|**[link](https://github.com/ai4finance-foundation/finrobot)**|**随着金融机构和专业人士越来越多地将大型语言模型（LLMs）融入工作流程，金融行业与AI社区之间仍存在显著障碍，如专有数据和专业知识。这些挑战限制了AI在提升金融任务效率方面的潜力。鉴于金融分析的重要性，我们旨在开发专门针对金融的LLM驱动工具链，并通过开源项目推动其普及，促进AI在金融决策中的广泛应用。本文介绍FinRobot，一个创新的开源AI代理平台，支持多个金融专业AI代理，每个都由LLM驱动。平台主要分为四层：1）金融AI代理层，通过构建金融Chain-of-Thought（CoT）将复杂的金融问题分解为逻辑序列；2）金融LLM算法层，根据特定任务动态配置合适的模型应用策略；3）LLMOps和DataOps层，通过训练/微调技术以及使用与任务相关的数据生成精确模型；4）多源LLM基础模型层，整合各种LLM，使上述各层可以直接访问。FinRobot旨在为专业分析师和非专业人士提供实践操作，让他们能够利用强大的AI技术进行高级金融分析。FinRobot的开源代码可在此获取：\url{https://github.com/AI4Finance-Foundation/FinRobot}。**|
|**2024-05-23**|**Evaluating Large Language Models for Public Health Classification and Extraction Tasks**|Joshua Harris et.al.|[2405.14766](http://arxiv.org/abs/2405.14766)|null|随着大型语言模型（LLMs）的快速发展，人们对其在公共卫生领域支持专家工作的潜力产生了浓厚兴趣。本研究通过结合六个外部标注的和七个内部标注的数据集，评估了LLMs在处理与健康负担、流行病学风险因素和公共卫生干预相关的文本分类和提取任务上的性能。我们首先对五个开源大模型（参数量从7亿到70亿不等）进行了零样本的上下文学习测试。结果显示，Llama-3-70B-Instruct表现出色，微-F1得分在17个任务中的15项中最高。各任务间的性能差异显著，例如，有些模型如Contact Classification的得分低于60%，而像GI疾病分类这样的任务，所有模型都能达到80%以上的微-F1。对于12个任务的子集，我们还评估了GPT-4，发现其与Llama-3-70B-Instruct的结果相当，Llama-3-70B-Instruct在其中6个任务上得分更高或持平。总体而言，根据初步结果，我们发现LLMs有可能成为公共卫生专家从各种自由文本源提取信息的有效工具，有助于公共卫生监测、研究和干预措施。|
|**2024-05-23**|**Large language models can be zero-shot anomaly detectors for time series?**|Sarah Alnegheimish et.al.|[2405.14755](http://arxiv.org/abs/2405.14755)|null|近期的研究表明，大型语言模型能够执行多种任务，包括时间序列预测。这些模型的灵活性使其适用于众多应用。本文提出一项新颖的研究，探讨大型语言模型在复杂的时间序列异常检测任务中的性能。对于语言模型而言，这涉及识别输入序列（或多个部分）中的异常点，以及处理时间序列数据而非传统的文本输入。我们介绍了sigllm，一个专为时间序列异常检测设计的大型语言模型框架。该框架包含将时间序列转换为文本的模块，以及端到端的流程，用于引导语言模型进行异常检测。我们试验了两种测试大型语言模型能力的方法：一是直接提示模型指出输入中的异常元素；二是利用语言模型的预测能力来辅助检测过程。  我们在11个来自不同来源的数据集上评估了我们的框架，使用了10种不同的管道。结果显示，预测方法在所有11个数据集中都显著优于提示方法，尤其是在F1分数上。尽管大型语言模型能够发现异常，但目前的深度学习模型在性能上仍占优，其表现比大型语言模型高出30%。|
|**2024-05-21**|**Reducing Transformer Key-Value Cache Size with Cross-Layer Attention**|William Brandon et.al.|[2405.12981](http://arxiv.org/abs/2405.12981)|null|## 翻译  键值缓存对于加速Transformer架构的自回归大型语言模型（LLMs）的解码至关重要。然而，随着序列长度增加和批量大小增大，存储键值缓存所需的内存可能会变得难以承受。自从Transformer诞生以来，两个最有效的内存减小策略是多查询注意力（MQA）及其推广，群组查询注意力（GQA）。MQA和GQA通过让多个查询头共享单个键/值头，显著减少了不同键/值头的数量，同时对准确性影响较小。本文展示了如何进一步发展MQA，即在相邻层之间也共享键和值头，我们将其称为跨层注意力（CLA）。实验表明，使用CLA，可以在保持接近原始MQA精度的同时，将键值缓存的大小再减少2倍。我们在从头训练10亿参数和30亿参数模型的实验中验证了这一点，结果表明，CLA在内存与准确性之间的权衡上提供了优于传统MQA的帕累托改进，使得更长的序列长度和更大的批量大小下的推理成为可能。|
|**2024-05-21**|**Energy Rank Alignment: Using Preference Optimization to Search Chemical Space at Scale**|Shriram Chennakesavalu et.al.|[2405.12961](http://arxiv.org/abs/2405.12961)|**[link](https://github.com/rotskoff-group/llm-era)**|在化学空间中的搜索是一个极具挑战性的问题，因为可能的分子数量随着原子数量呈组合级增长。大型自回归模型通过学习化学化合物数据库已经产生了强大的生成器，但我们仍然缺乏有效策略来生成具有特定性质的分子。这个问题与大型语言模型的“对齐”问题相似，尽管在许多化学任务中，我们有一个明确且易于评估的奖励函数。本文介绍了一种名为能量排名对齐（ERA）的算法，它利用明确的奖励函数构建了一个梯度优化目标，用于调整自回归策略。理论上，我们发现该算法与Proximal Policy Optimization（PPO）和Direct Preference Optimization（DPO）密切相关，但其最小化器收敛于一个理想的吉布斯-玻尔兹曼分布，奖励函数扮演了能量角色。此外，该算法具有高度可扩展性，无需强化学习，并且在每对样本的偏好观察次数较少时，相对于DPO表现出色。  我们将这种方法应用于分子变压器的对齐，以生成具有外部指定属性的分子，并发现它能稳健地进行搜索，探索化学空间的多样化部分。虽然我们的重点在于化学搜索，但我们在一个AI监督的任务上也取得了优秀结果，表明该方法是可扩展且通用的。|
|**2024-05-21**|**Aggregation of Reasoning: A Hierarchical Framework for Enhancing Answer Selection in Large Language Models**|Zhangyue Yin et.al.|[2405.12939](http://arxiv.org/abs/2405.12939)|**[link](https://github.com/yinzhangyue/AoR)**|## 背景 近期，Chain-of-Thought提示的进展极大地推动了大型语言模型（LLMs）在复杂推理任务中的突破。当前研究通过采样多种推理路径并根据答案频率进行ensemble，提高了LLMs的推理性能。然而，这种方法在正确答案处于少数的情况时失效。我们发现这是制约LLMs推理能力的关键因素，仅凭预测答案无法解决这个问题。为此，我们提出了一个层次化的推理聚合框架AoR（推理聚合），它依据推理链条的评估来选择答案。此外，AoR引入了动态采样策略，根据任务复杂度调整推理链条的数量。  ## 任务 一系列复杂推理任务的实验结果显示，AoR相较于主流ensemble方法表现出色。进一步分析表明，AoR不仅适用于各种LLMs，而且在与现有方法的性能天花板比较中，达到了更优秀的水平。|
|**2024-05-21**|**Skin-in-the-Game: Decision Making via Multi-Stakeholder Alignment in LLMs**|Bilgehan Sel et.al.|[2405.12933](http://arxiv.org/abs/2405.12933)|null|大型语言模型在诸如总结、算术推理和问答等任务上表现出色。然而，在道德推理和伦理决策方面，尤其是在涉及多个利益相关者的复杂情景中，它们面临严峻挑战。本文提出了一种名为Skin-in-the-Game（SKIG）的框架，旨在通过从不同利益相关者角度审视决策的后果，提升语言模型在道德推理中的能力。SKIG的核心机制是模拟行动的责任感，结合同理心练习和风险评估，对提高其有效性至关重要。我们使用专有和开源语言模型在各种道德推理基准上验证SKIG的表现，并通过深入的消融分析探究其关键组件。|
|**2024-05-21**|**Code-mixed Sentiment and Hate-speech Prediction**|Anjali Yadav et.al.|[2405.12929](http://arxiv.org/abs/2405.12929)|null|在多语言环境中，混合代码（code-mixed discourse）指的是单文本中融合多种语言的现象，尤其是在官方语言多元的国家的非正式交流中常见。随着大型语言模型在自然语言处理任务中的主导地位提升，我们针对代码混合语境的研究也随之展开。首先，我们特别设计了四款新的英语-印地语和英语-斯洛文尼亚双语预训练遮罩语言模型，以适应非正式语言。接着，我们对各种类型的模型——包括单语、双语、少量语言和大规模多语言模型——在社交媒体文本的情感分析和攻击性语言检测等任务上的性能进行了评估。结果显示，最有效的分类器是针对社交媒体文本的专业化双语和多语言模型，随后是非专业的大规模多语言和单语模型，而大型生成模型的表现并不突出。对于涉及情感的问题，模型在处理代码混合数据时总体上略优于非代码混合数据。|
|**2024-05-21**|**Streamlining Software Reviews: Efficient Predictive Modeling with Minimal Examples**|Tim Menzies et.al.|[2405.12920](http://arxiv.org/abs/2405.12920)|**[link](https://github.com/timm/ez)**|该论文提出了一项新的软件分析挑战任务。在这个被称为“软件审查”的过程中，一组SME（主题专家）会评审软件行为示例，以建议如何改进软件的运行。由于SME的时间通常非常有限，理想的状况是，该团队仅通过查看少量具有高度信息价值的示例就能完成优化任务。为了支持这个审查过程，研究探索了训练预测模型的方法，该模型能够预测某个专家是否会喜欢或不喜欢下一个示例。这种预测模型可以与SME合作，引导他们探索所有示例，同时在专家离开后，模型也可以作为代理，处理新出现的案例，以应对专家们的忙碌。  在31个案例研究中（涵盖了从软件流程的高层决策到视频编码软件配置的低层决策），我们展示了仅使用12到30个标签就能建立这样的预测模型。据我们所知，仅凭少数示例（不依赖大型语言模型）就能取得这样的成果，在当前尚属罕见。遵循开放科学的原则，我们将在<https://github.com/timm/ez/tree/Stable-EMSE-paper>提供所有的代码和数据，以便他人能复制、验证或在此基础上进一步改进这些结果。|
|**2024-05-21**|**G-DIG: Towards Gradient-based DIverse and hiGh-quality Instruction Data Selection for Machine Translation**|Xingyuan Pan et.al.|[2405.12915](http://arxiv.org/abs/2405.12915)|null|大型语言模型（LLMs）在通用场景中展现出显著能力，通过指令微调，它们能够与人类在多种任务上协同。然而，指令数据的多样性和质量是指令微调面临的两大挑战。为此，本论文提出了一种新颖的基于梯度的方法，用于自动选择机器翻译中的高质量和多样化的指令微调数据。我们的核心创新在于分析单个训练样例如何在训练过程中影响模型。通过结合影响力函数和一小部分高质量种子数据，我们选择对模型产生积极影响的样例作为高质量数据。此外，为了增加数据多样性，我们通过聚类其梯度并重采样，最大化它们对模型产生的影响多样性。在WMT22和FLORES翻译任务上的广泛实验验证了我们方法的优越性，深入分析进一步证实了其效果和泛化能力。|
|**2024-05-21**|**An Empirical Study and Analysis of Text-to-Image Generation Using Large Language Model-Powered Textual Representation**|Zhiyu Tan et.al.|[2405.12914](http://arxiv.org/abs/2405.12914)|**[link](https://github.com/llm-conditioned-diffusion/llm-conditioned-diffusion.github.io)**|一个关键的先决条件是准确理解文本输入，这对于忠实的文本到图像生成至关重要。现有的方法利用CLIP模型的文本编码器来表示提示。然而，预训练的CLIP模型仅能处理英文，且其文本编码器的模型容量相对有限。相比之下，大型语言模型（LLMs）支持多语言输入，能够处理更长的上下文，并提供更优秀的文本表示。本文研究了使用LLMs作为文本编码器以提升文本到图像生成中的语言理解能力。然而，从头开始训练包含LLMs的文本到图像生成模型需要大量的计算资源和数据。  为此，我们提出了一种三阶段训练流程，有效地整合现有文本到图像模型与LLMs，同时保持高效的训练。特别地，我们设计了一个轻量级适配器，使得能够快速使用LLMs生成的文本表示来训练文本到图像模型。大量的实验表明，我们的模型不仅支持多语言输入，还能处理更长的上下文，而且在图像生成质量上表现出色。|
|**2024-05-21**|**Topic Modelling Case Law Using a Large Language Model and a New Taxonomy for UK Law: AI Insights into Summary Judgment**|Holli Sargeant et.al.|[2405.12910](http://arxiv.org/abs/2405.12910)|**[link](https://github.com/AhmedIzzidien/TopicLLM)**|**该论文关注法律分析中的一个重要空白，通过构建和应用一种新颖的判例主题分类法，对英国的简易判决案件进行了探索。利用精心挑选的简易判决案例数据集，我们利用大型语言模型Claude 3 Opus研究功能性话题和趋势。结果显示，Claude 3 Opus在主题分类上的准确率为87.10%，揭示了不同法律领域中简易判决的明显模式。由于英国的判例法并未原始标注关键词或提供主题过滤选项，这项研究不仅深化了我们对简易判决主题本质的理解，还展示了传统方法与人工智能驱动分类方法结合的可能性。因此，本文提供了英国法律的新通用分类框架。这项工作的意义为司法行政领域的进一步研究和计算法学研究方法论讨论奠定了基础。**|
|**2024-05-21**|**Adversarial DPO: Harnessing Harmful Data for Reducing Toxicity with Minimal Impact on Coherence and Evasiveness in Dialogue Agents**|San Kim et.al.|[2405.12900](http://arxiv.org/abs/2405.12900)|null|近期，大规模语言模型（LLMs）和各种有效的训练方法的兴起推动了开放领域对话系统的发展。然而，这些模型中的毒性问题对用户体验构成重大挑战。本文提出了一种创新的训练算法——对抗式直接偏好优化（ADPO），它是在直接偏好优化（DPO）的基础上改进的。ADPO旨在训练模型增加对优选回复的概率分布，同时降低对使用有毒控制令牌生成的不安全回复的概率。研究显示，ADPO能够增强模型抵御有害对话的能力，同时尽量减少性能下降。此外，我们证明ADPO提供了比传统DPO更为稳定的训练流程。据我们所知，这是首次将有害数据直接融入生成模型的DPO变体，从而减少了人工创建安全对话数据的需求。|
|**2024-05-20**|**Adapting Large Multimodal Models to Distribution Shifts: The Role of In-Context Learning**|Guanglin Zhou et.al.|[2405.12217](http://arxiv.org/abs/2405.12217)|**[link](https://github.com/jameszhou-gl/icl-distribution-shift)**|**近期的研究表明，大型多模态模型（LMMs）在应对自然分布变化时表现出极高的鲁棒性，常常超越先前的基准。然而，领域特定的适应仍然是必要的，尤其是在医疗等专业领域。鉴于LMMs庞大的参数空间使其微调不切实际，本研究聚焦于探索上下文学习（ICL）作为一种增强LMM适应性的有效方法。我们发现，ICL的成功在很大程度上依赖于示例的选择，这与大型语言模型类似，但对面临分布变化的LMMs提出了独特挑战。为此，我们评估了一种无监督的ICL方法——TopKNearestPR，该方法通过特征相似性进行最近示例搜索来选择示例。研究揭示了这种方法在处理分布转移场景下的视觉编码器缺陷对其效果的限制。  为解决这些问题，我们提出了一种新颖的方法——InvariantSelectPR，它利用类条件对比不变性（CCI）来提升预训练视觉编码器的稳健性。CCI通过增强不同类别间的区分度并确保对领域特定变化的不变性，提高了编码器识别和检索最有信息价值示例的能力。这种方法有助于引导LMM适应新的查询样本，即使在不同的分布下也是如此。实验结果显示，InvariantSelectPR显著提高了LMM的适应性，在Camelyon17和HAM10000基准数据集上的7-shot任务中，分别实现了34.2%和16.9%的准确率提升，相对于零-shot性能，这是显著的进步。**|
|**2024-05-20**|**MathBench: Evaluating the Theory and Application Proficiency of LLMs with a Hierarchical Mathematics Benchmark**|Hongwei Liu et.al.|[2405.12209](http://arxiv.org/abs/2405.12209)|**[link](https://github.com/open-compass/mathbench)**|**随着大型语言模型（LLMs）的最新进展在数学领域取得了显著进步，传统的数学基准如GSM8k在全面评价这些模型的数学能力方面存在局限。为了弥补这一不足，我们提出了MathBench，这是一个全新基准，旨在严格评估大型语言模型的数学能力。MathBench覆盖广泛的数学学科，对理论理解和实际问题解决能力进行详尽评估。它分为五个阶段，从基础算术到大学数学，结构上设计用于考察模型在不同深度知识的理解。每个阶段包括理论问题和应用题，以衡量模型的数学熟练度及其在实际情境中应用概念的能力。MathBench的目标是提升对LLMs数学能力的评价，提供对其知识理解水平和问题解决技能的细致视角，同时支持双语环境。该项目已发布在https://github.com/open-compass/MathBench。**|
|**2024-05-20**|**Developers' Perceptions on the Impact of ChatGPT in Software Development: A Survey**|Thiago S. Vaillant et.al.|[2405.12195](http://arxiv.org/abs/2405.12195)|**[link](https://github.com/gpt-impact/Paper-content)**|随着大型语言模型（如ChatGPT）的不断发展，其强大的自然语言处理能力和广泛应用引起了广泛关注。尽管人工智能（AI）与软件工程（SE）的融合趋势日益明显，但关于这种融合如何影响软件开发实践和认知的研究仍显不足。为了揭示将AI驱动工具，如ChatGPT，融入软件开发过程的影响和挑战，我们进行了一项调查，针对207名软件开发者进行了研究。调查内容包括ChatGPT对软件质量、生产力以及开发者工作满意度的影响，同时还探讨了他们对未来ChatGPT应用的预期、对可能的工作岗位替代的担忧，以及对监管措施的看法。|
|**2024-05-20**|**CT-Eval: Benchmarking Chinese Text-to-Table Performance in Large Language Models**|Haoxiang Shi et.al.|[2405.12174](http://arxiv.org/abs/2405.12174)|null|该论文介绍了一个名为CT-Eval的中文文本转表格数据集，旨在衡量大语言模型在非英语语言环境下的文本转表格任务性能。由于现有英文文本转表格数据集主要面向英语，CT-Eval填补了这一空白，选择了一种流行的多学科中文在线百科作为来源，涵盖了28个领域以保证数据多样性。为了减少数据虚构（hallucination）问题，研究者首先训练了一个语言模型来识别并过滤掉存在虚构问题的样本，然后人工标注验证集和测试集中的错误。最终，CT-Eval包含了大约88,600个任务样本。通过CT-Eval，研究者评估了开源和闭源大语言模型（如GPT-4）的表现，结果显示零-shot模式下这些模型与人类判断仍有显著差距。经过微调后，开源模型在文本转表格能力上有了显著提升，大幅超越了GPT-4。总之，CT-Eval不仅为评估和理解现有大语言模型的中文文本转表格能力提供了有价值的工具，也为提升这类模型在这项任务上的性能提供了宝贵资源。|
|**2024-05-20**|**Fennec: Fine-grained Language Model Evaluation and Correction Extended through Branching and Bridging**|Xiaobo Liang et.al.|[2405.12163](http://arxiv.org/abs/2405.12163)|**[link](https://github.com/dropreg/fennec)**|**随着大型语言模型的迅速发展，它们在众多现实任务中的应用日益广泛，主要目标是符合人类的意图。然而，理解人类意图的复杂性使得依赖于耗时的人工评估成为必要。为了缓解这一问题，我们探讨了利用开源大型语言模型作为评估者的趋势，特别是在GPT-4的流行背景下。我们提出了一种名为\textbf{Fennec}的框架，专注于\textbf{F}ine-grained \textbf{E}valuation（细致评估）和\textbf{N}eeded \textbf{E}xtension（必要扩展）通过分支（Branching）和连接（Bridging）。分支操作将评估任务分解为不同维度和粒度，从而减轻评估挑战。同时，连接操作融合了多样化的训练数据集，增加了评估任务的多样性。实验结果显示，我们的7B模型在各种常用基准上的\textit{一致性}和\textit{一致同意}性能均优于开源的更大规模评估模型，接近GPT-4的表现。我们利用模型的精细校正功能改进多个模型响应，结果显示，这种优化提升了响应质量，在MT-Bench上提高了1-2分。我们的代码已在GitHub上开源\footnote{\url{https://github.com/dropreg/Fennec}}。**|
|**2024-05-20**|**Eliciting Problem Specifications via Large Language Models**|Robert E. Wray et.al.|[2405.12147](http://arxiv.org/abs/2405.12147)|null|这篇论文探讨了如何利用大型语言模型（LLMs）在认知系统中实现问题定义的转化。通常情况下，人类需要将问题描述转化为认知系统能理解的形式。研究者展示了LLMs能够处理自然语言中定义的问题类别，并将其转换为半形式化规格，这样现有推理和学习系统可以解决这类问题的具体实例。他们设计了一种由LLM驱动的认知任务分析师代理，这种系统能够根据自然语言描述的任务生成问题空间的定义。LLM提示源自人工智能文献中的问题空间概念和通用问题解决策略（如波利亚的《如何解决问题》）。随后，认知系统利用这些问题空间规格，结合领域通用的解决问题策略（如搜索），来解决该类问题的不同实例。这一初步结果表明，通过消除问题表述的中介过程，LLMs有可能加速认知系统的研究，同时保持其核心能力，如稳健的推理和在线学习。|
|**2024-05-20**|**MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning**|Ting Jiang et.al.|[2405.12130](http://arxiv.org/abs/2405.12130)|**[link](https://github.com/kongds/mora)**|**低秩适应是大型语言模型中流行的参数高效微调方法。在这篇论文中，我们研究了低秩更新（如LoRA实现）的影响。我们的发现指出，这种机制可能限制了大语言模型学习和记忆新知识的能力。受此启发，我们提出了一种新的方法MoRA，它利用平方矩阵实现高秩更新，同时保持与LoRA相同的可训练参数数量。为此，我们引入了相应的非参数运算器，以降低输入维度并增加输出维度处理平方矩阵。这些运算器确保权重能无缝融入到大语言模型中，使得我们的方法能够像LoRA一样部署。我们在五个任务上进行了全面评估：指令调整、数学推理、连续预训练、记忆以及预训练。在内存密集型任务上，我们的方法优于LoRA，并在其他任务上表现出相当的性能。**|
|**2024-05-20**|**Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation**|Zhankui He et.al.|[2405.12119](http://arxiv.org/abs/2405.12119)|null|大型语言模型（LLMs）正在通过出色地索引项目内容、理解复杂的对话上下文并生成相关项目标题，革新了对话推荐系统。然而，控制推荐项目的分布仍是一个挑战，导致在针对对话推荐平台的快速变化的数据分布，如项目流行度上，性能欠佳。在对话推荐中，LLMs通过自回归方式生成项目标题（作为多个令牌），这使得获取和控制所有项目推荐变得困难。因此，我们提出了一种名为“重索引-然后适应”（Reindex-Then-Adapt，RTA）的框架，它将多令牌项目标题转换为单个令牌于LLMs内，随后调整这些单令牌项目标题的概率分布。RTA框架结合了LLMs理解和复杂查询的优势，以及传统推荐系统（RecSys）在对话推荐中有效控制推荐项目分布的能力。实验结果表明，我们的框架在三个不同的对话推荐数据集和两种适应设置下，展示了改进的准确性指标。|
|**2024-05-20**|**Imp: Highly Capable Large Multimodal Models for Mobile Devices**|Zhenwei Shao et.al.|[2405.12107](http://arxiv.org/abs/2405.12107)|**[link](https://github.com/milvlg/imp)**|**尽管大型语言模型（LLMs）和大型多模态模型（LMMs）在开放世界多模态理解方面展现出惊人的能力，但它们通常参数量大、计算需求高，限制了在资源受限环境中的应用。为了应对这一问题，研究人员已经提出了一系列轻量级LMM，旨在在有限规模（如30亿参数）下最大化性能。然而，这些方法多数仅关注设计空间的单一或两个方面，对影响模型能力的关键设计选择尚未进行全面探讨。  本文系统地研究了轻量级LMM的设计，包括模型架构、训练策略和训练数据。根据我们的研究结果，我们构建了一套名为Imp的高性能LMM家族，覆盖20亿到40亿参数规模。尤其值得注意的是，我们的Imp-30亿模型在与同类规模的现有轻量级模型相比时持续领先，并超越了130亿参数规模的最新LMM状态。通过低精度量化和分辨率降低技术，Imp模型能够在高通骁龙8Gen3移动芯片上实现高速部署，每秒处理大约13个令牌的推理速度。**|
|**2024-05-20**|**DOP: Diagnostic-Oriented Prompting for Large Language Models in Mathematical Correction**|Hao Chen et.al.|[2405.12100](http://arxiv.org/abs/2405.12100)|null|## 背景 数学世界问题修正（MWPC）是一个专门针对解决数学问题过程中错误推理的修正任务。本文利用大语言模型（LLMs）的进步，关注两点：（1）区分数学推理与错误修正；（2）探索策略以提升LLMs在数学领域的错误修正能力，以应对MWPC任务。我们注意到，在实时教育中，帮助学生识别错误比单纯提供正确答案更为关键。然而，当前研究往往侧重于获取精确的解题答案，而非纠正可能的错误。因此，我们调整了研究范式，表明提升数学推理能力并不等同于精通错误修正。同时，我们提出了一种名为诊断导向提示（DOP）的新方法，旨在促进LLMs在错误修正方面表现出色。实验结果显示，DOP表现出卓越性能，彰显其重要性。我们强调，在数学教育中，对出色修正者的需要超过了对熟练推理者的追求。代码和数据可在<https://github.com/ChenhaoEcnuCS/Reason-Correct>获取。|
|**2024-05-17**|**A Survey on Large Language Models with Multilingualism: Recent Advances and New Frontiers**|Kaiyu Huang et.al.|[2405.10936](http://arxiv.org/abs/2405.10936)|**[link](https://github.com/kaiyuhwang/mllm-survey)**|**随着大型语言模型（LLMs）的快速发展，在自然语言处理领域展现出显著的多语言能力，引起了学术界和业界的广泛关注。为了减少潜在的歧视并提升技术的通用性和可访问性，对于多语言技术的发展至关重要。尽管LLMs取得了突破，但对多语言场景的深入研究仍显不足。因此，迫切需要一份全面的综述，总结近期的方法、进展、局限性和可能的解决方案。本文旨在从多个角度审视LLMs在多语言环境中的应用。我们首先回顾了预训练语言模型研究的历史演变。接着，我们探讨了LLMs的多语言特性，包括训练和推理方法、模型安全、跨领域与文化适应以及数据集使用。我们还分析了这些方面面临的挑战，并提出可能的解决策略。此外，我们指出了未来的研究方向，以进一步提升LLMs的多语言性能。本综述旨在帮助研究界应对多语言问题，提供一个关于基于LLMs的多语言自然语言处理核心概念、关键技术及最新进展的全面理解。**|
|**2024-05-17**|**The Local Interaction Basis: Identifying Computationally-Relevant and Sparsely Interacting Features in Neural Networks**|Lucius Bushnaq et.al.|[2405.10928](http://arxiv.org/abs/2405.10928)|**[link](https://github.com/apolloresearch/rib)**|### 概述  机械解释性目标是通过逆向工程理解神经网络的行为。然而，现有方法在解析神经网络激活方面面临挑战，因为缺乏对激活的分解，使得单个神经元或模型组件无法清晰对应于独特的特征或功能。为此，我们提出了一种新颖的可解释性方法——局部交互基（Local Interaction Basis，LIB）。LIB旨在通过消除无关激活和交互，识别计算特征。该方法摒弃无意义的激活方向，并使基础与相邻层间雅可比矩阵的奇异向量对齐。同时，它根据特征对后续计算的重要性进行缩放，生成一个显示模型中所有计算相关特性和交互的图谱。  我们在模块加法和CIFAR-10模型上评估了LIB的有效性，结果表明，相比于主成分分析，LIB能识别出更多计算相关的特征，并呈现出更稀疏的交互。然而，在应用于语言模型时，LIB并未显著提高可解释性或交互稀疏度。因此，我们得出结论，尽管LIB是一种有前景的理论驱动方法，但当前形式并不适用于大型语言模型。|
|**2024-05-17**|**COGNET-MD, an evaluation framework and dataset for Large Language Model benchmarks in the medical domain**|Dimitrios P. Panagoulias et.al.|[2405.10893](http://arxiv.org/abs/2405.10893)|null|这篇技术论文阐述了COGNET-MD，一个专为医疗领域设计的大型语言模型评估的新基准。我们提出了一种评分框架，旨在评估语言模型理解医学文本的能力，并且设计了一系列难度分级的多项选择题（MCQ）数据库。这个数据库由多个医疗领域的专家合作创建，以反映当前医学趋势，确保安全、实用和适用性。初期版本包含了精神科、牙科、肺病学、皮肤科和内分泌学等领域的题目，但会持续扩展，未来还会加入更多医学学科。|
|**2024-05-17**|**Application of Artificial Intelligence in Schizophrenia Rehabilitation Management: Systematic Literature Review**|Hongyi Yang et.al.|[2405.10883](http://arxiv.org/abs/2405.10883)|null|该综述旨在系统地评估人工智能（AI）在精神分裂症患者康复管理中的现状和前景，以及其对康复过程的影响。我们从2012年至现在筛选了70项研究，重点关注机器学习、深度学习、强化学习等技术在心理健康干预和管理中的应用、技术类别、产品和数据类型，如生态瞬时评估、行为和语音数据的分析。结果显示，AI在症状监测、复发风险预测和康复治疗中具有广泛的应用潜力。此外，本研究还探讨了基于AI的新兴产品、技术和分析方法，如社交媒体分析、严肃游戏和大型语言模型在康复中的潜在挑战和未来发展方向。总的来说，这篇论文系统回顾了AI在精神分裂症康复管理中的应用，并为未来的研究路径提供了有价值的见解和建议。|
|**2024-05-17**|**The Future of Large Language Model Pre-training is Federated**|Lorenzo Sani et.al.|[2405.10853](http://arxiv.org/abs/2405.10853)|null|## 背景  生成式预训练大型语言模型（LLMs）因其在众多任务上的出色表现而备受瞩目，这得益于它们所接受的海量训练数据。根据已建立的规模法则，LLMs未来性能的提升在很大程度上依赖于我们能够利用的计算和数据资源。联邦学习（FL）有可能释放全球大部分未充分利用的数据和计算能力，这些是当前以数据中心为中心的LLM训练方法所忽视的。本文提出了一种稳健、灵活且可复现的FL方法，旨在促进机构间的大规模协作，共同训练LLMs，从而动员更多的计算和数据资源，甚至可能达到或超越中心化的性能。  ## 任务  我们的工作展示了一种FL训练方法，它能够在有限资源下扩展到百亿元级的联邦LLM，使得拥有丰富数据的实体能够成为预训练LLMs的主导力量，而不是仅让计算资源丰富的机构独占鳌头。这种方法强调了联邦训练的规模效益，并为实现这一目标提供了一种实用路径。|
|**2024-05-17**|**Large Language Model (LLM) for Telecommunications: A Comprehensive Survey on Principles, Key Techniques, and Opportunities**|Hao Zhou et.al.|[2405.10825](http://arxiv.org/abs/2405.10825)|null|随着大型语言模型（LLMs）因其卓越的理解和推理能力而备受瞩目，它们在各个领域取得了显著进步，尤其在第六代（6G）通信技术的推动下展现出人工智能通用性（AGI）的潜力。本研究旨在全面概述LLM赋能的电信网络。首先，我们概述了LLMs的基础，包括模型架构、预训练、微调、推理与应用、模型评估，以及在电信部署中的运用。接着，我们将探讨LLM支持的关键技术和电信应用，涉及生成、分类、优化和预测问题。生成应用包括电信领域知识、代码和网络配置自动生成。基于LLM的分类任务涵盖网络安全、文本、图像和流量分类。此外，我们介绍了利用LLMs的自动化优化技术，如强化学习的奖励函数设计和口语强化学习。对于预测问题，LLMs可用于时间序列预测和多模态电信预测。最后，我们指出了LLM赋能电信网络所面临的挑战，并展望了未来的研究方向。|
|**2024-05-17**|**ActiveLLM: Large Language Model-based Active Learning for Textual Few-Shot Scenarios**|Markus Bayer et.al.|[2405.10808](http://arxiv.org/abs/2405.10808)|null|主动学习旨在通过优先处理最能提升学习效果的实例来减少标注工作量。然而，许多主动学习策略面临“冷启动”问题，即在初期需要大量数据才能发挥效能，这限制了它们在预训练模型（如BERT）上的应用，这些模型在少量样本情况下已表现良好。为此，我们提出了一种新颖的主动学习方法——ActiveLLM，它利用大型语言模型（如GPT-4、Llama 3和Mistral Large）进行实例选择。实验证明，ActiveLLM显著提高了BERT分类器在少量样本情况下的性能，超越了传统主动学习方法和SetFit等少数样本学习方法。此外，ActiveLLM还能扩展到非少量样本场景，支持迭代选择，从而帮助其他主动学习策略克服冷启动难题。结果表明，ActiveLLM为改善不同学习环境中的模型性能提供了有前景的解决方案。|
|**2024-05-17**|**Empowering Small-Scale Knowledge Graphs: A Strategy of Leveraging General-Purpose Knowledge Graphs for Enriched Embeddings**|Albert Sawczyn et.al.|[2405.10745](http://arxiv.org/abs/2405.10745)|null|### 翻译  知识密集型任务对机器学习（ML）技术提出了严峻挑战。通常采用的方法，如大型语言模型（LLMs），在处理这类任务时往往存在局限性。然而，人们已经努力通过知识图谱（KG）来弥补这些不足，尤其是通过将小规模的领域特定KG与通用KG相结合。尽管KG在知识表示方面具有优势，但构建它们的成本可能阻碍了广泛的研究和应用。为此，我们提出了一种框架，旨在通过链接到大规模通用KG来提升小型领域特定KG嵌入的学习性能。实验结果显示，这种方法带来了显著的提升，例如，Hits@10指标最高提高了44%。这一相对未被充分探索的研究方向有望促进KG在知识密集型任务中的更频繁运用，从而产生更为稳健、可靠的ML解决方案，它们相较于流行但易出错的LLM方法更具可靠性。关键词：知识图谱、知识图谱补全、实体对齐、表示学习、机器学习|
|**2024-05-17**|**Efficient Multimodal Large Language Models: A Survey**|Yizhang Jin et.al.|[2405.10739](http://arxiv.org/abs/2405.10739)|**[link](https://github.com/lijiannuist/efficient-multimodal-llms-survey)**|**在过去一年里，多模态大型语言模型（Multimodal Large Language Models，MLLMs）在诸如视觉问答、视觉理解和推理等任务上展现出卓越性能。然而，这些模型的庞大规模和高昂的训练与推理成本限制了它们在学术界和工业界的广泛应用。因此，研究高效且轻量级的MLLM具有巨大的潜力，特别是在边缘计算环境中。本综述全面系统地回顾了当前高效MLLM的研究现状。我们概述了代表性高效模型的发展历程，总结了有效结构和策略的研究状态，以及其实用应用。最后，我们讨论了当前高效MLLM研究的局限，并展望了有前景的未来发展方向。如需更多信息，请参考我们的GitHub仓库：https://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey。**|
|**2024-05-17**|**INDUS: Effective and Efficient Language Models for Scientific Applications**|Bishwaranjan Bhattacharjee et.al.|[2405.10725](http://arxiv.org/abs/2405.10725)|null|大型通用语言模型在自然语言处理任务上表现出色。然而，先前的研究表明，针对特定领域的训练数据可以使模型在专业任务上表现更佳。为此，我们开发了INDUS，一套专为地球科学、生物学、物理学、太阳物理、行星科学和天文学领域设计的定制化语言模型。这些模型基于精心挑选的科学语料库，包括：（1）一个使用领域专用词汇和数据集训练的编码器，用于提升自然语言理解任务的表现；（2）一个基于对比学习的通用文本嵌入模型，利用多源数据集进行训练，以优化信息检索任务；（3）通过知识蒸馏技术缩小规模的模型，适用于对延迟和资源有限的应用。此外，我们创建了三个新的科学基准数据集：CLIMATE-CHANGE-NER（实体识别）、NASA-QA（抽取式问答）和NASA-IR（信息检索），以推动跨学科领域的研究进展。最后，实验结果显示，我们的模型在新任务和相关领域现有基准任务上均优于通用编码器（如RoBERTa）和现有的领域特定编码器（如SciBERT）。|
|**2024-05-16**|**UniRAG: Universal Retrieval Augmentation for Multi-Modal Large Language Models**|Sahel Sharifymoghaddam et.al.|[2405.10311](http://arxiv.org/abs/2405.10311)|null|## 背景  近期，多模态（MM）大型语言模型（LLMs）已经解锁了许多需要多模态理解（如图像描述或视觉问答）和生成（如文本引导的图像生成或编辑）复杂任务。为了进一步提升MM-LLMs的输出质量，我们提出了一种模型通用的UniRAG技术，它在推理阶段将相关检索信息添加到提示中，作为少量样例。与普遍认为检索增强（RA）主要改进罕见实体的生成或理解不同，我们在MSCOCO数据集上对包括GPT4、Gemini-Pro在内的专有模型以及Llava、LaVIT和Emu2等开源小型模型进行了评估，结果显示，这些模型在输入提示通过MM检索器（如UniIR模型）增强后，显著提高了生成质量。|
|**2024-05-16**|**4D Panoptic Scene Graph Generation**|Jingkang Yang et.al.|[2405.10305](http://arxiv.org/abs/2405.10305)|**[link](https://github.com/jingkang50/psg4d)**|**我们生活在一个三维空间中，同时通过第四维时间向前推进。为了使人工智能能够全面理解这种4D环境，我们提出了一种新的表示形式——4D全景场景图（PSG-4D），它将动态4D世界中的原始视觉数据抽象为节点和边，节点代表具有精确位置和状态信息的实体，边捕捉时间关系。为了促进在这一新领域的研究，我们构建了一个丰富的注释PSG-4D数据集，包含3000个RGB-D视频，总计100万帧，每帧都带有4D全景分割掩码以及详细的动态场景图标签。我们为此任务提出了一种名为PSG4DFormer的Transformer模型，该模型能够预测全景分割掩码，沿时间轴跟踪掩码，并通过关系组件生成相应的场景图。在新数据集上的大量实验表明，我们的方法为未来的PSG-4D研究提供了一个强大的基准。最后，我们展示了如何通过将大型语言模型融入我们的PSG-4D系统来实现动态场景理解的一个实际应用示例。**|
|**2024-05-16**|**HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models**|Rhea Sanjay Sukthanker et.al.|[2405.10299](http://arxiv.org/abs/2405.10299)|**[link](https://github.com/automl/hw-aware-llm-bench)**|**随着语言模型的规模不断扩大，对硬件指标（如延迟、能耗、GPU内存使用和性能）之间的权衡需求日益增长。人们正在寻求为不同语言模型配置建立帕累托前沿，以在指定硬件限制下找到最优模型。然而，对多种架构在多台设备上的全面训练和评估在计算上是不可行的。为此，我们提出了HW-GPT-Bench，这是一个基于硬件感知的语言模型代理基准，利用神经架构搜索（NAS）中的权重共享技术，在一个模型中高效地训练包含不同规模语言模型的超网络。我们在13种设备上对这些模型进行了性能剖析，考虑了5种硬件指标和3种不同的模型规模。最后，我们通过8种不同的多目标NAS算法展示了HW-GPT-Bench的可用性，并评估了由此产生的帕累托前沿的质量。我们的目标是推动和加速大型语言模型的多目标方法，如NAS和结构化剪枝的研究。**|
|**2024-05-16**|**Timeline-based Sentence Decomposition with In-Context Learning for Temporal Fact Extraction**|Jianhao Chen et.al.|[2405.10288](http://arxiv.org/abs/2405.10288)|null|**摘要：**  事实抽取对于构建知识图谱至关重要。随着对时间相关事实在下游任务中的需求增长，出现了时间性事实抽取的任务。本文特别关注从自然语言文本中提取时间性事实。先前的研究未能妥善处理复杂句子中时间与事实对应关系的建立难题。为解决这一挑战，我们提出了一种基于时间线的句子分解策略，利用大语言模型（LLMs）进行上下文学习，以实现对事实相关时间线的精细理解。然而，直接使用LLMs进行时间性事实抽取的性能并不理想。因此，我们引入了TSDRE方法，将LLMs的分解能力融入到小型预训练语言模型（PLMs）的传统微调过程中。  为了支持评估，我们构建了一个复杂的时序事实抽取数据集ComplexTRED。实验结果显示，TSDRE在HyperRED-Temporal和ComplexTRED数据集上实现了最先进的性能。|
|**2024-05-16**|**Revisiting OPRO: The Limitations of Small-Scale LLMs as Optimizers**|Tuo Zhang et.al.|[2405.10276](http://arxiv.org/abs/2405.10276)|null|近年来，许多研究旨在通过策略性提示提升大型语言模型（LLMs）的效能。特别是优化通过prompting（OPRO）方法表现出顶尖性能，它利用LLMs作为优化器，目标是寻找能最大化任务准确性的指令。本论文重新审视了OPRO在小型LLMs（如LaMa-2系列和Mistral 7B）上的自动化提示效果。我们的研究表明，对于小型LLMs，OPRO的效果有限，因为其有限的推理能力限制了优化潜力。因此，我们建议未来的自动提示工程应同时考虑模型能力和计算成本。针对小型LLMs，我们推荐直接提供明确阐述目标和方法的指令，作为稳健的提示基线，以确保在当前研究中实现高效且有效的提示设计。|
|**2024-05-16**|**Keep It Private: Unsupervised Privatization of Online Text**|Calvin Bao et.al.|[2405.10260](http://arxiv.org/abs/2405.10260)|**[link](https://github.com/csbao/kip-privatization)**|**## 背景  作者身份混淆技术有望通过自动重写文本来保护网络通信中的个人隐私。然而，在自然语言处理（NLP）文献中，这些技术的评估大多局限在狭小场景下，主要依赖于表面的编辑操作，可能导致输出不自然。本研究提出了一种自动文本私密化框架，通过强化学习对大型语言模型进行微调，以生成兼顾准确、连贯和隐私的重写。我们在大规模的英语Reddit帖子测试集上进行了详尽的评估，该数据集由68,000名作者撰写，包含短到中等长度的文本。我们探讨了在不同评估条件下，如作者简介长度和作者识别策略，性能的变化。我们的方法在自动化指标和人工评估中保持高文本质量，并成功地规避了几种自动作者识别攻击。**|
|**2024-05-16**|**When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models**|Xianzheng Ma et.al.|[2405.10255](http://arxiv.org/abs/2405.10255)|null|随着大型语言模型（LLMs）的不断发展，它们与三维空间数据（3D-LLMs）的融合取得了显著进步，这极大地增强了理解和互动物理环境的能力。这篇综述详细探讨了使LLMs能够处理、理解并生成三维数据的方法论，强调了LLMs的独特优势，如上下文学习、逐步推理、开放词汇能力和丰富的世界知识，这些将极大地推动人工智能体在空间理解与交互方面的发展。研究覆盖了从点云到神经辐射场（NeRF）等各种三维数据表示，并考察了它们与LLMs在任务中的结合，如三维场景理解、描述、问答和对话，以及基于LLM的代理进行空间推理、规划和导航。此外，我们还简要回顾了其他结合三维和语言的方法。本文的元分析显示了显著的进步，但也指出了挖掘3D-LLMs全部潜力所需的创新方法的必要性。因此，本文旨在为未来的研究方向提供指导，探索和扩展3D-LLMs在理解和互动复杂三维世界的能力。为了支持本调查，我们已在GitHub上建立了一个项目页面，整理并列出了相关论文：https://github.com/ActiveVisionLab/Awesome-LLM-3D。|
|**2024-05-16**|**A Systematic Evaluation of Large Language Models for Natural Language Generation Tasks**|Xuanfan Ni et.al.|[2405.10251](http://arxiv.org/abs/2405.10251)|null|近期的研究已评估了大型语言模型（LLMs）在常识推理、数学推理和代码生成等方面的能力。然而，据我们所知，尚无专门针对自然语言生成（NLG）任务的深入研究，这是衡量模型优秀程度的关键标准。因此，本论文旨在全面评估知名且性能出色的LLMs，包括ChatGPT、ChatGLM、基于T5的模型、基于LLaMA的模型和Pythia模型，在对话生成和文本总结等NLG任务中的表现。我们选择了涵盖英语和中文的数据集，并设计了一种共同的评估框架，包括输入模板和后处理策略。研究结果报告了自动评分，同时进行了详细分析。|
|**2024-05-16**|**IntelliExplain: Enhancing Interactive Code Generation through Natural Language Explanations for Non-Professional Programmers**|Hao Yan et.al.|[2405.10250](http://arxiv.org/abs/2405.10250)|null|大型语言模型（LLMs）在根据自然语言描述自动生成可执行代码方面展现出巨大潜力，特别是通过互动功能，用户可以通过迭代反馈指导模型。然而，当前的互动方式往往假设用户具备调试源代码的专业知识，对非专业程序员不太友好。这使得使互动代码生成对不同编程水平的个体更易于使用成为一个挑战。为解决这个问题，我们提出了IntelliExplain，这是一种创新的人机交互范式，通过让用户通过自然语言解释与源代码互动，提升非专业人士的体验。用户通过提供他们发现错误的自然语言纠正反馈，来指导系统修订代码，直到用户对系统的代码解释感到满意。我们的用户研究显示，使用IntelliExplain的用户在Text-to-SQL和Python代码生成任务中的成功率分别比纯GPT-3.5提高了11.6%和25.3%，同时所需时间分别减少了39.0%和15.6%。|
|**2024-05-16**|**CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations**|Jiahao Zhao et.al.|[2405.10212](http://arxiv.org/abs/2405.10212)|null|在这篇论文中，我们提出了一种创新的心理学基准测试——CPsyExam，它源于中国语言考试的问题。CPsyExam旨在分别强调心理学知识和案例分析的重要性，认识到将心理学知识应用于实际情境的价值。从22,000个问题库中，我们精选了4,000个来构建该基准，确保了主题的均衡覆盖，并包含了各种案例分析方法的多样性。此外，我们对一系列现有的大型语言模型（LLMs）进行了评估，包括开源和API基础的模型。实验和分析结果显示，CPsyExam是一个有效的确立语言模型对心理学理解能力的基准，同时支持在不同粒度上比较这些模型。|

<p align=right>(<a href=#updated-on-20240606>back to top</a>)</p>

